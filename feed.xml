<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Production-Grade Container Orchestration</title>
    <link>https://kubernetes.io/</link>
    <description>Recent Hugo news from gohugo.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 30 Apr 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kubernetes.io/img/hugo.png</url>
      <title>GoHugo.io</title>
      <link>https://kubernetes.io/</link>
    </image>
    
	<atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: Airflow on Kubernetes (Part 1): A Different Kind of Operator</title>
      <link>https://kubernetes.io/blog/2018/06/28/airflow-on-kubernetes-part-1-a-different-kind-of-operator/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/06/28/airflow-on-kubernetes-part-1-a-different-kind-of-operator/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Daniel Imberman (Bloomberg LP)&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As part of Bloomberg&amp;rsquo;s &lt;a href=&#34;https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/&#34; target=&#34;_blank&#34;&gt;continued commitment to developing the Kubernetes ecosystem&lt;/a&gt;, we are excited to announce the Kubernetes Airflow Operator; a mechanism for &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Airflow&lt;/a&gt;, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.&lt;/p&gt;

&lt;h2 id=&#34;what-is-airflow&#34;&gt;What Is Airflow?&lt;/h2&gt;

&lt;p&gt;Apache Airflow is one realization of the DevOps philosophy of &amp;ldquo;Configuration As Code.&amp;rdquo; Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow_dags.png&#34; width=&#34;85%&#34; alt=&#34;Airflow DAGs&#34; /&gt;
&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow.png&#34; width=&#34;85%&#34; alt=&#34;Airflow UI&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-airflow-on-kubernetes&#34;&gt;Why Airflow on Kubernetes?&lt;/h2&gt;

&lt;p&gt;Since its inception, Airflow&amp;rsquo;s greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.&lt;/p&gt;

&lt;p&gt;To address this issue, we&amp;rsquo;ve utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an &amp;ldquo;any job you want&amp;rdquo; workflow orchestrator.&lt;/p&gt;

&lt;h2 id=&#34;the-kubernetes-operator&#34;&gt;The Kubernetes Operator&lt;/h2&gt;

&lt;p&gt;Before we move any further, we should clarify that an &lt;a href=&#34;https://airflow.apache.org/concepts.html#operators&#34; target=&#34;_blank&#34;&gt;Operator&lt;/a&gt; in Airflow is a task definition. When a user creates a DAG, they would use an operator like the &amp;ldquo;SparkSubmitOperator&amp;rdquo; or the &amp;ldquo;PythonOperator&amp;rdquo; to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.&lt;/p&gt;

&lt;p&gt;Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Increased flexibility for deployments:&lt;/strong&gt;&lt;br /&gt;
Airflow&amp;rsquo;s plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Flexibility of configurations and dependencies:&lt;/strong&gt;
For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires &lt;a href=&#34;https://www.scipy.org&#34; target=&#34;_blank&#34;&gt;SciPy&lt;/a&gt; and another that requires &lt;a href=&#34;http://www.numpy.org&#34; target=&#34;_blank&#34;&gt;NumPy&lt;/a&gt;, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Usage of kubernetes secrets for added security:&lt;/strong&gt;
Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow-architecture.png&#34; width=&#34;85%&#34; alt=&#34;Airflow Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Kubernetes Operator uses the &lt;a href=&#34;https://github.com/kubernetes-client/Python&#34; target=&#34;_blank&#34;&gt;Kubernetes Python Client&lt;/a&gt; to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you&amp;rsquo;ve defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.&lt;/p&gt;

&lt;h1 id=&#34;using-the-kubernetes-operator&#34;&gt;Using the Kubernetes Operator&lt;/h1&gt;

&lt;h2 id=&#34;a-basic-example&#34;&gt;A Basic Example&lt;/h2&gt;

&lt;p&gt;The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the &lt;code&gt;passing-task&lt;/code&gt; pod should complete, while the &lt;code&gt;failing-task&lt;/code&gt; pod returns a failure to the Airflow webserver.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DAG
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;datetime&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; datetime, timedelta
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.contrib.operators.kubernetes_pod_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; KubernetesPodOperator
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.operators.dummy_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DummyOperator


default_args &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; {
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow&amp;#39;&lt;/span&gt;,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;depends_on_past&amp;#39;&lt;/span&gt;: False,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;: datetime&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;utcnow(),
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow@example.com&amp;#39;&lt;/span&gt;],
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_failure&amp;#39;&lt;/span&gt;: False,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_retry&amp;#39;&lt;/span&gt;: False,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retries&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retry_delay&amp;#39;&lt;/span&gt;: timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)
}

dag &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DAG(
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;kubernetes_sample&amp;#39;&lt;/span&gt;, default_args&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;default_args, schedule_interval&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;))


start &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DummyOperator(task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;run_this_first&amp;#39;&lt;/span&gt;, dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag)

passing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,
                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python:3.6&amp;#34;&lt;/span&gt;,
                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],
                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],
                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},
                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-test&amp;#34;&lt;/span&gt;,
                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-task&amp;#34;&lt;/span&gt;,
                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,
                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag
                          )

failing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,
                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ubuntu:1604&amp;#34;&lt;/span&gt;,
                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],
                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],
                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},
                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,
                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,
                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,
                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag
                          )

passing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)
failing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-basic-dag-run.png&#34; width=&#34;85%&#34; alt=&#34;Basic DAG Run&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;but-how-does-this-relate-to-my-workflow&#34;&gt;But how does this relate to my workflow?&lt;/h2&gt;

&lt;p&gt;While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.&lt;/p&gt;

&lt;h3 id=&#34;1-pr-in-github&#34;&gt;1: PR in github&lt;/h3&gt;

&lt;p&gt;Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR&amp;rsquo;ing your code, and merge to the master branch to trigger an automated CI build.&lt;/p&gt;

&lt;h3 id=&#34;2-ci-cd-via-jenkins-docker-image&#34;&gt;2: CI/CD via Jenkins -&amp;gt; Docker Image&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers&#34; target=&#34;_blank&#34;&gt;Generate your Docker images and bump release version within your Jenkins build&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;3-airflow-launches-task&#34;&gt;3: Airflow launches task&lt;/h3&gt;

&lt;p&gt;Finally, update your DAGs to reflect the new release version and you should be ready to go!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;production_task &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,
                          &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span&gt;
                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span&gt;,
                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],
                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],
                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,
                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,
                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,
                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag
                          )&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id=&#34;launching-a-test-deployment&#34;&gt;Launching a test deployment&lt;/h1&gt;

&lt;p&gt;Since the Kubernetes Operator is not yet released, we haven&amp;rsquo;t released an official &lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;helm&lt;/a&gt; chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below  and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:&lt;/p&gt;

&lt;h2 id=&#34;step-1-set-your-kubeconfig-to-point-to-a-kubernetes-cluster&#34;&gt;Step 1: Set your kubeconfig to point to a kubernetes cluster&lt;/h2&gt;

&lt;h2 id=&#34;step-2-clone-the-airflow-repo&#34;&gt;Step 2: Clone the Airflow Repo:&lt;/h2&gt;

&lt;p&gt;Run &lt;code&gt;git clone https://github.com/apache/incubator-airflow.git&lt;/code&gt; to clone the official Airflow repo.&lt;/p&gt;

&lt;h2 id=&#34;step-3-run&#34;&gt;Step 3: Run&lt;/h2&gt;

&lt;p&gt;To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml
./scripts/ci/kubernetes/Docker/build.sh
./scripts/ci/kubernetes/kube/deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we move on, let&amp;rsquo;s discuss what these commands are doing:&lt;/p&gt;

&lt;h3 id=&#34;sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml&#34;&gt;sed -ie &amp;ldquo;s/KubernetesExecutor/LocalExecutor/g&amp;rdquo; scripts/ci/kubernetes/kube/configmaps.yaml&lt;/h3&gt;

&lt;p&gt;The Kubernetes Executor is another Airflow feature that allows for dynamic allocation  of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-docker-build-sh&#34;&gt;./scripts/ci/kubernetes/Docker/build.sh&lt;/h3&gt;

&lt;p&gt;This script will tar the Airflow master source code build a Docker container based on the Airflow distribution&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-kube-deploy-sh&#34;&gt;./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3&gt;

&lt;p&gt;Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml&lt;/p&gt;

&lt;h2 id=&#34;step-4-log-into-your-webserver&#34;&gt;Step 4: Log into your webserver&lt;/h2&gt;

&lt;p&gt;Now that your Airflow instance is running let&amp;rsquo;s take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WEB=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39; | grep &amp;quot;airflow&amp;quot; | head -1)
kubectl port-forward $WEB 8080:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the Airflow UI will exist on &lt;a href=&#34;http://localhost:8080&#34; target=&#34;_blank&#34;&gt;http://localhost:8080&lt;/a&gt;. To log in simply enter &lt;code&gt;airflow&lt;/code&gt;/&lt;code&gt;airflow&lt;/code&gt; and you should have full access to the Airflow web UI.&lt;/p&gt;

&lt;h2 id=&#34;step-5-upload-a-test-document&#34;&gt;Step 5: Upload a test document&lt;/h2&gt;

&lt;p&gt;To modify/add your own DAGs, you can use &lt;code&gt;kubectl cp&lt;/code&gt; to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl cp &amp;lt;local file&amp;gt; &amp;lt;namespace&amp;gt;/&amp;lt;pod&amp;gt;:/root/airflow/dags -c scheduler&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-6-enjoy&#34;&gt;Step 6: Enjoy!&lt;/h2&gt;

&lt;h1 id=&#34;so-when-will-i-be-able-to-use-this&#34;&gt;So when will I be able to use this?&lt;/h1&gt;

&lt;p&gt;While this feature is still in the early stages, we hope to see it released for wide release in the next few months.&lt;/p&gt;

&lt;h1 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h1&gt;

&lt;p&gt;This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the &lt;a href=&#34;https://github.com/apache/incubator-airflow/tree/v1-10-test&#34; target=&#34;_blank&#34;&gt;1.10 release branch of Airflow&lt;/a&gt; (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributers can have a huge influence on the future of these features.&lt;/p&gt;

&lt;p&gt;For those interested in joining these efforts, I&amp;rsquo;d recommend checkint out these steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Join the airflow-dev mailing list at dev@airflow.apache.org.&lt;/li&gt;
&lt;li&gt;File an issue in &lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues/&#34; target=&#34;_blank&#34;&gt;Apache Airflow JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join our SIG-BigData meetings on Wednesdays at 10am PST.&lt;/li&gt;
&lt;li&gt;Reach us on slack at #sig-big-data on kubernetes.slack.com&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.11: In-Cluster Load Balancing and CoreDNS Plugin Graduate to General Availability</title>
      <link>https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Kubernetes 1.11 &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md&#34; target=&#34;_blank&#34;&gt;Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.11, our second release of 2018!&lt;/p&gt;

&lt;p&gt;Today’s release continues to advance maturity, scalability, and flexibility of Kubernetes, marking significant progress on features that the team has been hard at work on over the last year. This newest version graduates key features in networking, opens up two major features from SIG-API Machinery and SIG-Node for beta testing, and continues to enhance storage features that have been a focal point of the past two releases. The features in this release make it increasingly possible to plug any infrastructure, cloud or on-premise, into the Kubernetes system.&lt;/p&gt;

&lt;p&gt;Notable additions in this release include two highly-anticipated features graduating to general availability: IPVS-based In-Cluster Load Balancing and CoreDNS as a cluster DNS add-on option, which means increased scalability and flexibility for production applications.&lt;/p&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;ipvs-based-in-cluster-service-load-balancing-graduates-to-general-availability&#34;&gt;IPVS-Based In-Cluster Service Load Balancing Graduates to General Availability&lt;/h2&gt;

&lt;p&gt;In this release, &lt;a href=&#34;https://github.com/kubernetes/features/issues/265&#34; target=&#34;_blank&#34;&gt;IPVS-based in-cluster service load balancing&lt;/a&gt; has moved to stable. IPVS (IP Virtual Server) provides high-performance in-kernel load balancing, with a simpler programming interface than iptables. This change delivers better network throughput, better programming latency, and higher scalability limits for the cluster-wide distributed load-balancer that comprises the Kubernetes Service model. IPVS is not yet the default but clusters can begin to use it for production traffic.&lt;/p&gt;

&lt;h2 id=&#34;coredns-promoted-to-general-availability&#34;&gt;CoreDNS Promoted to General Availability&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://coredns.io&#34; target=&#34;_blank&#34;&gt;CoreDNS&lt;/a&gt; is now available as a &lt;a href=&#34;https://github.com/kubernetes/features/issues/427&#34; target=&#34;_blank&#34;&gt;cluster DNS add-on option&lt;/a&gt;, and is the default when using kubeadm. CoreDNS is a flexible, extensible authoritative DNS server and directly integrates with the Kubernetes API. CoreDNS has fewer moving parts than the previous DNS server, since it’s a single executable and a single process, and supports flexible use cases by creating custom DNS entries. It’s also written in Go making it memory-safe. You can learn more about CoreDNS &lt;a href=&#34;https://youtu.be/dz9S7R8r5gw&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;dynamic-kubelet-configuration-moves-to-beta&#34;&gt;Dynamic Kubelet Configuration Moves to Beta&lt;/h2&gt;

&lt;p&gt;This feature makes it possible for new Kubelet configurations to be rolled out in a live cluster.  Currently, Kubelets are configured via command-line flags, which makes it difficult to update Kubelet configurations in a running cluster. With this beta feature, &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/&#34; target=&#34;_blank&#34;&gt;users can configure Kubelets in a live cluster&lt;/a&gt; via the API server.&lt;/p&gt;

&lt;h2 id=&#34;custom-resource-definitions-can-now-define-multiple-versions&#34;&gt;Custom Resource Definitions Can Now Define Multiple Versions&lt;/h2&gt;

&lt;p&gt;Custom Resource Definitions are no longer restricted to defining a single version of the custom resource, a restriction that was difficult to work around. Now, with this beta &lt;a href=&#34;https://github.com/kubernetes/features/issues/544&#34; target=&#34;_blank&#34;&gt;feature&lt;/a&gt;, multiple versions of the resource can be defined. In the future, this will be expanded to support some automatic conversions; for now, this feature allows custom resource authors to “promote with safe changes, e.g. v1beta1 to v1,” and to create a migration path for resources which do have changes.&lt;/p&gt;

&lt;p&gt;Custom Resource Definitions now also support &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/customresources-subresources.md&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;status&amp;rdquo; and &amp;ldquo;scale&amp;rdquo; subresources&lt;/a&gt;, which integrate with monitoring and high-availability frameworks. These two changes advance the ability to run cloud-native applications in production using Custom Resource Definitions.&lt;/p&gt;

&lt;h2 id=&#34;enhancements-to-csi&#34;&gt;Enhancements to CSI&lt;/h2&gt;

&lt;p&gt;Container Storage Interface (CSI) has been a major topic over the last few releases. After moving to &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;beta in 1.10&lt;/a&gt;, the 1.11 release continues enhancing CSI with a number of features. The 1.11 release adds alpha support for raw block volumes to CSI, integrates CSI with the new kubelet plugin registration mechanism, and makes it easier to pass secrets to CSI plugins.&lt;/p&gt;

&lt;h2 id=&#34;new-storage-features&#34;&gt;New Storage Features&lt;/h2&gt;

&lt;p&gt;Support for &lt;a href=&#34;https://github.com/kubernetes/features/issues/284&#34; target=&#34;_blank&#34;&gt;online resizing of Persistent Volumes&lt;/a&gt; has been introduced as an alpha feature. This enables users to increase the size of PVs without having to terminate pods and unmount volume first. The user will update the PVC to request a new size and kubelet will resize the file system for the PVC.&lt;/p&gt;

&lt;p&gt;Support for &lt;a href=&#34;https://github.com/kubernetes/features/issues/554&#34; target=&#34;_blank&#34;&gt;dynamic maximum volume count&lt;/a&gt; has been introduced as an alpha feature. This new feature enables in-tree volume plugins to specify the maximum number of volumes that can be attached to a node and allows the limit to vary depending on the type of node. Previously, these limits were hard coded or configured via an environment variable.&lt;/p&gt;

&lt;p&gt;The StorageObjectInUseProtection feature is now stable and prevents the removal of both &lt;a href=&#34;https://github.com/kubernetes/features/issues/499&#34; target=&#34;_blank&#34;&gt;Persistent Volumes&lt;/a&gt; that are bound to a Persistent Volume Claim, and &lt;a href=&#34;https://github.com/kubernetes/features/issues/498&#34; target=&#34;_blank&#34;&gt;Persistent Volume Claims&lt;/a&gt; that are being used by a pod. This safeguard will help prevent issues from deleting a PV or a PVC that is currently tied to an active pod.&lt;/p&gt;

&lt;p&gt;Each Special Interest Group (SIG) within the community continues to deliver the most-requested enhancements, fixes, and functionality for their respective specialty areas. For a complete list of inclusions by SIG, please visit the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#111-release-notes&#34; target=&#34;_blank&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.11 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.11.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can also install 1.11 using Kubeadm. Version 1.11.0 will be available as Deb and RPM packages, installable using the &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;Kubeadm cluster installer&lt;/a&gt; sometime on June 28th.&lt;/p&gt;

&lt;h2 id=&#34;5-day-features-blog-series&#34;&gt;5 Day Features Blog Series&lt;/h2&gt;

&lt;p&gt;If you’re interested in exploring these features more in depth, check back in two weeks for our 5 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Day 1: IPVS-Based In-Cluster Service Load Balancing Graduates to General Availability&lt;/li&gt;
&lt;li&gt;Day 2: CoreDNS Promoted to General Availability&lt;/li&gt;
&lt;li&gt;Day 3: Dynamic Kubelet Configuration Moves to Beta&lt;/li&gt;
&lt;li&gt;Day 4: Custom Resource Definitions Can Now Define Multiple Versions&lt;/li&gt;
&lt;li&gt;Day 5: Overview of CSI Enhancements&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Josh Berkus, Kubernetes Community Manager at Red Hat. The 20 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has over 20,000 individual contributors to date and an active community of more than 40,000 people.&lt;/p&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average, 250 different companies and over 1,300 individuals contribute to Kubernetes each month. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;h2 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h2&gt;

&lt;p&gt;Established, global organizations are using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at massive scale. Recently published user stories from the community include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The New York Times&lt;/strong&gt;, known as the newspaper of record, &lt;a href=&#34;https://kubernetes.io/case-studies/newyorktimes/&#34; target=&#34;_blank&#34;&gt;moved out of its data centers and into the public cloud&lt;/a&gt; with the help of Google Cloud Platform and Kubernetes. This move meant a significant increase in speed of delivery, from 45 minutes to just a few seconds with Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nordstrom&lt;/strong&gt;, a leading fashion retailer based in the U.S., began their cloud native journey by &lt;a href=&#34;https://kubernetes.io/case-studies/nordstrom/&#34; target=&#34;_blank&#34;&gt;adopting Docker containers orchestrated with Kubernetes&lt;/a&gt;. The results included a major increase in Ops efficiency, improving CPU utilization from 5x to 12x depending on the workload.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Squarespace&lt;/strong&gt;, a SaaS solution for easily building and hosting websites, &lt;a href=&#34;https://kubernetes.io/case-studies/squarespace/&#34; target=&#34;_blank&#34;&gt;moved their monolithic application to microservices with the help of Kubernetes&lt;/a&gt;. This resulted in a deployment time reduction of almost 85%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Crowdfire&lt;/strong&gt;, a leading social media management platform, moved from a monolithic application to a &lt;a href=&#34;https://kubernetes.io/case-studies/crowdfire/&#34; target=&#34;_blank&#34;&gt;custom Kubernetes-based setup&lt;/a&gt;. This move reduced deployment time from 15 minutes to less than a minute.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is Kubernetes helping your team? Share your story with the community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The CNCF recently expanded its certification offerings to include a Certified Kubernetes Application Developer exam. The CKAD exam certifies an individual&amp;rsquo;s ability to design, build, configure, and expose cloud native applications for Kubernetes. More information can be found &lt;a href=&#34;https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The CNCF recently added a new partner category, Kubernetes Training Partners (KTP). KTPs are a tier of vetted training providers who have deep experience in cloud native technology training. View partners and learn more &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CNCF also offers &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;online training&lt;/a&gt; that teaches the skills needed to create and configure a real-world Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Kubernetes documentation now features &lt;a href=&#34;https://k8s.io/docs/home/&#34; target=&#34;_blank&#34;&gt;user journeys&lt;/a&gt;: specific pathways for learning based on who readers are and what readers want to do. Learning Kubernetes is easier than ever for beginners, and more experienced users can find task journeys specific to cluster admins and application developers.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubecon&#34;&gt;KubeCon&lt;/h2&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to &lt;a href=&#34;https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/&#34; target=&#34;_blank&#34;&gt;Shanghai&lt;/a&gt; from November 14-15, 2018 and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/&#34; target=&#34;_blank&#34;&gt;Seattle&lt;/a&gt; from December 11-13, 2018. This conference will feature technical sessions, case studies, developer deep dives, salons and more! The CFP for both event is currently open. &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/program/call-for-proposals-cfp/&#34; target=&#34;_blank&#34;&gt;Submit your talk&lt;/a&gt; and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/attend/register/&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.11 release team on July 31st at 10am PDT to learn about the major features in this release including In-Cluster Load Balancing and the CoreDNS Plugin. Register &lt;a href=&#34;https://www.cncf.io/event/webinar-kubernetes-1-11/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Chat with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Dynamic Ingress in Kubernetes</title>
      <link>https://kubernetes.io/blog/2018/06/07/dynamic-ingress-in-kubernetes/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/06/07/dynamic-ingress-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Richard Li (Datawire)&lt;/p&gt;

&lt;p&gt;Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services.  One approach is &lt;a href=&#34;https://www.getambassador.io&#34; target=&#34;_blank&#34;&gt;Ambassador&lt;/a&gt;, a Kubernetes-native open source API Gateway built on the &lt;a href=&#34;https://www.envoyproxy.io&#34; target=&#34;_blank&#34;&gt;Envoy Proxy&lt;/a&gt;. Ambassador is designed for dynamic environment where services may come and go frequently.&lt;/p&gt;

&lt;p&gt;Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.&lt;/p&gt;

&lt;h2 id=&#34;a-basic-ambassador-example&#34;&gt;A Basic Ambassador Example&lt;/h2&gt;

&lt;p&gt;Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: httpbin
  annotations:
    getambassador.io/config: |
      ---
      apiVersion: ambassador/v0
      kind:  Mapping
      name:  httpbin_mapping
      prefix: /httpbin/
      service: httpbin.org:80
      host_rewrite: httpbin.org
spec:
  type: ClusterIP
  ports:
    - port: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP &lt;code&gt;host&lt;/code&gt; header should be set to httpbin.org.&lt;/p&gt;

&lt;h2 id=&#34;kubeflow&#34;&gt;Kubeflow&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow&#34; target=&#34;_blank&#34;&gt;Kubeflow&lt;/a&gt; provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-06-01-dynamic-ingress-kubernetes/kubeflow.png&#34; alt=&#34;kubeflow&#34; /&gt;
&lt;center&gt;&lt;i&gt;Kubeflow architecture, pre-Ambassador&lt;/center&gt;&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&#34;service-configuration&#34;&gt;Service configuration&lt;/h2&gt;

&lt;p&gt;With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: ambassador/v0
kind:  Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the “test” service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.&lt;/p&gt;

&lt;h2 id=&#34;kubeflow-and-ambassador&#34;&gt;Kubeflow and Ambassador&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-06-01-dynamic-ingress-kubernetes/kubeflow-ambassador.png&#34; alt=&#34;kubeflow-ambassador&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic  to specific backends. For example, when deploying TensorFlow services,  Kubeflow creates and and annotates a K8s service so that the model will be served at https://&lt;ingress host&gt;/models/&lt;model name&gt;/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.&lt;/p&gt;

&lt;p&gt;If you’re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.&lt;/p&gt;

&lt;p&gt;If you’re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the &lt;a href=&#34;https://www.getambassador.io/user-guide/getting-started&#34; target=&#34;_blank&#34;&gt;Getting Started with Ambassador guide&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 4 Years of K8s</title>
      <link>https://kubernetes.io/blog/2018/06/06/4-years-of-k8s/</link>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/06/06/4-years-of-k8s/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Joe Beda (CTO and Founder, Heptio)&lt;/p&gt;

&lt;p&gt;On June 6, 2014 I checked in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56&#34; target=&#34;_blank&#34;&gt;first commit&lt;/a&gt; of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesn’t tell the whole story.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png&#34; alt=&#34;k8s_first_commit&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.&lt;/p&gt;

&lt;p&gt;Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.&lt;/p&gt;

&lt;p&gt;Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.&lt;/p&gt;

&lt;p&gt;After we got the nod, it was time to actually build the system.  We took Brendan’s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across.  By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith.  Once we had something working, someone had to sign up to clean things up to get it ready for public launch.  That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in.  So while I have the first public commit to the repo, there was work underway well before that.&lt;/p&gt;

&lt;p&gt;The version of Kubernetes at that point was really just a shadow of what it was to become.  The core concepts were there but it was very raw.  For example, Pods were called Tasks.  That was changed a day before we went public.  All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon.  You can watch that video here:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YrxnVKZeqK8&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger.  Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt.  The success the project has seen  is based not just on code and technology but also the way that an amazing group of people have come together to create something special.  The best expression of this is the &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/values.md&#34; target=&#34;_blank&#34;&gt;set of Kubernetes values&lt;/a&gt; that Sarah Novotny helped curate.&lt;/p&gt;

&lt;p&gt;Here is to another 4 years and beyond! 🎉🎉🎉&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Say Hello to Discuss Kubernetes</title>
      <link>https://kubernetes.io/blog/2018/05/30/say-hello-to-discuss-kubernetes/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/30/say-hello-to-discuss-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jorge Castro (Heptio)&lt;/p&gt;

&lt;p&gt;Communication is key when it comes to engaging a community of over 35,000 people in a global and remote environment. Keeping track of everything in the Kubernetes community can be an overwhelming task. On one hand we have our official resources, like Stack Overflow, GitHub, and the mailing lists, and on the other we have more ephemeral resources like Slack, where you can hop in, chat with someone, and then go on your merry way.&lt;/p&gt;

&lt;p&gt;Slack is great for casual and timely conversations and keeping up with other community members, but communication can&amp;rsquo;t be easily referenced in the future. Plus it can be hard to raise your hand in a room filled with 35,000 participants and find a voice. Mailing lists are useful when trying to reach a specific group of people with a particular ask and want to keep track of responses on the thread, but can be daunting with a large amount of people. Stack Overflow and GitHub are ideal for collaborating on projects or questions that involve code and need to be searchable in the future, but certain topics like &amp;ldquo;What&amp;rsquo;s your favorite CI/CD tool&amp;rdquo; or &amp;ldquo;&lt;a href=&#34;https://discuss.kubernetes.io/t/kubectl-tips-and-tricks/192&#34; target=&#34;_blank&#34;&gt;Kubectl tips and tricks&lt;/a&gt;&amp;rdquo; are offtopic there.&lt;/p&gt;

&lt;p&gt;While our current assortment of communication channels are valuable in their own rights, we found that there was still a gap between email and real time chat. Across the rest of the web, many other open source projects like Docker, Mozilla, Swift, Ghost, and Chef have had success building communities on top of &lt;a href=&#34;https://www.discourse.org/features&#34; target=&#34;_blank&#34;&gt;Discourse&lt;/a&gt;, an open source discussion platform. So what if we could use this tool to bring our discussions together under a modern roof, with an open API, and perhaps not let so much of our information fade into the ether? There&amp;rsquo;s only one way to find out: Welcome to &lt;a href=&#34;https://discuss.kubernetes.io&#34; target=&#34;_blank&#34;&gt;discuss.kubernetes.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-30-say-hello-to-discuss-kubernetes.png&#34; alt=&#34;discuss_screenshot&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Right off the bat we have categories that users can browse. Checking and posting in these categories allow users to participate in things they might be interested in without having to commit to subscribing to a list. Granular notification controls allow the users to subscribe to just the category or tag they want, and allow for responding to topics via email.&lt;/p&gt;

&lt;p&gt;Ecosystem partners and developers now have a place where they can &lt;a href=&#34;https://discuss.kubernetes.io/c/announcements&#34; target=&#34;_blank&#34;&gt;announce projects&lt;/a&gt; that they&amp;rsquo;re working on to users without wondering if it would be offtopic on an official list. We can make this place be not just about core Kubernetes, but about the hundreds of wonderful tools our community is building.&lt;/p&gt;

&lt;p&gt;This new community forum gives people a place to go where they can discuss Kubernetes, and a sounding board for developers to make announcements of things happening around Kubernetes, all while being searchable and easily accessible to a wider audience.&lt;/p&gt;

&lt;p&gt;Hop in and take a look. We&amp;rsquo;re just getting started, so you might want to begin by &lt;a href=&#34;https://discuss.kubernetes.io/t/introduce-yourself-here/56&#34; target=&#34;_blank&#34;&gt;introducing yourself&lt;/a&gt; and then browsing around. Apps are also available for &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.discourse&amp;amp;hl=en_US&amp;amp;rdid=com.discourse&amp;amp;pli=1&#34; target=&#34;_blank&#34;&gt;Android &lt;/a&gt;and &lt;a href=&#34;https://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8&#34; target=&#34;_blank&#34;&gt;iOS&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing kustomize; Template-free Configuration Customization for Kubernetes</title>
      <link>https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jeff Regan (Google), Phil Wittrock (Google)&lt;/p&gt;

&lt;p&gt;If you run a Kubernetes environment, chances are you’ve
customized a Kubernetes configuration — you&amp;rsquo;ve copied
some API object YAML files and editted them to suit
your needs.&lt;/p&gt;

&lt;p&gt;But there are drawbacks to this approach — it can be
hard to go back to the source material and incorporate
any improvements that were made to it. Today Google is
announcing &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;kustomize&lt;/strong&gt;&lt;/a&gt;, a command-line tool
contributed as a &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/keps/sig-cli/0008-kustomize.md&#34; target=&#34;_blank&#34;&gt;subproject&lt;/a&gt; of &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cli&#34; target=&#34;_blank&#34;&gt;SIG-CLI&lt;/a&gt;.  The tool
provides a new, purely &lt;em&gt;declarative&lt;/em&gt; approach to
configuration customization that adheres to and
leverages the familiar and carefully designed
Kubernetes API.&lt;/p&gt;

&lt;p&gt;Here’s a common scenario. Somewhere on the internet you
find someone’s Kubernetes configuration for a content
management system.  It&amp;rsquo;s a set of files containing YAML
specifications of Kubernetes API objects. Then, in some
corner of your own company you find a configuration for
a database to back that CMS — a database you prefer
because you know it well.&lt;/p&gt;

&lt;p&gt;You want to use these together, somehow. Further, you
want to customize the files so that your resource
instances appear in the cluster with a label that
distinguishes them from a colleague’s resources who’s
doing the same thing in the same cluster.
You also want to set appropriate values for CPU, memory
and replica count.&lt;/p&gt;

&lt;p&gt;Additionally, you’ll want &lt;em&gt;multiple variants&lt;/em&gt; of the
entire configuration: a small variant (in terms of
computing resources used) devoted to testing and
experimentation, and a much larger variant devoted to
serving outside users in production. Likewise, other
teams will want their own variants.&lt;/p&gt;

&lt;p&gt;This raises all sorts of questions.  Do you copy your
configuration to multiple locations and edit them
independently? What if you have dozens of development
teams who need slightly different variations of the
stack? How do you maintain and upgrade the aspects of
configuration that they share in common?  Workflows
using &lt;strong&gt;kustomize&lt;/strong&gt; provide answers to these questions.&lt;/p&gt;

&lt;h2 id=&#34;customization-is-reuse&#34;&gt;Customization is reuse&lt;/h2&gt;

&lt;p&gt;Kubernetes configurations aren&amp;rsquo;t code (being YAML
specifications of API objects, they are more strictly
viewed as data), but configuration lifecycle has many
similarities to code lifecycle.&lt;/p&gt;

&lt;p&gt;You should keep configurations in version
control. Configuration owners aren’t necessarily the
same set of people as configuration
users. Configurations may be used as parts of a larger
whole. Users will want to &lt;em&gt;reuse&lt;/em&gt; configurations for
different purposes.&lt;/p&gt;

&lt;p&gt;One approach to configuration reuse, as with code
reuse, is to simply copy it all and customize the
copy. As with code, severing the connection to the
source material makes it difficult to benefit from
ongoing improvements to the source material. Taking
this approach with many teams or environments, each
with their own variants of a configuration, makes a
simple upgrade intractable.&lt;/p&gt;

&lt;p&gt;Another approach to reuse is to express the source
material as a parameterized template.  A tool processes
the template—executing any embedded scripting and
replacing parameters with desired values—to generate
the configuration. Reuse comes from using different
sets of values with the same template. The challenge
here is that the templates and value files are not
specifications of Kubernetes API resources. They are,
necessarily, a new thing, a new language, that wraps
the Kubernetes API. And yes, they can be powerful, but
bring with them learning and tooling costs. Different
teams want different changes—so almost every
specification that you can include in a YAML file
becomes a parameter that needs a value. As a result,
the value sets get large, since all parameters (that
don&amp;rsquo;t have trusted defaults) must be specified for
replacement. This defeats one of the goals of
reuse—keeping the differences between the variants
small in size and easy to understand in the absence of
a full resource declaration.&lt;/p&gt;

&lt;h2 id=&#34;a-new-option-for-configuration-customization&#34;&gt;A new option for configuration customization&lt;/h2&gt;

&lt;p&gt;Compare that to &lt;strong&gt;kustomize&lt;/strong&gt;, where the tool’s
behavior is determined by declarative specifications
expressed in a file called &lt;code&gt;kustomization.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;kustomize&lt;/strong&gt; program reads the file and the
Kubernetes API resource files it references, then emits
complete resources to standard output. This text output
can be further processed by other tools, or streamed
directly to &lt;strong&gt;kubectl&lt;/strong&gt; for application to a cluster.&lt;/p&gt;

&lt;p&gt;For example, if a file called &lt;code&gt;kustomization.yaml&lt;/code&gt;
containing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   commonLabels:
     app: hello
   resources:
   - deployment.yaml
   - configMap.yaml
   - service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is in the current working directory, along with
the three resource files it mentions, then running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kustomize build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;emits a YAML stream that includes the three given
resources, and adds a common label &lt;code&gt;app: hello&lt;/code&gt; to
each resource.&lt;/p&gt;

&lt;p&gt;Similarly, you can use a &lt;em&gt;commonAnnotations&lt;/em&gt; field to
add an annotation to all resources, and a &lt;em&gt;namePrefix&lt;/em&gt;
field to add a common prefix to all resource
names. This trivial yet common customization is just
the beginning.&lt;/p&gt;

&lt;p&gt;A more common use case is that you’ll need multiple
variants of a common set of resources, e.g., a
&lt;em&gt;development&lt;/em&gt;, &lt;em&gt;staging&lt;/em&gt; and &lt;em&gt;production&lt;/em&gt; variant.&lt;/p&gt;

&lt;p&gt;For this purpose, &lt;strong&gt;kustomize&lt;/strong&gt; supports the idea of an
&lt;em&gt;overlay&lt;/em&gt; and a &lt;em&gt;base&lt;/em&gt;. Both are represented by a
kustomization file. The base declares things that the
variants share in common (both resources and a common
customization of those resources), and the overlays
declare the differences.&lt;/p&gt;

&lt;p&gt;Here’s a file system layout to manage a &lt;em&gt;staging&lt;/em&gt; and
&lt;em&gt;production&lt;/em&gt; variant of a given cluster app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   someapp/
   ├── base/
   │   ├── kustomization.yaml
   │   ├── deployment.yaml
   │   ├── configMap.yaml
   │   └── service.yaml
   └── overlays/
      ├── production/
      │   └── kustomization.yaml
      │   ├── replica_count.yaml
      └── staging/
          ├── kustomization.yaml
          └── cpu_count.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The file &lt;code&gt;someapp/base/kustomization.yaml&lt;/code&gt; specifies the
common resources and common customizations to those
resources (e.g., they all get some label, name prefix
and annotation).&lt;/p&gt;

&lt;p&gt;The contents of
&lt;code&gt;someapp/overlays/production/kustomization.yaml&lt;/code&gt; could
be&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   commonLabels:
    env: production
   bases:
   - ../../base
   patches:
   - replica_count.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This kustomization specifies a &lt;em&gt;patch&lt;/em&gt; file
&lt;code&gt;replica_count.yaml&lt;/code&gt;, which could be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: the-deployment
   spec:
     replicas: 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A patch is a partial resource declaration, in this case
a patch of the deployment in
&lt;code&gt;someapp/base/deployment.yaml&lt;/code&gt;, modifying only the
&lt;em&gt;replicas&lt;/em&gt; count to handle production traffic.&lt;/p&gt;

&lt;p&gt;The patch, being a partial deployment spec, has a clear
context and purpose and can be validated even if it’s
read in isolation from the remaining
configuration. It’s not just a context free &lt;em&gt;{parameter
name, value}&lt;/em&gt; tuple.&lt;/p&gt;

&lt;p&gt;To create the resources for the production variant, run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kustomize build someapp/overlays/production
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is printed to stdout as a set of complete
resources, ready to be applied to a cluster.  A
similar command defines the staging environment.&lt;/p&gt;

&lt;h2 id=&#34;in-summary&#34;&gt;In summary&lt;/h2&gt;

&lt;p&gt;With &lt;strong&gt;kustomize&lt;/strong&gt;, you can manage an arbitrary number
of distinctly customized Kubernetes configurations
using only Kubernetes API resource files. Every
artifact that &lt;strong&gt;kustomize&lt;/strong&gt; uses is plain YAML and can
be validated and processed as such.  kustomize encourages
a fork/modify/rebase &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/docs/workflows.md&#34; target=&#34;_blank&#34;&gt;workflow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To get started, try the &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/examples/helloWorld&#34; target=&#34;_blank&#34;&gt;hello world&lt;/a&gt; example.
For discussion and feedback, join the &lt;a href=&#34;https://groups.google.com/forum/#!forum/kustomize&#34; target=&#34;_blank&#34;&gt;mailing list&lt;/a&gt; or
&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/issues/new&#34; target=&#34;_blank&#34;&gt;open an issue&lt;/a&gt;.  Pull requests are welcome.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Containerd Integration Goes GA</title>
      <link>https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/</guid>
      <description>
        
        
        

&lt;h1 id=&#34;kubernetes-containerd-integration-goes-ga&#34;&gt;Kubernetes Containerd Integration Goes GA&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Lantao Liu, Software Engineer, Google and Mike Brown, Open Source Developer Advocate, IBM&lt;/p&gt;

&lt;p&gt;In a previous blog - &lt;a href=&#34;https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes&#34; target=&#34;_blank&#34;&gt;Containerd Brings More Container Runtime Options for Kubernetes&lt;/a&gt;, we introduced the alpha version of the Kubernetes containerd integration. With another 6 months of development, the integration with containerd is now generally available! You can now use &lt;a href=&#34;https://github.com/containerd/containerd/releases/tag/v1.1.0&#34; target=&#34;_blank&#34;&gt;containerd 1.1&lt;/a&gt; as the container runtime for production Kubernetes clusters!&lt;/p&gt;

&lt;p&gt;Containerd 1.1 works with Kubernetes 1.10 and above, and supports all Kubernetes features. The test coverage of containerd integration on &lt;a href=&#34;https://cloud.google.com/&#34; target=&#34;_blank&#34;&gt;Google Cloud Platform&lt;/a&gt; in Kubernetes test infrastructure is now equivalent to the Docker integration (See: &lt;a href=&#34;https://k8s-testgrid.appspot.com/sig-node-containerd&#34; target=&#34;_blank&#34;&gt;test dashboard)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We&amp;rsquo;re very glad to see containerd rapidly grow to this big milestone. Alibaba Cloud started to use containerd actively since its first day, and thanks to the simplicity and robustness emphasise, make it a perfect container engine running in our Serverless Kubernetes product, which has high qualification on performance and stability. No doubt, containerd will be a core engine of container era, and continue to driving innovation forward.&lt;/em&gt;&lt;/p&gt;

&lt;p style=&#34;text-align: right&#34;&gt;
&lt;em&gt;— Xinwei, Staff Engineer in Alibaba Cloud&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&#34;architecture-improvements&#34;&gt;Architecture Improvements&lt;/h1&gt;

&lt;p&gt;The Kubernetes containerd integration architecture has evolved twice. Each evolution has made the stack more stable and efficient.&lt;/p&gt;

&lt;h2 id=&#34;containerd-1-0-cri-containerd-end-of-life&#34;&gt;Containerd 1.0 - CRI-Containerd (end of life)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png&#34; width=&#34;100%&#34; alt=&#34;cri-containerd architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For containerd 1.0, a daemon called cri-containerd was required to operate between Kubelet and containerd. Cri-containerd handled the &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&#34; target=&#34;_blank&#34;&gt;Container Runtime Interface (CRI)&lt;/a&gt; service requests from Kubelet and used containerd to manage containers and container images correspondingly. Compared to the Docker CRI implementation (&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/v1.10.2/pkg/kubelet/dockershim&#34; target=&#34;_blank&#34;&gt;dockershim&lt;/a&gt;), this eliminated one extra hop in the stack.&lt;/p&gt;

&lt;p&gt;However, cri-containerd and containerd 1.0 were still 2 different daemons which interacted via grpc. The extra daemon in the loop made it more complex for users to understand and deploy, and introduced unnecessary communication overhead.&lt;/p&gt;

&lt;h2 id=&#34;containerd-1-1-cri-plugin-current&#34;&gt;Containerd 1.1 - CRI Plugin (current)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/containerd.png&#34; width=&#34;100%&#34; alt=&#34;containerd architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In containerd 1.1, the cri-containerd daemon is now refactored to be a containerd CRI plugin. The CRI plugin is built into containerd 1.1, and enabled by default. Unlike cri-containerd, the CRI plugin interacts with containerd through direct function calls. This new architecture makes the integration more stable and efficient, and eliminates another grpc hop in the stack. Users can now use Kubernetes with containerd 1.1 directly. The cri-containerd daemon is no longer needed.&lt;/p&gt;

&lt;h1 id=&#34;performance&#34;&gt;Performance&lt;/h1&gt;

&lt;p&gt;Improving performance was one of the major focus items for the containerd 1.1 release. Performance was optimized in terms of pod startup latency and daemon resource usage.&lt;/p&gt;

&lt;p&gt;The following results are a comparison between containerd 1.1 and Docker 18.03 CE. The containerd 1.1 integration uses the CRI plugin built into containerd; and the Docker 18.03 CE integration uses the dockershim.&lt;/p&gt;

&lt;p&gt;The results were generated using the Kubernetes node performance benchmark, which is part of &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-node-tests.md&#34; target=&#34;_blank&#34;&gt;Kubernetes node e2e test&lt;/a&gt;. Most of the containerd benchmark data is publicly accessible on the &lt;a href=&#34;http://node-perf-dash.k8s.io/&#34; target=&#34;_blank&#34;&gt;node performance dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;pod-startup-latency&#34;&gt;Pod Startup Latency&lt;/h3&gt;

&lt;p&gt;The &amp;ldquo;105 pod batch startup benchmark&amp;rdquo; results show that the containerd 1.1 integration has lower pod startup latency than Docker 18.03 CE integration with dockershim (lower is better).&lt;/p&gt;

&lt;p style=&#34;text-align:center;&#34;&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/latency.png&#34; alt=&#34;latency&#34;/&gt;&lt;/p&gt;

&lt;h3 id=&#34;cpu-and-memory&#34;&gt;CPU and Memory&lt;/h3&gt;

&lt;p&gt;At the steady state, with 105 pods, the containerd 1.1 integration consumes less CPU and memory overall compared to Docker 18.03 CE integration with dockershim. The results vary with the number of pods running on the node, 105 is chosen because it is the current default for the maximum number of user pods per node.&lt;/p&gt;

&lt;p&gt;As shown in the figures below, compared to Docker 18.03 CE integration with dockershim, the containerd 1.1 integration has 30.89% lower kubelet cpu usage, 68.13% lower container runtime cpu usage, 11.30% lower kubelet resident set size (RSS) memory usage, 12.78% lower container runtime RSS memory usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cpu.png&#34; alt=&#34;cpu&#34; width=&#34;50%&#34;/&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/memory.png&#34; alt=&#34;memory&#34; width=&#34;50%&#34;/&gt;&lt;/p&gt;

&lt;h1 id=&#34;crictl&#34;&gt;crictl&lt;/h1&gt;

&lt;p&gt;Container runtime command-line interface (CLI) is a useful tool for system and application troubleshooting. When using Docker as the container runtime for Kubernetes, system administrators sometimes login to the Kubernetes node to run Docker commands for collecting system and/or application information. For example, one may use &lt;em&gt;docker ps&lt;/em&gt; and &lt;em&gt;docker inspect&lt;/em&gt; to check application process status, &lt;em&gt;docker images&lt;/em&gt; to list images on the node, and &lt;em&gt;docker info&lt;/em&gt; to identify container runtime configuration, etc.&lt;/p&gt;

&lt;p&gt;For containerd and all other CRI-compatible container runtimes, e.g. dockershim, we recommend using &lt;em&gt;crictl&lt;/em&gt; as a replacement CLI over the Docker CLI for troubleshooting pods, containers, and container images on Kubernetes nodes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;crictl&lt;/em&gt; is a tool providing a similar experience to the Docker CLI for Kubernetes node troubleshooting and &lt;em&gt;crictl&lt;/em&gt; works consistently across all CRI-compatible container runtimes. It is hosted in the &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-tools&#34; target=&#34;_blank&#34;&gt;kubernetes-incubator/cri-tools&lt;/a&gt; repository and the current version is &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-tools/releases/tag/v1.0.0-beta.1&#34; target=&#34;_blank&#34;&gt;v1.0.0-beta.1&lt;/a&gt;. &lt;em&gt;crictl&lt;/em&gt; is designed to resemble the Docker CLI to offer a better transition experience for users, but it is not exactly the same. There are a few important differences, explained below.&lt;/p&gt;

&lt;h2 id=&#34;limited-scope-crictl-is-a-troubleshooting-tool&#34;&gt;Limited Scope - crictl is a Troubleshooting Tool&lt;/h2&gt;

&lt;p&gt;The scope of &lt;em&gt;crictl&lt;/em&gt; is limited to troubleshooting, it is not a replacement to docker or kubectl. Docker&amp;rsquo;s CLI provides a rich set of commands, making it a very useful development tool. But it is not the best fit for troubleshooting on Kubernetes nodes. Some Docker commands are not useful to Kubernetes, such as &lt;em&gt;docker network&lt;/em&gt; and &lt;em&gt;docker build&lt;/em&gt;; and some may even break the system, such as &lt;em&gt;docker rename&lt;/em&gt;. &lt;em&gt;crictl&lt;/em&gt; provides just enough commands for node troubleshooting, which is arguably safer to use on production nodes.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-oriented&#34;&gt;Kubernetes Oriented&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;crictl&lt;/em&gt; offers a more kubernetes-friendly view of containers. Docker CLI lacks core Kubernetes concepts, e.g. &lt;em&gt;pod&lt;/em&gt; and &lt;em&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&#34; target=&#34;_blank&#34;&gt;namespace&lt;/a&gt;&lt;/em&gt;, so it can&amp;rsquo;t provide a clear view of containers and pods. One example is that &lt;em&gt;docker ps&lt;/em&gt; shows somewhat obscure, long Docker container names, and shows pause containers and application containers together:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/docker-ps.png&#34; width=&#34;100%&#34; alt=&#34;docker ps&#34; /&gt;&lt;/p&gt;

&lt;p&gt;However, &lt;a href=&#34;https://www.ianlewis.org/en/almighty-pause-container&#34; target=&#34;_blank&#34;&gt;pause containers&lt;/a&gt; are a pod implementation detail, where one pause container is used for each pod, and thus should not be shown when listing containers that are members of pods.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;crictl&lt;/em&gt;, by contrast, is designed for Kubernetes. It has different sets of commands for pods and containers. For example, &lt;em&gt;crictl pods&lt;/em&gt; lists pod information, and &lt;em&gt;crictl ps&lt;/em&gt; only lists application container information. All information is well formatted into table columns.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/crictl-pods.png&#34; width=&#34;100%&#34; alt=&#34;crictl pods&#34; /&gt;
&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/crictl-ps.png&#34; width=&#34;100%&#34; alt=&#34;crictl ps&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As another example, &lt;em&gt;crictl pods&lt;/em&gt; includes a &lt;em&gt;&amp;ndash;namespace&lt;/em&gt; option for filtering pods by the namespaces specified in Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/crictl-pods-filter.png&#34; width=&#34;100%&#34; alt=&#34;crictl pods filter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For more details about how to use &lt;em&gt;crictl&lt;/em&gt; with containerd:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/containerd/cri/blob/master/docs/crictl.md&#34; target=&#34;_blank&#34;&gt;Document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://asciinema.org/a/179047&#34; target=&#34;_blank&#34;&gt;Demo video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;what-about-docker-engine&#34;&gt;What about Docker Engine?&lt;/h1&gt;

&lt;p&gt;&amp;ldquo;Does switching to containerd mean I can&amp;rsquo;t use Docker Engine anymore?&amp;rdquo; We hear this question a lot, the short answer is NO.&lt;/p&gt;

&lt;p&gt;Docker Engine is built on top of containerd. The next release of &lt;a href=&#34;https://www.docker.com/community-edition&#34; target=&#34;_blank&#34;&gt;Docker Community Edition (Docker CE)&lt;/a&gt; will use containerd version 1.1. Of course, it will have the CRI plugin built-in and enabled by default. This means users will have the option to continue using Docker Engine for other purposes typical for Docker users, while also being able to configure Kubernetes to use the underlying containerd that came with and is simultaneously being used by Docker Engine on the same node. See the architecture figure below showing the same containerd being used by Docker Engine and Kubelet:&lt;/p&gt;

&lt;p style=&#34;text-align:center;&#34;&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/docker-ce.png&#34; width=&#34;60%&#34; alt=&#34;docker-ce&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Since containerd is being used by both Kubelet and Docker Engine, this means users who choose the containerd integration will not just get new Kubernetes features, performance, and stability improvements, they will also have the option of keeping Docker Engine around for other use cases.&lt;/p&gt;

&lt;p&gt;A containerd &lt;a href=&#34;https://github.com/containerd/containerd/blob/master/docs/namespaces.md&#34; target=&#34;_blank&#34;&gt;namespace&lt;/a&gt; mechanism is employed to guarantee that Kubelet and Docker Engine won&amp;rsquo;t see or have access to containers and images created by each other. This makes sure they won&amp;rsquo;t interfere with each other. This also means that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Users won&amp;rsquo;t see Kubernetes created containers with the &lt;em&gt;docker ps&lt;/em&gt; command. Please use &lt;em&gt;crictl ps&lt;/em&gt; instead. And vice versa, users won&amp;rsquo;t see Docker CLI created containers in Kubernetes or with &lt;em&gt;crictl ps&lt;/em&gt; command. The &lt;em&gt;crictl create&lt;/em&gt; and &lt;em&gt;crictl runp&lt;/em&gt; commands are only for troubleshooting. Manually starting pod or container with &lt;em&gt;crictl&lt;/em&gt; on production nodes is not recommended.&lt;/li&gt;
&lt;li&gt;Users won&amp;rsquo;t see Kubernetes pulled images with the &lt;em&gt;docker images&lt;/em&gt; command. Please use the &lt;em&gt;crictl images&lt;/em&gt; command instead. And vice versa, Kubernetes won&amp;rsquo;t see images created by &lt;em&gt;docker pull&lt;/em&gt;, &lt;em&gt;docker load&lt;/em&gt; or &lt;em&gt;docker build&lt;/em&gt; commands. Please use the &lt;em&gt;crictl pull&lt;/em&gt; command instead, and &lt;em&gt;&lt;a href=&#34;https://github.com/containerd/containerd/blob/master/docs/man/ctr.1.md&#34; target=&#34;_blank&#34;&gt;ctr&lt;/a&gt; cri load&lt;/em&gt; if you have to load an image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Containerd 1.1 natively supports CRI. It can be used directly by Kubernetes.&lt;/li&gt;
&lt;li&gt;Containerd 1.1 is production ready.&lt;/li&gt;
&lt;li&gt;Containerd 1.1 has good performance in terms of pod startup latency and system resource utilization.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;crictl&lt;/em&gt; is the CLI tool to talk with containerd 1.1 and other CRI-conformant container runtimes for node troubleshooting.&lt;/li&gt;
&lt;li&gt;The next stable release of Docker CE will include containerd 1.1. Users have the option to continue using Docker for use cases not specific to Kubernetes, and configure Kubernetes to use the same underlying containerd that comes with Docker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;d like to thank all the contributors from Google, IBM, Docker, ZTE, ZJU and many other individuals who made this happen!&lt;/p&gt;

&lt;p&gt;For a detailed list of changes in the containerd 1.1 release, please see the release notes here: &lt;a href=&#34;https://github.com/containerd/containerd/releases/tag/v1.1.0&#34; target=&#34;_blank&#34;&gt;https://github.com/containerd/containerd/releases/tag/v1.1.0&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;try-it-out&#34;&gt;Try it out&lt;/h1&gt;

&lt;p&gt;To setup a Kubernetes cluster using containerd as the container runtime:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For a production quality cluster on GCE brought up with kube-up.sh, see &lt;a href=&#34;https://github.com/containerd/cri/blob/v1.0.0/docs/kube-up.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For a multi-node cluster installer and bring up steps using ansible and kubeadm, see &lt;a href=&#34;https://github.com/containerd/cri/blob/v1.0.0/contrib/ansible/README.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For creating a cluster from scratch on Google Cloud, see &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kubernetes the Hard Way&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For a custom installation from release tarball, see &lt;a href=&#34;https://github.com/containerd/cri/blob/v1.0.0/docs/installation.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;To install using LinuxKit on a local VM, see &lt;a href=&#34;https://github.com/linuxkit/linuxkit/tree/master/projects/kubernetes&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;contribute&#34;&gt;Contribute&lt;/h1&gt;

&lt;p&gt;The containerd CRI plugin is an open source github project within containerd &lt;a href=&#34;https://github.com/containerd/cri&#34; target=&#34;_blank&#34;&gt;https://github.com/containerd/cri&lt;/a&gt;. Any contributions in terms of ideas, issues, and/or fixes are welcome. The &lt;a href=&#34;https://github.com/containerd/cri#getting-started-for-developers&#34; target=&#34;_blank&#34;&gt;getting started guide for developers&lt;/a&gt; is a good place to start for contributors.&lt;/p&gt;

&lt;h1 id=&#34;community&#34;&gt;Community&lt;/h1&gt;

&lt;p&gt;The project is developed and maintained jointly by members of the Kubernetes SIG-Node community and the containerd community. We&amp;rsquo;d love to hear feedback from you. To join the communities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node&#34; target=&#34;_blank&#34;&gt;sig-node community site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Slack:

&lt;ul&gt;
&lt;li&gt;#sig-node channel in &lt;a href=&#34;http://kubernetes.slack.com&#34; target=&#34;_blank&#34;&gt;kubernetes.slack.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;#containerd channel in &lt;a href=&#34;https://dockr.ly/community&#34; target=&#34;_blank&#34;&gt;https://dockr.ly/community&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-node&#34; target=&#34;_blank&#34;&gt;https://groups.google.com/forum/#!forum/kubernetes-sig-node&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Getting to Know Kubevirt</title>
      <link>https://kubernetes.io/blog/2018/05/22/getting-to-know-kubevirt/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/22/getting-to-know-kubevirt/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;mailto:jbrooks@redhat.com&#34; target=&#34;_blank&#34;&gt;Jason Brooks&lt;/a&gt; (Red Hat)&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve become accustomed to running Linux container workloads on Kubernetes, you may find yourself wishing that you could run other sorts of workloads on your Kubernetes cluster. Maybe you need to run an application that isn&amp;rsquo;t architected for containers, or that requires a different version of the Linux kernel &amp;ndash; or an all together different operating system &amp;ndash; than what&amp;rsquo;s available on your container host.&lt;/p&gt;

&lt;p&gt;These sorts of workloads are often well-suited to running in virtual machines (VMs), and &lt;a href=&#34;http://www.kubevirt.io/&#34; target=&#34;_blank&#34;&gt;KubeVirt&lt;/a&gt;, a virtual machine management add-on for Kubernetes, is aimed at allowing users to run VMs right alongside containers in the their Kubernetes or OpenShift clusters.&lt;/p&gt;

&lt;p&gt;KubeVirt extends Kubernetes by adding resource types for VMs and sets of VMs through Kubernetes&amp;rsquo; &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions&#34; target=&#34;_blank&#34;&gt;Custom Resource Definitions API&lt;/a&gt; (CRD). KubeVirt VMs run within regular Kubernetes pods, where they have access to standard pod networking and storage, and can be managed using standard Kubernetes tools such as kubectl.&lt;/p&gt;

&lt;p&gt;Running VMs with Kubernetes involves a bit of an adjustment compared to using something like oVirt or OpenStack, and understanding the basic architecture of KubeVirt is a good place to begin.&lt;/p&gt;

&lt;p&gt;In this post, we’ll talk about some of the components that are involved in KubeVirt at a high level. The components we’ll check out are CRDs, the KubeVirt virt-controller, virt-handler and virt-launcher components, libvirt, storage, and networking.&lt;/p&gt;

&lt;h2 id=&#34;kubevirt-components&#34;&gt;KubeVirt Components&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-22-getting-to-know-kubevirt/kubevirt-components.png&#34; width=&#34;70%&#34; alt=&#34;Kubevirt Components&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;custom-resource-definitions&#34;&gt;Custom Resource Definitions&lt;/h2&gt;

&lt;p&gt;Kubernetes resources are endpoints in the Kubernetes API that store collections of related API objects. For instance, the built-in pods resource contains a collection of Pod objects. The Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions&#34; target=&#34;_blank&#34;&gt;Custom Resource Definition&lt;/a&gt; API allows users to extend Kubernetes with additional resources by defining new objects with a given name and schema. Once you&amp;rsquo;ve applied a custom resource to your cluster, the Kubernetes API server serves and handles the storage of your custom resource.&lt;/p&gt;

&lt;p&gt;KubeVirt&amp;rsquo;s primary CRD is the VirtualMachine (VM) resource, which contains a collection of VM objects inside the Kubernetes API server. The VM resource defines all the properties of the Virtual machine itself, such as the machine and CPU type, the amount of RAM and vCPUs, and the number and type of NICs available in the VM.&lt;/p&gt;

&lt;h2 id=&#34;virt-controller&#34;&gt;virt-controller&lt;/h2&gt;

&lt;p&gt;The virt-controller is a Kubernetes &lt;a href=&#34;https://coreos.com/operators/&#34; target=&#34;_blank&#34;&gt;Operator&lt;/a&gt; that’s responsible for cluster-wide virtualization functionality. When new VM objects are posted to the Kubernetes API server, the virt-controller takes notice and creates the pod in which the VM will run. When the pod is scheduled on a particular node, the virt-controller updates the VM object with the node name, and hands off further responsibilities to a node-specific KubeVirt component, the virt-handler, an instance of which runs on every node in the cluster.&lt;/p&gt;

&lt;h2 id=&#34;virt-handler&#34;&gt;virt-handler&lt;/h2&gt;

&lt;p&gt;Like the virt-controller, the virt-handler is also reactive, watching for changes to the VM object, and performing all necessary operations to change a VM to meet the required state. The virt-handler references the VM specification and signals the creation of a corresponding domain using a libvirtd instance in the VM&amp;rsquo;s pod. When a VM object is deleted, the virt-handler observes the deletion and turns off the domain.&lt;/p&gt;

&lt;h2 id=&#34;virt-launcher&#34;&gt;virt-launcher&lt;/h2&gt;

&lt;p&gt;For every VM object one pod is created. This pod&amp;rsquo;s primary container runs the virt-launcher KubeVirt component. The main purpose of the virt-launcher Pod is to provide the cgroups and namespaces which will be used to host the VM process.&lt;/p&gt;

&lt;p&gt;virt-handler signals virt-launcher to start a VM by passing the VM&amp;rsquo;s CRD object to virt-launcher. virt-launcher then uses a local libvirtd instance within its container to start the VM. From there virt-launcher monitors the VM process and terminates once the VM has exited.&lt;/p&gt;

&lt;p&gt;If the Kubernetes runtime attempts to shutdown the virt-launcher pod before the VM has exited, virt-launcher forwards signals from Kubernetes to the VM process and attempts to hold off the termination of the pod until the VM has shutdown successfully.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl get pods

NAME                                   READY     STATUS        RESTARTS   AGE
virt-controller-7888c64d66-dzc9p   1/1       Running   0          2h
virt-controller-7888c64d66-wm66x   0/1       Running   0          2h
virt-handler-l2xkt                 1/1       Running   0          2h
virt-handler-sztsw                 1/1       Running   0          2h
virt-launcher-testvm-ephemeral-dph94   2/2       Running       0          2h
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;libvirtd&#34;&gt;libvirtd&lt;/h2&gt;

&lt;p&gt;An instance of libvirtd is present in every VM pod. virt-launcher uses libvirtd to manage the life-cycle of the VM process.&lt;/p&gt;

&lt;h2 id=&#34;storage-and-networking&#34;&gt;Storage and Networking&lt;/h2&gt;

&lt;p&gt;KubeVirt VMs may be configured with disks, backed by volumes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubevirt/kubevirt/blob/master/docs/direct-pv-disks.md&#34; target=&#34;_blank&#34;&gt;Persistent Volume Claim&lt;/a&gt; volumes make Kubernetes persistent volume available as disks directly attached to the VM. This is the primary way to provide KubeVirt VMs with persistent storage. Currently, persistent volumes must be iscsi block devices, although work is underway to enable &lt;a href=&#34;https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#persistentvolumeclaim&#34; target=&#34;_blank&#34;&gt;file-based pv disks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#ephemeral-volume&#34; target=&#34;_blank&#34;&gt;Ephemeral Volumes&lt;/a&gt; are a local copy on write images that use a network volume as a read-only backing store. KubeVirt dynamically generates the ephemeral images associated with a VM when the VM starts, and discards the ephemeral images when the VM stops. Currently, ephemeral volumes must be backed by pvc volumes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#registrydisk&#34; target=&#34;_blank&#34;&gt;Registry Disk&lt;/a&gt; volumes reference docker image that embed a qcow or raw disk. As the name suggests, these volumes are pulled from a container registry. Like regular ephemeral container images, data in these volumes persists only while the pod lives.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#cloudinitnocloud&#34; target=&#34;_blank&#34;&gt;CloudInit NoCloud&lt;/a&gt; volumes provide VMs with a cloud-init NoCloud user-data source, which is added as a disk to the VM, where it&amp;rsquo;s available to provide configuration details to guests with cloud-init installed. Cloud-init details can be provided in clear text, as base64 encoded UserData files, or via Kubernetes secrets.&lt;/p&gt;

&lt;p&gt;In the example below, a Registry Disk is configured to provide the image from which to boot the VM. A cloudInit NoCloud volume, paired with an ssh-key stored as clear text in the userData field, is provided for authentication with the VM:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: kubevirt.io/v1alpha1
kind: VirtualMachine
metadata:
  name: myvm
spec:
  terminationGracePeriodSeconds: 5
  domain:
    resources:
      requests:
        memory: 64M
    devices:
      disks:
      - name: registrydisk
        volumeName: registryvolume
        disk:
          bus: virtio
      - name: cloudinitdisk
        volumeName: cloudinitvolume
        disk:
          bus: virtio
  volumes:
    - name: registryvolume
      registryDisk:
        image: kubevirt/cirros-registry-disk-demo:devel
    - name: cloudinitvolume
      cloudInitNoCloud:
        userData: |
          ssh-authorized-keys:
            - ssh-rsa AAAAB3NzaK8L93bWxnyp test@test.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just as with regular Kubernetes pods, basic networking functionality is made available automatically to each KubeVirt VM, and particular TCP or UDP ports can be exposed to the outside world using regular Kubernetes services.  No special network configuration is required.&lt;/p&gt;

&lt;h2 id=&#34;getting-involved&#34;&gt;Getting Involved&lt;/h2&gt;

&lt;p&gt;KubeVirt development is accelerating, and the project is eager for new contributors. If you&amp;rsquo;re interested in getting involved, check out the project&amp;rsquo;s &lt;a href=&#34;https://github.com/kubevirt/kubevirt/issues&#34; target=&#34;_blank&#34;&gt;open issues&lt;/a&gt; and check out the &lt;a href=&#34;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&#34; target=&#34;_blank&#34;&gt;project calendar&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you need some help or want to chat you can connect to the team via freenode IRC in #kubevirt, or on the KubeVirt &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubevirt-dev&#34; target=&#34;_blank&#34;&gt;mailing list&lt;/a&gt;. User documentation is available at &lt;a href=&#34;https://kubevirt.gitbooks.io/user-guide/&#34; target=&#34;_blank&#34;&gt;https://kubevirt.gitbooks.io/user-guide/&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Gardener - The Kubernetes Botanist</title>
      <link>https://kubernetes.io/blog/2018/05/17/gardener/</link>
      <pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/17/gardener/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;a href=&#34;mailto:rafael.franzke@sap.com&#34; target=&#34;_blank&#34;&gt;Rafael Franzke&lt;/a&gt; (SAP), &lt;a href=&#34;mailto:vasu.chandrasekhara@sap.com&#34; target=&#34;_blank&#34;&gt;Vasu Chandrasekhara&lt;/a&gt; (SAP)&lt;/p&gt;

&lt;p&gt;Today, Kubernetes is the natural choice for running software in the Cloud. More and more developers and corporations are in the process of containerizing their applications, and many of them are adopting Kubernetes for automated deployments of their Cloud Native workloads.&lt;/p&gt;

&lt;p&gt;There are many Open Source tools which help in creating and updating single Kubernetes clusters. However, the more clusters you need the harder it becomes to operate, monitor, manage, and keep all of them alive and up-to-date.&lt;/p&gt;

&lt;p&gt;And that is exactly what project &amp;ldquo;&lt;a href=&#34;https://github.com/gardener&#34; target=&#34;_blank&#34;&gt;Gardener&lt;/a&gt;&amp;rdquo; focuses on. It is not just another provisioning tool, but it is rather designed to manage Kubernetes clusters as a service. It provides &lt;a href=&#34;https://github.com/cncf/k8s-conformance&#34; target=&#34;_blank&#34;&gt;Kubernetes-conformant&lt;/a&gt; clusters on various cloud providers and the ability to maintain hundreds or thousands of them at scale. At SAP, we face this heterogeneous multi-cloud &amp;amp; on-premise challenge not only in our own platform, but also encounter the same demand at all our larger and smaller customers implementing Kubernetes &amp;amp; Cloud Native.&lt;/p&gt;

&lt;p&gt;Inspired by the possibilities of Kubernetes and the ability to self-host, the foundation of Gardener is Kubernetes itself. While self-hosting, as in, to run Kubernetes components inside Kubernetes is a popular topic in the community, we apply a special pattern catering to the needs of operating a huge number of clusters with minimal total cost of ownership. We take an initial Kubernetes cluster (called &amp;ldquo;seed&amp;rdquo; cluster) and seed the control plane components (such as the API server, scheduler, controller-manager, etcd and others) of an end-user cluster as simple Kubernetes pods. In essence, the focus of the seed cluster is to deliver a robust Control-Plane-as-a-Service at scale. Following our botanical terminology, the end-user clusters when ready to sprout are called &amp;ldquo;shoot&amp;rdquo; clusters. Considering network latency and other fault scenarios, we recommend a seed cluster per cloud provider and region to host the control planes of the many shoot clusters.&lt;/p&gt;

&lt;p&gt;Overall, this concept of reusing Kubernetes primitives already simplifies deployment, management, scaling &amp;amp; patching/updating of the control plane. Since it builds upon highly available initial seed clusters, we can evade multiple quorum number of master node requirements for shoot cluster control planes and reduce waste/costs. Furthermore, the actual shoot cluster consists only of worker nodes for which full administrative access to the respective owners could be granted, thereby structuring a necessary separation of concerns to deliver a higher level of SLO. The architectural role &amp;amp; operational ownerships are thus defined as following (cf. &lt;code&gt;Figure 1&lt;/code&gt;):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes as a Service provider owns, operates, and manages the garden and the seed clusters. They represent parts of the required landscape/infrastructure.&lt;/li&gt;
&lt;li&gt;The control planes of the shoot clusters are run in the seed and, consequently, within the separate security domain of the service provider.&lt;/li&gt;
&lt;li&gt;The shoot clusters&amp;rsquo; machines are run under the ownership of and in the cloud provider account and the environment of the customer, but still managed by the Gardener.&lt;/li&gt;
&lt;li&gt;For on-premise or private cloud scenarios the delegation of ownership &amp;amp; management of the seed clusters (and the IaaS) is feasible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-17-gardener-the-kubernetes-botanist/architecture.png&#34; width=&#34;70%&#34; alt=&#34;Gardener architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1 Technical Gardener landscape with components.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Gardener is developed as an aggregated API server and comes with a bundled set of controllers. It runs inside another dedicated Kubernetes cluster (called &amp;ldquo;garden&amp;rdquo; cluster) and it extends the Kubernetes API with custom resources. Most prominently, the Shoot resource allows a description of the entire configuration of a user&amp;rsquo;s Kubernetes cluster in a declarative way. Corresponding controllers will, just like native Kubernetes controllers, watch these resources and bring the world&amp;rsquo;s actual state to the desired state (resulting in create, reconcile, update, upgrade, or delete operations.)
The following example manifest shows what needs to be specified:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: garden.sapcloud.io/v1beta1
kind: Shoot
metadata:
  name: dev-eu1
  namespace: team-a
spec:
  cloud:
    profile: aws
    region: us-east-1
    secretBindingRef:
      name: team-a-aws-account-credentials
    aws:
      machineImage:
        ami: ami-34237c4d
        name: CoreOS
      networks:
        vpc:
          cidr: 10.250.0.0/16
        ...
      workers:
      - name: cpu-pool
        machineType: m4.xlarge
        volumeType: gp2
        volumeSize: 20Gi
        autoScalerMin: 2
        autoScalerMax: 5
  dns:
    provider: aws-route53
    domain: dev-eu1.team-a.example.com
  kubernetes:
    version: 1.10.2
  backup:
    ...
  maintenance:
    ...
  addons:
    cluster-autoscaler:
      enabled: true
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once sent to the garden cluster, Gardener will pick it up and provision the actual shoot. What is not shown above is that each action will enrich the &lt;code&gt;Shoot&lt;/code&gt;&amp;rsquo;s &lt;code&gt;status&lt;/code&gt; field indicating whether an operation is currently running and recording the last error (if there was any) and the health of the involved components. Users are able to configure and monitor their cluster&amp;rsquo;s state in true Kubernetes style. Our users have even written their own custom controllers watching &amp;amp; mutating these &lt;code&gt;Shoot&lt;/code&gt; resources.&lt;/p&gt;

&lt;h1 id=&#34;technical-deep-dive&#34;&gt;Technical deep dive&lt;/h1&gt;

&lt;p&gt;The Gardener implements a Kubernetes inception approach; thus, it leverages Kubernetes capabilities to perform its operations. It provides a couple of controllers (cf. &lt;code&gt;[A]&lt;/code&gt;) watching &lt;code&gt;Shoot&lt;/code&gt; resources whereas the main controller is responsible for the standard operations like create, update, and delete. Another controller named &amp;ldquo;shoot care&amp;rdquo; is performing regular health checks and garbage collections, while a third&amp;rsquo;s (&amp;ldquo;shoot maintenance&amp;rdquo;) tasks are to cover actions like updating the shoot&amp;rsquo;s machine image to the latest available version.&lt;/p&gt;

&lt;p&gt;For every shoot, Gardener creates a dedicated &lt;code&gt;Namespace&lt;/code&gt; in the seed with appropriate security policies and within it pre-creates the later required certificates managed as &lt;code&gt;Secrets&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;etcd&#34;&gt;etcd&lt;/h3&gt;

&lt;p&gt;The backing data store etcd (cf. &lt;code&gt;[B]&lt;/code&gt;) of a Kubernetes cluster is deployed as a &lt;code&gt;StatefulSet&lt;/code&gt; with one replica and a &lt;code&gt;PersistentVolume(Claim)&lt;/code&gt;. Embracing best practices, we run another etcd shard-instance to store &lt;code&gt;Events&lt;/code&gt; of a shoot. Anyway, the main etcd pod is enhanced with a sidecar validating the data at rest and taking regular snapshots which are then efficiently backed up to an object store. In case etcd&amp;rsquo;s data is lost or corrupt, the sidecar restores it from the latest available snapshot. We plan to develop incremental/continuous backups to avoid discrepancies (in case of a recovery) between a restored etcd state and the actual state [1].&lt;/p&gt;

&lt;h3 id=&#34;kubernetes-control-plane&#34;&gt;Kubernetes control plane&lt;/h3&gt;

&lt;p&gt;As already mentioned above, we have put the other Kubernetes control plane components into native &lt;code&gt;Deployments&lt;/code&gt; and run them with the rolling update strategy. By doing so, we can not only leverage the existing deployment and update capabilities of Kubernetes, but also its monitoring and liveliness proficiencies. While the control plane itself uses in-cluster communication, the API Servers&amp;rsquo; &lt;code&gt;Service&lt;/code&gt; is exposed via a load balancer for external communication (cf. &lt;code&gt;[C]&lt;/code&gt;). In order to uniformly generate the deployment manifests (mainly depending on both the Kubernetes version and cloud provider), we decided to utilize &lt;a href=&#34;https://github.com/kubernetes/helm&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt; charts whereas Gardener leverages only Tillers rendering capabilities, but deploys the resulting manifests directly without running Tiller at all [2].&lt;/p&gt;

&lt;h3 id=&#34;infrastructure-preparation&#34;&gt;Infrastructure preparation&lt;/h3&gt;

&lt;p&gt;One of the first requirements when creating a cluster is a well-prepared infrastructure on the cloud provider side including networks and security groups. In our current provider specific in-tree implementation of Gardener (called the &amp;ldquo;Botanist&amp;rdquo;), we employ &lt;a href=&#34;https://github.com/hashicorp/terraform&#34; target=&#34;_blank&#34;&gt;Terraform&lt;/a&gt; to accomplish this task. Terraform provides nice abstractions for the major cloud providers and implements capabilities like parallelism, retry mechanisms, dependency graphs, idempotency, and more. However, we found that Terraform is challenging when it comes to error handling and it does not provide a technical interface to extract the root cause of an error. Currently, Gardener generates a Terraform script based on the shoot specification and stores it inside a &lt;code&gt;ConfigMap&lt;/code&gt; in the respective namespace of the seed cluster. The &lt;a href=&#34;https://github.com/gardener/terraformer&#34; target=&#34;_blank&#34;&gt;Terraformer component&lt;/a&gt; then runs as a &lt;code&gt;Job&lt;/code&gt; (cf. &lt;code&gt;[D]&lt;/code&gt;), executes the mounted Terraform configuration, and writes the produced state back into another &lt;code&gt;ConfigMap&lt;/code&gt;. Using the Job primitive in this manner helps to inherit its retry logic and achieve fault tolerance against temporary connectivity issues or resource constraints. Moreover, Gardener only needs to access the Kubernetes API of the seed cluster to submit the Job for the underlying IaaS. This design is important for private cloud scenarios in which typically the IaaS API is not exposed publicly.&lt;/p&gt;

&lt;h3 id=&#34;machine-controller-manager&#34;&gt;Machine controller manager&lt;/h3&gt;

&lt;p&gt;What is required next are the nodes to which the actual workload of a cluster is to be scheduled. However, Kubernetes offers no primitives to request nodes forcing a cluster administrator to use external mechanisms. The considerations include the full lifecycle, beginning with initial provisioning and continuing with providing security fixes, and performing health checks and rolling updates. While we started with instantiating static machines or utilizing instance templates of the cloud providers to create the worker nodes, we concluded (also from our previous production experience with running a cloud platform) that this approach requires extensive effort. During discussions at KubeCon 2017, we recognized that the best way, of course, to manage cluster nodes is to again apply core Kubernetes concepts and to teach the system to self-manage the nodes/machines it runs. For that purpose, we developed the &lt;a href=&#34;https://github.com/gardener/machine-controller-manager&#34; target=&#34;_blank&#34;&gt;machine controller manager&lt;/a&gt; (cf. &lt;code&gt;[E]&lt;/code&gt;) which extends Kubernetes with &lt;code&gt;MachineDeployment&lt;/code&gt;, &lt;code&gt;MachineClass&lt;/code&gt;, &lt;code&gt;MachineSet&lt;/code&gt; &amp;amp; &lt;code&gt;Machine&lt;/code&gt; resources and enables declarative management of (virtual) machines from within the Kubernetes context just like &lt;code&gt;Deployments&lt;/code&gt;, &lt;code&gt;ReplicaSets&lt;/code&gt; &amp;amp; &lt;code&gt;Pods&lt;/code&gt;. We reused code from existing Kubernetes controllers and just needed to abstract a few IaaS/cloud provider specific methods for creating, deleting, and listing machines in dedicated drivers. When comparing Pods and Machines a subtle difference becomes evident: creating virtual machines directly results in costs, and if something unforeseen happens, these costs can increase very quickly. To safeguard against such rampage, the machine controller manager comes with a safety controller that terminates orphaned machines and freezes the rollout of MachineDeployments and MachineSets beyond certain thresholds and time-outs. Furthermore, we leverage the existing official &lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler&#34; target=&#34;_blank&#34;&gt;cluster-autoscaler&lt;/a&gt; already including the complex logic of determining which node pool to scale out or down. Since its cloud provider interface is well-designed, we enabled the autoscaler to directly modify the number of replicas in the respective &lt;code&gt;MachineDeployment&lt;/code&gt; resource when triggering to scale out or down.&lt;/p&gt;

&lt;h3 id=&#34;addons&#34;&gt;Addons&lt;/h3&gt;

&lt;p&gt;Besides providing a properly setup control plane, every Kubernetes cluster requires a few system components to work. Usually, that&amp;rsquo;s the kube-proxy, an overlay network, a cluster DNS, and an ingress controller. Apart from that, Gardener allows to order optional add-ons configurable by the user (in the shoot resource definition), e.g. Heapster, the Kubernetes Dashboard, or Cert-Manager. Again, the Gardener renders the manifests for all these components via Helm charts (partly adapted and curated from the &lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable&#34; target=&#34;_blank&#34;&gt;upstream charts repository&lt;/a&gt;). However, these resources are managed in the shoot cluster and can thus be tweaked by users with full administrative access. Hence, Gardener ensures that these deployed resources always match the computed/desired configuration by utilizing an existing watch dog, the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager&#34; target=&#34;_blank&#34;&gt;kube-addon-manager&lt;/a&gt; (cf. &lt;code&gt;[F]&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&#34;network-air-gap&#34;&gt;Network air gap&lt;/h3&gt;

&lt;p&gt;While the control plane of a shoot cluster runs in a seed managed &amp;amp; supplied by your friendly platform-provider, the worker nodes are typically provisioned in a separate cloud provider (billing) account of the user. Typically, these worker nodes are placed into private networks [3] to which the API Server in the seed control plane establishes direct communication, using a simple &lt;a href=&#34;https://github.com/gardener/vpn&#34; target=&#34;_blank&#34;&gt;VPN&lt;/a&gt; solution based on ssh (cf. &lt;code&gt;[G]&lt;/code&gt;). We have recently migrated the SSH-based implementation to an &lt;a href=&#34;https://github.com/OpenVPN/openvpn&#34; target=&#34;_blank&#34;&gt;OpenVPN&lt;/a&gt;-based implementation which significantly increased the network bandwidth.&lt;/p&gt;

&lt;h3 id=&#34;monitoring-logging&#34;&gt;Monitoring &amp;amp; Logging&lt;/h3&gt;

&lt;p&gt;Monitoring, alerting, and logging are crucial to supervise clusters and keep them healthy so as to avoid outages and other issues. &lt;a href=&#34;https://github.com/prometheus/prometheus&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; has become the most used monitoring system in the Kubernetes domain. Therefore, we deploy a central Prometheus instance into the &lt;code&gt;garden&lt;/code&gt; namespace of every seed. It collects metrics from all the seed&amp;rsquo;s kubelets including those for all pods running in the seed cluster. In addition, next to every control plane a dedicated tenant Prometheus instance is provisioned for the shoot itself (cf. &lt;code&gt;[H]&lt;/code&gt;). It gathers metrics for its own control plane as well as for the pods running on the shoot&amp;rsquo;s worker nodes. The former is done by fetching data from the central Prometheus&amp;rsquo; federation endpoint and filtering for relevant control plane pods of the particular shoot. Other than that, Gardener deploys two &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics&#34; target=&#34;_blank&#34;&gt;kube-state-metrics&lt;/a&gt; instances, one responsible for the control plane and one for the workload, exposing cluster-level metrics to enrich the data. The &lt;a href=&#34;https://github.com/prometheus/node_exporter&#34; target=&#34;_blank&#34;&gt;node exporter&lt;/a&gt; provides more detailed node statistics. A dedicated tenant &lt;a href=&#34;http://github.com/grafana/grafana&#34; target=&#34;_blank&#34;&gt;Grafana&lt;/a&gt; dashboard displays the analytics and insights via lucid dashboards. We also defined alerting rules for critical events and employed the &lt;a href=&#34;https://github.com/prometheus/alertmanager&#34; target=&#34;_blank&#34;&gt;AlertManager&lt;/a&gt; to send emails to operators and support teams in case any alert is fired.&lt;/p&gt;

&lt;p&gt;[1] This is also the reason for not supporting point-in-time recovery. There is no reliable infrastructure reconciliation implemented in Kubernetes so far. Thus, restoring from an old backup without refreshing the actual workload and state of the concerned cluster would generally not be of much help.&lt;/p&gt;

&lt;p&gt;[2] The most relevant criteria for this decision was that Tiller requires a port-forward connection for communication which we experienced to be too unstable and error-prone for our automated use case. Nevertheless, we are looking forward to Helm v3 hopefully interacting with Tiller using &lt;code&gt;CustomResourceDefinitions&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;[3] Gardener offers to either create &amp;amp; prepare these networks with the Terraformer or it can be instructed to reuse pre-existing networks.&lt;/p&gt;

&lt;h1 id=&#34;usability-and-interaction&#34;&gt;Usability and Interaction&lt;/h1&gt;

&lt;p&gt;Despite requiring only the familiar &lt;code&gt;kubectl&lt;/code&gt; command line tool for managing all of Gardener, we provide a central &lt;a href=&#34;https://github.com/gardener/dashboard&#34; target=&#34;_blank&#34;&gt;dashboard&lt;/a&gt; for comfortable interaction. It enables users to easily keep track of their clusters&amp;rsquo; health, and operators to monitor, debug, and analyze the clusters they are responsible for. Shoots are grouped into logical projects in which teams managing a set of clusters can collaborate and even track issues via an integrated ticket system (e.g. GitHub Issues). Moreover, the dashboard helps users to add &amp;amp; manage their infrastructure account secrets and to view the most relevant data of all their shoot clusters in one place while being independent from the cloud provider they are deployed to.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-17-gardener-the-kubernetes-botanist/dashboard.gif&#34; alt=&#34;Gardener architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2 Animated Gardener dashboard.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;More focused on the duties of developers and operators, the Gardener command line client &lt;a href=&#34;https://github.com/gardener/gardenctl&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;gardenctl&lt;/code&gt;&lt;/a&gt; simplifies administrative tasks by introducing easy higher-level abstractions with simple commands that help condense and multiplex information &amp;amp; actions from/to large amounts of seed and shoot clusters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ gardenctl ls shoots
projects:
- project: team-a
  shoots:
  - dev-eu1
  - prod-eu1

$ gardenctl target shoot prod-eu1
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;prod-eu1&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;

$ gardenctl show prometheus
NAME           READY     STATUS    RESTARTS   AGE       IP              NODE
prometheus-0   &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;/3       Running   &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;          106d      &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;.241.241.42   ip-10-240-7-72.eu-central-1.compute.internal

URL: https://user:password@p.prod-eu1.team-a.seed.aws-eu1.example.com&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id=&#34;outlook-and-future-plans&#34;&gt;Outlook and future plans&lt;/h1&gt;

&lt;p&gt;The Gardener is already capable of managing Kubernetes clusters on AWS, Azure, GCP, OpenStack [4]. Actually, due to the fact that it relies only on Kubernetes primitives, it nicely connects to private cloud or on-premise requirements. The only difference from Gardener&amp;rsquo;s point of view would be the quality and scalability of the underlying infrastructure - the lingua franca of Kubernetes ensures strong portability guarantees for our approach.&lt;/p&gt;

&lt;p&gt;Nevertheless, there are still challenges ahead. We are probing a possibility to include an option to create a federation control plane delegating to multiple shoot clusters in this Open Source project. In the previous sections we have not explained how to &lt;a href=&#34;https://en.wikipedia.org/wiki/M%C3%BCnchhausen_trilemma&#34; target=&#34;_blank&#34;&gt;bootstrap&lt;/a&gt; the garden and the seed clusters themselves. You could indeed use any production ready cluster provisioning tool or the cloud providers&amp;rsquo; Kubernetes as a Service offering. We have built an uniform tool called &lt;a href=&#34;https://github.com/gardener/kubify&#34; target=&#34;_blank&#34;&gt;Kubify&lt;/a&gt; based on Terraform and reused many of the mentioned Gardener components. We envision the required Kubernetes infrastructure to be able to be spawned in its entirety by an initial bootstrap Gardener and are already discussing how we could achieve that.&lt;/p&gt;

&lt;p&gt;Another important topic we are focusing on is disaster recovery. When a seed cluster fails, the user&amp;rsquo;s static workload will continue to operate. However, administrating the cluster won&amp;rsquo;t be possible anymore. We are considering to move control planes of the shoots hit by a disaster to another seed. Conceptually, this approach is feasible and we already have the required components in place to implement that, e.g. automated etcd backup and restore. The contributors for this project not only have a mandate for developing Gardener for production, but most of us even run it in true DevOps mode as well. We completely trust the Kubernetes concepts and are committed to follow the &amp;ldquo;eat your own dog food&amp;rdquo; approach.&lt;/p&gt;

&lt;p&gt;In order to enable a more independent evolution of the Botanists, which contain the infrastructure provider specific parts of the implementation, we plan to describe well-defined interfaces and factor out the Botanists into their own components. This is similar to what Kubernetes is currently doing with the cloud-controller-manager. Currently, all the cloud specifics are part of the core Gardener repository presenting a soft barrier to extending or supporting new cloud providers.&lt;/p&gt;

&lt;p&gt;When taking a look at how the shoots are actually provisioned, we need to gain more experience on how really large clusters with thousands of nodes and pods (or more) behave. Potentially, we will have to deploy e.g. the API server and other components in a scaled-out fashion for large clusters to spread the load. Fortunately, horizontal pod autoscaling based on custom metrics from Prometheus will make this relatively easy with our setup. Additionally, the feedback from teams who run production workloads on our clusters, is that Gardener should support with prearranged Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/&#34; target=&#34;_blank&#34;&gt;QoS&lt;/a&gt;. Needless to say, our aspiration is going to be the integration and contribution to the vision of &lt;a href=&#34;https://speakerdeck.com/thockin/a-few-things-to-know-about-resource-scheduling&#34; target=&#34;_blank&#34;&gt;Kubernetes Autopilot&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] Prototypes already validated CTyun &amp;amp; Aliyun.&lt;/p&gt;

&lt;h1 id=&#34;gardener-is-open-source&#34;&gt;Gardener is open source&lt;/h1&gt;

&lt;p&gt;The Gardener project is developed as Open Source and hosted on GitHub: &lt;a href=&#34;https://github.com/gardener&#34; target=&#34;_blank&#34;&gt;https://github.com/gardener&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;SAP is working on Gardener since mid 2017 and is focused on building up a project that can easily be evolved and extended. Consequently, we are now looking for further partners and contributors to the project. As outlined above, we completely rely on Kubernetes primitives, add-ons, and specifications and adapt its innovative Cloud Native approach. We are looking forward to aligning with and contributing to the Kubernetes community. In fact, we envision contributing the complete project to the CNCF.&lt;/p&gt;

&lt;p&gt;At the moment, an important focus on collaboration with the community is the &lt;a href=&#34;https://sigs.k8s.io/cluster-api&#34; target=&#34;_blank&#34;&gt;Cluster API working group&lt;/a&gt; within the SIG Cluster Lifecycle founded a few months ago. Its primary goal is the definition of a portable API representing a Kubernetes cluster. That includes the configuration of control planes and the underlying infrastructure. The overlap of what we have already in place with Shoot and Machine resources compared to what the community is working on is striking. Hence, we joined this working group and are actively participating in their regular meetings, trying to contribute back our learnings from production. Selfishly, it is also in our interest to shape a robust API.&lt;/p&gt;

&lt;p&gt;If you see the potential of the Gardener project then please learn more about it on GitHub and help us make Gardener even better by asking questions, engaging in discussions, and by contributing code. Also, try out our &lt;a href=&#34;https://github.com/gardener/landscape-setup-template&#34; target=&#34;_blank&#34;&gt;quick start setup&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We are looking forward to seeing you there!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Docs are Migrating from Jekyll to Hugo</title>
      <link>https://kubernetes.io/blog/2018/05/05/hugo-migration/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/05/hugo-migration/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;https://www.cncf.io/people/staff/&#34; target=&#34;_blank&#34;&gt;Zach Corleissen&lt;/a&gt; (CNCF)&lt;/p&gt;

&lt;h2 id=&#34;changing-the-site-framework&#34;&gt;Changing the site framework&lt;/h2&gt;

&lt;p&gt;After nearly a year of investigating how to enable multilingual support for Kubernetes docs, we&amp;rsquo;ve decided to migrate the site&amp;rsquo;s static generator from Jekyll to &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What does the Hugo migration mean for users and contributors?&lt;/p&gt;

&lt;h3 id=&#34;things-will-break&#34;&gt;Things will break&lt;/h3&gt;

&lt;p&gt;Hugo&amp;rsquo;s Markdown parser is &lt;a href=&#34;https://gohugo.io/getting-started/configuration/#configure-blackfriday&#34; target=&#34;_blank&#34;&gt;differently strict than Jekyll&amp;rsquo;s&lt;/a&gt;. As a consequence, some Markdown formatting that rendered fine in Jekyll now produces some unexpected results: &lt;a href=&#34;https://github.com/kubernetes/website/issues/8258&#34; target=&#34;_blank&#34;&gt;strange left nav ordering&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes/website/issues/8247&#34; target=&#34;_blank&#34;&gt;vanishing tutorials&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kubernetes/website/issues/8246&#34; target=&#34;_blank&#34;&gt;broken links&lt;/a&gt;, among others.&lt;/p&gt;

&lt;p&gt;If you encounter any site weirdness or broken formatting, please &lt;a href=&#34;https://github.com/kubernetes/website/issues/new&#34; target=&#34;_blank&#34;&gt;open an issue&lt;/a&gt;. You can see the list of issues that are &lt;a href=&#34;https://github.com/kubernetes/website/issues?q=is%3Aissue+is%3Aopen+Hugo+label%3A%22Needs+Docs+Review%22&#34; target=&#34;_blank&#34;&gt;specific to Hugo migration&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;multilingual-support-is-coming&#34;&gt;Multilingual support is coming&lt;/h3&gt;

&lt;p&gt;Our initial search focused on finding a language selector that would play well with Jekyll. The  projects we found weren&amp;rsquo;t well-supported, and a prototype of one plugin made it clear that a Jekyll implementation would create technical debt that drained resources away from the quality of the docs.&lt;/p&gt;

&lt;p&gt;We chose Hugo after months of research and conversations with other open source translation projects. (Special thanks to &lt;a href=&#34;https://twitter.com/jaegerandi?lang=da&#34; target=&#34;_blank&#34;&gt;Andreas Jaeger&lt;/a&gt; and his experience at OpenStack). Hugo&amp;rsquo;s &lt;a href=&#34;https://gohugo.io/content-management/multilingual/&#34; target=&#34;_blank&#34;&gt;multilingual support&lt;/a&gt; is built in and easy.&lt;/p&gt;

&lt;h3 id=&#34;pain-now-relief-later&#34;&gt;Pain now, relief later&lt;/h3&gt;

&lt;p&gt;Another advantage of Hugo is that &lt;a href=&#34;https://gohugo.io/troubleshooting/build-performance/&#34; target=&#34;_blank&#34;&gt;build performance&lt;/a&gt; scales well at size. At 250+ pages, the Kubernetes site&amp;rsquo;s build times suffered significantly with Jekyll. We&amp;rsquo;re excited about removing the barrier to contribution created by slow site build times.&lt;/p&gt;

&lt;p&gt;Again, if you encounter any broken, missing, or unexpected content, please &lt;a href=&#34;https://github.com/kubernetes/website/issues/new&#34; target=&#34;_blank&#34;&gt;open an issue&lt;/a&gt; and let us know.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Announcing Kubeflow 0.1</title>
      <link>https://kubernetes.io/blog/2018/05/04/announcing-kubeflow-0.1/</link>
      <pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/04/announcing-kubeflow-0.1/</guid>
      <description>
        
        
        

&lt;h1 id=&#34;since-last-we-met&#34;&gt;Since Last We Met&lt;/h1&gt;

&lt;p&gt;Since the &lt;a href=&#34;https://kubernetes.io/blog/2017/12/introducing-kubeflow-composable&#34; target=&#34;_blank&#34;&gt;initial announcement&lt;/a&gt; of Kubeflow at &lt;a href=&#34;https://kccncna17.sched.com/event/CU5v/hot-dogs-or-not-at-scale-with-kubernetes-i-vish-kannan-david-aronchick-google&#34; target=&#34;_blank&#34;&gt;the last KubeCon+CloudNativeCon&lt;/a&gt;, we have been both surprised and delighted by the excitement for building great ML stacks for Kubernetes. In just over five months, the &lt;a href=&#34;https://github.com/kubeflow&#34; target=&#34;_blank&#34;&gt;Kubeflow project&lt;/a&gt; now has:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;70+ contributors&lt;/li&gt;
&lt;li&gt;20+ contributing organizations&lt;/li&gt;
&lt;li&gt;15 repositories&lt;/li&gt;
&lt;li&gt;3100+ GitHub stars&lt;/li&gt;
&lt;li&gt;700+ commits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and already is among the top 2% of GitHub projects &lt;strong&gt;&lt;em&gt;ever&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;People are excited to chat about Kubeflow as well! The Kubeflow community has also held meetups, talks and public sessions all around the world with thousands of attendees. With all this help, we’ve started to make substantial in every step of ML, from building your first model all the way to building a production-ready, high-scale deployments. At the end of the day, our mission remains the same: we want to let data scientists and software engineers focus on the things they do well by giving them an easy-to-use, portable and scalable ML stack.&lt;/p&gt;

&lt;h1 id=&#34;introducing-kubeflow-0-1&#34;&gt;Introducing Kubeflow 0.1&lt;/h1&gt;

&lt;p&gt;Today, we’re proud to announce the availability of Kubeflow 0.1, which provides a minimal set of packages to begin developing, training and deploying ML. In just a few commands, you can get:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Hub&lt;/strong&gt; - for collaborative &amp;amp; interactive training&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;TensorFlow Training Controller&lt;/strong&gt; with native distributed training&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;TensorFlow Serving&lt;/strong&gt; for hosting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Argo&lt;/strong&gt; for workflows&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SeldonCore&lt;/strong&gt; for complex inference and non TF models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ambassador&lt;/strong&gt; for Reverse Proxy&lt;/li&gt;
&lt;li&gt;Wiring to make it work on any Kubernetes anywhere&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started, it’s just as easy as it always has been:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Create a namespace for kubeflow deployment
NAMESPACE=kubeflow
kubectl create namespace ${NAMESPACE}
VERSION=v0.1.3

# Initialize a ksonnet app. Set the namespace for it&#39;s default environment.
APP_NAME=my-kubeflow
ks init ${APP_NAME}
cd ${APP_NAME}
ks env set default --namespace ${NAMESPACE}

# Install Kubeflow components
ks registry add kubeflow github.com/kubeflow/kubeflow/tree/${VERSION}/kubeflow
ks pkg install kubeflow/core@${VERSION}
ks pkg install kubeflow/tf-serving@${VERSION}
ks pkg install kubeflow/tf-job@${VERSION}

# Create templates for core components
ks generate kubeflow-core kubeflow-core

# Deploy Kubeflow
ks apply default -c kubeflow-core
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And thats it! JupyterHub is deployed so we can now use Jupyter to begin developing models. Once we have python code to build our model we can build a docker image and train our model using our TFJob operator by running commands like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ks generate tf-job my-tf-job --name=my-tf-job --image=gcr.io/my/image:latest
ks apply default -c my-tf-job

We could then deploy the model by doing

ks generate tf-serving ${MODEL_COMPONENT} --name=${MODEL_NAME}
ks param set ${MODEL_COMPONENT} modelPath ${MODEL_PATH}
ks apply ${ENV} -c ${MODEL_COMPONENT}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Within just a few commands, data scientists and software engineers can now create even complicated ML solutions and focus on what they do best: answering business critical questions.&lt;/p&gt;

&lt;h1 id=&#34;community-contributions&#34;&gt;Community Contributions&lt;/h1&gt;

&lt;p&gt;It’d be impossible to have gotten where we are without enormous help from everyone in the community. Some specific contributions that we want to highlight include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow/tree/master/kubeflow/argo&#34; target=&#34;_blank&#34;&gt;Argo&lt;/a&gt; for managing ML workflows&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/caffe2-operator&#34; target=&#34;_blank&#34;&gt;Caffe2 Operator&lt;/a&gt; for running Caffe2 jobs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow/tree/master/kubeflow/openmpi&#34; target=&#34;_blank&#34;&gt;Horovod &amp;amp; OpenMPI&lt;/a&gt; for improved distributed training performance of TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow/blob/master/docs/gke/iap.md&#34; target=&#34;_blank&#34;&gt;Identity Aware Proxy&lt;/a&gt;, which enables using security your services with identities, rather than VPNs and Firewalls&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/katib&#34; target=&#34;_blank&#34;&gt;Katib&lt;/a&gt; for hyperparameter tuning&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/experimental-kvc&#34; target=&#34;_blank&#34;&gt;Kubernetes volume controller&lt;/a&gt; which provides basic volume and data management using volumes and volume sources in a Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/kubebench&#34; target=&#34;_blank&#34;&gt;Kubebench&lt;/a&gt; for benchmarking of HW and ML stacks&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow/tree/master/kubeflow/pachyderm&#34; target=&#34;_blank&#34;&gt;Pachyderm&lt;/a&gt; for managing complex data pipelines&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/pytorch-operator&#34; target=&#34;_blank&#34;&gt;PyTorch operator&lt;/a&gt; for running PyTorch jobs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow/tree/master/kubeflow/seldon&#34; target=&#34;_blank&#34;&gt;Seldon Core&lt;/a&gt; for running complex model deployments and non-TensorFlow serving&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s difficult to overstate how much the community has helped bring all these projects (and more) to fruition. Just a few of the contributing companies include: Alibaba Cloud, Ant Financial, Caicloud, Canonical, Cisco, Datawire, Dell, Github, Google, Heptio, Huawei, Intel, Microsoft, Momenta, One Convergence, Pachyderm, Project Jupyter, Red Hat, Seldon, Uber and Weaveworks.&lt;/p&gt;

&lt;h1 id=&#34;learning-more&#34;&gt;Learning More&lt;/h1&gt;

&lt;p&gt;If you’d like to try out Kubeflow, we have a number of options for you:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You can use sample walkthroughs hosted on &lt;a href=&#34;https://www.katacoda.com/kubeflow&#34; target=&#34;_blank&#34;&gt;Katacoda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;You can follow a guided tutorial with existing models from the &lt;a href=&#34;https://github.com/kubeflow/examples&#34; target=&#34;_blank&#34;&gt;examples repository&lt;/a&gt;. These include the &lt;a href=&#34;https://github.com/kubeflow/examples/tree/master/github_issue_summarization&#34; target=&#34;_blank&#34;&gt;Github Issue Summarization&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubeflow/examples/tree/master/mnist&#34; target=&#34;_blank&#34;&gt;MNIST&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubeflow/examples/tree/master/agents&#34; target=&#34;_blank&#34;&gt;Reinforcement Learning with Agents&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You can start a cluster on your own and try your own model. Any Kubernetes conformant cluster will support Kubeflow  including those from contributors &lt;a href=&#34;https://www.prnewswire.com/news-releases/caicloud-releases-its-kubernetes-based-cluster-as-a-service-product-claas-20-and-the-first-tensorflow-as-a-service-taas-11-while-closing-6m-series-a-funding-300418071.html&#34; target=&#34;_blank&#34;&gt;Caicloud&lt;/a&gt;, &lt;a href=&#34;https://jujucharms.com/canonical-kubernetes/&#34; target=&#34;_blank&#34;&gt;Canonical&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-container-cluster&#34; target=&#34;_blank&#34;&gt;Google&lt;/a&gt;, &lt;a href=&#34;https://heptio.com/products/kubernetes-subscription/&#34; target=&#34;_blank&#34;&gt;Heptio&lt;/a&gt;, &lt;a href=&#34;https://github.com/mesosphere/dcos-kubernetes-quickstart&#34; target=&#34;_blank&#34;&gt;Mesosphere&lt;/a&gt;, &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough&#34; target=&#34;_blank&#34;&gt;Microsoft&lt;/a&gt;, &lt;a href=&#34;https://console.bluemix.net/docs/containers/cs_tutorials.html#cs_cluster_tutorial&#34; target=&#34;_blank&#34;&gt;IBM&lt;/a&gt;, &lt;a href=&#34;https://docs.openshift.com/container-platform/3.3/install_config/install/quick_install.html#install-config-install-quick-install&#34; target=&#34;_blank&#34;&gt;Red Hat/Openshift &lt;/a&gt;and &lt;a href=&#34;https://www.weave.works/product/cloud/&#34; target=&#34;_blank&#34;&gt;Weaveworks&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There were also a number of sessions at KubeCon + CloudNativeCon  EU 2018 covering Kubeflow. The links to the talks are here; the associated videos will be posted in the coming days.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Wednesday, May 2:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sched.co/Drmt&#34; target=&#34;_blank&#34;&gt;Kubeflow Intro - Michał Jastrzębski &amp;amp; Ala Raddaoui, Intel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thursday, May 3:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sched.co/Drnd&#34; target=&#34;_blank&#34;&gt;Kubeflow Deep Dive - Jeremy Lewi, Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sched.co/Dquu&#34; target=&#34;_blank&#34;&gt;Write Once, Train &amp;amp; Predict Everywhere: Portable ML Stacks with Kubeflow - Jeremy Lewi, Google &amp;amp; Stephan Fabel, Canonical&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sched.co/DqvC&#34; target=&#34;_blank&#34;&gt;Compliant Data Management and Machine Learning on Kubernetes - Daniel Whitenack, Pachyderm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kccnceu18.sched.com/event/E46y/bringing-your-data-pipeline-into-the-machine-learning-era-chris-gaun-jorg-schad-mesosphere-intermediate-skill-level&#34; target=&#34;_blank&#34;&gt;Bringing Your Data Pipeline into The Machine Learning Era - Chris Gaun &amp;amp; Jörg Schad, Mesosphere&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Friday, May 4:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sched.co/Duoq&#34; target=&#34;_blank&#34;&gt;Keynote: Kubeflow ML on Kubernetes - David Aronchick &amp;amp; Vishnu Kannan, Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sched.co/Dqv6&#34; target=&#34;_blank&#34;&gt;Conquering a Kubeflow Kubernetes Cluster with ksonnet, Ark, and Sonobuoy - Kris Nova, Heptio &amp;amp; David Aronchick, Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sched.co/Dqvw&#34; target=&#34;_blank&#34;&gt;Serving ML Models at Scale with Seldon and Kubeflow - Clive Cox, Seldon.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;what-s-next&#34;&gt;What’s Next?&lt;/h1&gt;

&lt;p&gt;Our next major release will be 0.2 coming this summer. In it, we expect to land the following new features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simplified setup via a bootstrap container&lt;/li&gt;
&lt;li&gt;Improved accelerator integration&lt;/li&gt;
&lt;li&gt;Support for more ML frameworks, e.g., Spark ML, XGBoost, sklearn&lt;/li&gt;
&lt;li&gt;Autoscaled TF Serving&lt;/li&gt;
&lt;li&gt;Programmatic data transforms, e.g., tf.transform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the most important feature is the one we haven’t heard yet. Please tell us! Some options for making your voice heard include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&#34; target=&#34;_blank&#34;&gt;Kubeflow Slack channel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubeflow-discuss&#34; target=&#34;_blank&#34;&gt;Kubeflow-discuss&lt;/a&gt; email list&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://twitter.com/kubeflow&#34; target=&#34;_blank&#34;&gt;Kubeflow twitter&lt;/a&gt; account&lt;/li&gt;
&lt;li&gt;Our &lt;a href=&#34;https://github.com/kubeflow/community&#34; target=&#34;_blank&#34;&gt;weekly community meeting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Please download and run &lt;a href=&#34;https://github.com/kubeflow/kubeflow/pull/330/files&#34; target=&#34;_blank&#34;&gt;kubeflow&lt;/a&gt;, and submit bugs!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for all your support so far!
&lt;em&gt;Jeremy Lewi &amp;amp; David Aronchick&lt;/em&gt; Google&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Current State of Policy in Kubernetes</title>
      <link>https://kubernetes.io/blog/2018/05/02/policy-in-kubernetes/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/02/policy-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;Kubernetes has grown dramatically in its impact to the industry; and with rapid growth, we are beginning to see variations across components in how they define and apply policies.&lt;/p&gt;

&lt;p&gt;Currently, policy related components could be found in identity services, networking services, storage services, multi-cluster federation, RBAC and many other areas, with different degree of maturity and also different motivations for specific problems. Within each component, some policies are extensible while others are not. The languages used by each project to express intent vary based on the original authors and experience. Driving consistent views of policies across the entire domain is a daunting task.&lt;/p&gt;

&lt;p&gt;Adoption of Kubernetes in regulated industries will also drive the need to ensure that a deployed cluster confirms with various legal requirements, such as PCI, HIPPA, or GDPR. Each of these compliance standards enforces a certain level of privacy around user information, data, and isolation.&lt;/p&gt;

&lt;p&gt;The core issues with the current Kubernetes policy implementations are identified as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lack of big picture across the platform&lt;/li&gt;
&lt;li&gt;Lack of coordination and common language among different policy components&lt;/li&gt;
&lt;li&gt;Lack of consistency for extensible policy creation across the platform.

&lt;ul&gt;
&lt;li&gt;There are areas where policy components are extensible, and there are also areas where strict end-to-end solutions are enforced. No consensus is established on the preference to a extensible and pluggable architecture.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Lack of consistent auditability across the Kubernetes architecture of policies which are created, modified, or disabled as well as the actions performed on behalf of the policies which are applied.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;forming-kubernetes-policy-wg&#34;&gt;Forming Kubernetes Policy WG&lt;/h2&gt;

&lt;p&gt;We have established a new WG to directly address these issues. We intend to provide an overall architecture that describes both the current policy related implementations as well as future policy related proposals in Kubernetes. Through a collaborative method, we want to present both dev and end user a universal view of policy in Kubernetes.&lt;/p&gt;

&lt;p&gt;We are not seeking to redefine and replace existing implementations which have been reached by thorough discussion and consensus. Rather to establish a summarized review of current implementation and addressing gaps to address broad end to end scenarios as defined in our initial design proposal.&lt;/p&gt;

&lt;p&gt;Kubernetes Policy WG has been focusing on the design proposal document and using the weekly meeting for discussions among WG members. The design proposal outlines the background and motivation of why we establish the WG, the concrete use cases from which the gaps/requirement analysis is deduced, the overall architecture and the container policy interface proposal.&lt;/p&gt;

&lt;h2 id=&#34;key-policy-scenarios-in-kubernetes&#34;&gt;Key Policy Scenarios in Kubernetes&lt;/h2&gt;

&lt;p&gt;Among several use cases the workgroup has brainstormed, eventually three major scenario stands out.&lt;/p&gt;

&lt;p&gt;The first one is about legislation/regulation compliance which requires the Kubernetes clusters conform to. The compliance scenario takes GDPR as an legislation example and the suggested policy architecture out of the discussion is to have a datapolicy controller responsible for the auditing.&lt;/p&gt;

&lt;p&gt;The second scenario is about capacity leasing, or multi-tenant quota in traditional IaaS concept, which deals with when a large enterprise want to delegate the resource control to various Lines Of Business it has, how the Kubernetes cluster should be configured to have a policy driven mechanism to enforce the quota system. The ongoing multi-tenant controller design proposed in the multi-tenancy working group could be an ideal enforcement point for the quota policy controller, which in turn might take a look at kube-arbitrator for inspiration.&lt;/p&gt;

&lt;p&gt;The last scenario is about cluster policy which refers to the general security and resource oriented policy control. Luster policy will involve both cluster level and namespace level policy control as well as enforcement, and there is a proposal called Kubernetes Security Profile that is under development by the Policy WG member to provide a PoC for this use case.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-policy-architecture&#34;&gt;Kubernetes Policy Architecture&lt;/h2&gt;

&lt;p&gt;Building upon the three scenarios, the WG is now working on three concrete proposals together with sig-arch, sig-auth and other related projects. Besides the Kubernetes security profile proposal aiming at the cluster policy use case, we also have the scheduling policy proposal which partially targets the capacity leasing use case and the topology service policy proposal which deals with affinity based upon service requirement and enforcement on routing level.&lt;/p&gt;

&lt;p&gt;When these concrete proposals got clearer the WG will be able to provide a high level Kubernetes policy architecture as part of the motivation of the establishment of the Policy WG.&lt;/p&gt;

&lt;h2 id=&#34;towards-cloud-native-policy-driven-architecture&#34;&gt;Towards Cloud Native Policy Driven Architecture&lt;/h2&gt;

&lt;p&gt;Policy is definitely something goes beyond Kubernetes and applied to a broader cloud native context. Our work in the Kubernetes Policy WG will provide the foundation of building a CNCF wide policy architecture, with the integration of Kubernetes and various other cloud native components such as open policy agent, Istio, Envoy, SPIFEE/SPIRE and so forth. The Policy WG has already collaboration with the CNCF SAFE WG (in-forming) team, and will work on more alignments to make sure a community driven cloud native policy architecture design.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Zhipeng Huang, Torin Sandall, Michael Elder, Erica Von Buelow, Khalid Ahmed, Yisui Hu&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Developing on Kubernetes</title>
      <link>https://kubernetes.io/blog/2018/05/01/developing-on-kubernetes/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/05/01/developing-on-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/mhausenblas&#34; target=&#34;_blank&#34;&gt;Michael Hausenblas&lt;/a&gt; (Red Hat), &lt;a href=&#34;https://twitter.com/errordeveloper&#34; target=&#34;_blank&#34;&gt;Ilya Dmitrichenko&lt;/a&gt; (Weaveworks)&lt;/p&gt;

&lt;p&gt;How do you develop a Kubernetes app? That is, how do you write and test an app that is supposed to run on Kubernetes? This article focuses on the challenges, tools and methods you might want to be aware of to successfully write Kubernetes apps alone or in a team setting.&lt;/p&gt;

&lt;p&gt;We’re assuming you are a developer, you have a favorite programming language, editor/IDE, and a testing framework available. The overarching goal is to introduce minimal changes to your current workflow when developing the app for Kubernetes. For example, if you’re a Node.js developer and are used to a hot-reload setup—that is, on save in your editor the running app gets automagically updated—then dealing with containers and container images, with container registries, Kubernetes deployments, triggers, and more can not only be overwhelming but really take all the fun out if it.&lt;/p&gt;

&lt;p&gt;In the following, we’ll first discuss the overall development setup, then review tools of the trade, and last but not least do a hands-on walkthrough of three exemplary tools that allow for iterative, local app development against Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;where-to-run-your-cluster&#34;&gt;Where to run your cluster?&lt;/h2&gt;

&lt;p&gt;As a developer you want to think about where the Kubernetes cluster you’re developing against runs as well as where the development environment sits. Conceptually there are four development modes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-devmodes_preview.png&#34; alt=&#34;Dev Modes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A number of tools support pure offline development including Minikube, Docker for Mac/Windows, Minishift, and the ones we discuss in detail below. Sometimes, for example, in a microservices setup where certain microservices already run in the cluster, a proxied setup (forwarding traffic into and from the cluster) is preferable and Telepresence is an example tool in this category. The live mode essentially means you’re building and/or deploying against a remote cluster and, finally, the pure online mode means both your development environment and the cluster are remote, as this is the case with, for example, &lt;a href=&#34;https://www.eclipse.org/che/docs/kubernetes-single-user.html&#34; target=&#34;_blank&#34;&gt;Eclipse Che&lt;/a&gt; or &lt;a href=&#34;https://github.com/errordeveloper/k9c&#34; target=&#34;_blank&#34;&gt;Cloud 9&lt;/a&gt;. Let’s now have a closer look at the basics of offline development: running Kubernetes locally.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/minikube/&#34; target=&#34;_blank&#34;&gt;Minikube&lt;/a&gt; is a popular choice for those who prefer to run Kubernetes in a local VM. More recently Docker for &lt;a href=&#34;https://docs.docker.com/docker-for-mac/kubernetes/&#34; target=&#34;_blank&#34;&gt;Mac&lt;/a&gt; and &lt;a href=&#34;https://docs.docker.com/docker-for-windows/kubernetes/&#34; target=&#34;_blank&#34;&gt;Windows&lt;/a&gt; started shipping Kubernetes as an experimental package (in the “edge” channel). Some reasons why you may want to prefer using Minikube over the Docker desktop option are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You already have Minikube installed and running&lt;/li&gt;
&lt;li&gt;You prefer to wait until Docker ships a stable package&lt;/li&gt;
&lt;li&gt;You’re a Linux desktop user&lt;/li&gt;
&lt;li&gt;You are a Windows user who doesn’t have Windows 10 Pro with Hyper-V&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Running a local cluster allows folks to work offline and that you don’t have to pay for using cloud resources. Cloud provider costs are often rather affordable and free tiers exists, however some folks prefer to avoid having to approve those costs with their manager as well as potentially incur unexpected costs, for example, when leaving cluster running over the weekend.&lt;/p&gt;

&lt;p&gt;Some developers prefer to use a remote Kubernetes cluster, and this is usually to allow for larger compute and storage capacity and also enable collaborative workflows more easily. This means it’s easier for you to pull in a colleague to help with debugging or share access to an app in the team. Additionally, for some developers it can be critical to mirror production environment as closely as possible, especially when it comes down to external cloud services, say,  proprietary databases, object stores, message queues, external load balancer, or mail delivery systems.&lt;/p&gt;

&lt;p&gt;In summary, there are good reasons for you to develop against a local cluster as well as a remote one. It very much depends on in which phase you are: from early prototyping and/or developing alone to integrating a set of more stable microservices.&lt;/p&gt;

&lt;p&gt;Now that you have a basic idea of the options around the runtime environment, let’s move on to how to iteratively develop and deploy your app.&lt;/p&gt;

&lt;h2 id=&#34;the-tools-of-the-trade&#34;&gt;The tools of the trade&lt;/h2&gt;

&lt;p&gt;We are now going to review tooling allowing you to develop apps on Kubernetes with the focus on having minimal impact on your existing workflow. We strive to provide an unbiased description including implications of using each of the tools in general terms.&lt;/p&gt;

&lt;p&gt;Note that this is a tricky area since even for established technologies such as, for example, JSON vs YAML vs XML or REST vs gRPC vs SOAP a lot depends on your background, your preferences and organizational settings. It’s even harder to compare tooling in the Kubernetes ecosystem as things evolve very rapidly and new tools are announced almost on a weekly basis; during the preparation of this post alone, for example, &lt;a href=&#34;https://gitkube.sh/&#34; target=&#34;_blank&#34;&gt;Gitkube&lt;/a&gt; and &lt;a href=&#34;https://github.com/MinikubeAddon/watchpod&#34; target=&#34;_blank&#34;&gt;Watchpod&lt;/a&gt; came out. To cover these new tools as well as related, existing tooling such as &lt;a href=&#34;https://github.com/weaveworks/flux&#34; target=&#34;_blank&#34;&gt;Weave Flux&lt;/a&gt; and OpenShift’s &lt;a href=&#34;https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html&#34; target=&#34;_blank&#34;&gt;S2I&lt;/a&gt; we are planning a follow-up blog post to the one you’re reading.&lt;/p&gt;

&lt;h3 id=&#34;draft&#34;&gt;Draft&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Azure/draft&#34; target=&#34;_blank&#34;&gt;Draft&lt;/a&gt; aims to help you get started deploying any app to Kubernetes. It is capable of applying heuristics as to what programming language your app is written in and generates a Dockerfile along with a Helm chart. It then runs the build for you and deploys resulting image to the target cluster via the Helm chart. It also allows user to setup port forwarding to localhost very easily.&lt;/p&gt;

&lt;p&gt;Implications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;User can customise the chart and Dockerfile templates however they like, or even create a &lt;a href=&#34;https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md&#34; target=&#34;_blank&#34;&gt;custom pack&lt;/a&gt; (with Dockerfile, the chart and more) for future use&lt;/li&gt;
&lt;li&gt;It’s not very simple to guess how just any app is supposed to be built, in some cases user may need to tweak Dockerfile and the Helm chart that Draft generates&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://github.com/Azure/draft/releases/tag/v0.12.0&#34; target=&#34;_blank&#34;&gt;Draft version 0.12.0&lt;/a&gt; or older, every time user wants to test a change, they need to wait for Draft to copy the code to the cluster, then run the build, push the image and release updated chart; this can timely, but it results in an image being for every single change made by the user (whether it was committed to git or not)&lt;/li&gt;
&lt;li&gt;As of Draft version 0.12.0, builds are executed locally&lt;/li&gt;
&lt;li&gt;User doesn’t have an option to choose something other than Helm for deployment&lt;/li&gt;
&lt;li&gt;It can watch local changes and trigger deployments, but this feature is not enabled by default&lt;/li&gt;
&lt;li&gt;It allows developer to use either local or remote Kubernetes cluster&lt;/li&gt;
&lt;li&gt;Deploying to production is up to the user, Draft authors recommend their other project – Brigade&lt;/li&gt;
&lt;li&gt;Can be used instead of Skaffold, and along the side of Squash&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More info:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/blog/2017/05/draft-kubernetes-container-development&#34; target=&#34;_blank&#34;&gt;Draft: Kubernetes container development made easy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Azure/draft/blob/master/docs/getting-started.md&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;skaffold&#34;&gt;Skaffold&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt; is a tool that aims to provide portability for CI integrations with different build system, image registry and deployment tools. It is different from Draft, yet somewhat comparable. It has a basic capability for generating manifests, but it’s not a prominent feature. Skaffold is extendible and lets user pick tools for use in each of the steps in building and deploying their app.&lt;/p&gt;

&lt;p&gt;Implications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Modular by design&lt;/li&gt;
&lt;li&gt;Works independently of CI vendor, user doesn’t need Docker or Kubernetes plugin&lt;/li&gt;
&lt;li&gt;Works without CI as such, i.e. from the developer’s laptop&lt;/li&gt;
&lt;li&gt;It can watch local changes and trigger deployments&lt;/li&gt;
&lt;li&gt;It allows developer to use either local or remote Kubernetes cluster&lt;/li&gt;
&lt;li&gt;It can be used to deploy to production, user can configure how exactly they prefer to do it and provide different kind of pipeline for each target environment&lt;/li&gt;
&lt;li&gt;Can be used instead of Draft, and along the side with most other tools&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More info:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudplatform.googleblog.com/2018/03/introducing-Skaffold-Easy-and-repeatable-Kubernetes-development.html&#34; target=&#34;_blank&#34;&gt;Introducing Skaffold: Easy and repeatable Kubernetes development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold#getting-started-with-local-tooling&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;squash&#34;&gt;Squash&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/solo-io/squash&#34; target=&#34;_blank&#34;&gt;Squash&lt;/a&gt; consists of a debug server that is fully integrated with Kubernetes, and a IDE plugin. It allows you to insert breakpoints and do all the fun stuff you are used to doing when debugging an application using an IDE. It bridges IDE debugging experience with your Kubernetes cluster by allowing you to attach the debugger to a pod running in your Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Implications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Can be used independently of other tools you chose&lt;/li&gt;
&lt;li&gt;Requires a privileged DaemonSet&lt;/li&gt;
&lt;li&gt;Integrates with popular IDEs&lt;/li&gt;
&lt;li&gt;Supports Go, Python, Node.js, Java and gdb&lt;/li&gt;
&lt;li&gt;User must ensure application binaries inside the container image are compiled with debug symbols&lt;/li&gt;
&lt;li&gt;Can be used in combination with any other tools described here&lt;/li&gt;
&lt;li&gt;It can be used with either local or remote Kubernetes cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More info:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5TrV3qzXlgI&#34; target=&#34;_blank&#34;&gt;Squash: A Debugger for Kubernetes Apps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/solo-io/squash/blob/master/docs/getting-started.md&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;telepresence&#34;&gt;Telepresence&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.telepresence.io/&#34; target=&#34;_blank&#34;&gt;Telepresence&lt;/a&gt; connects containers running on developer’s workstation with a remote Kubernetes cluster using a two-way proxy and emulates in-cluster environment as well as provides access to config maps and secrets. It aims to improve iteration time for container app development by eliminating the need for deploying app to the cluster and leverages local container to abstract network and filesystem interface in order to make it appear as if the app was running in the cluster.&lt;/p&gt;

&lt;p&gt;Implications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It can be used independently of other tools you chose&lt;/li&gt;
&lt;li&gt;Using together with Squash is possible, although Squash would have to be used for pods in the cluster, while conventional/local debugger would need to be used for debugging local container that’s connected to the cluster via Telepresence&lt;/li&gt;
&lt;li&gt;Telepresence imposes some network latency&lt;/li&gt;
&lt;li&gt;It provides connectivity via a side-car process - sshuttle, which is based on SSH&lt;/li&gt;
&lt;li&gt;More intrusive dependency injection mode with LD_PRELOAD/DYLD_INSERT_LIBRARIES is also available&lt;/li&gt;
&lt;li&gt;It is most commonly used with a remote Kubernetes cluster, but can be used with a local one also&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More info:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.telepresence.io/&#34; target=&#34;_blank&#34;&gt;Telepresence: fast, realistic local development for Kubernetes microservices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.telepresence.io/tutorials/docker&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.telepresence.io/discussion/how-it-works&#34; target=&#34;_blank&#34;&gt;How It Works&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ksync&#34;&gt;Ksync&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vapor-ware/ksync&#34; target=&#34;_blank&#34;&gt;Ksync&lt;/a&gt; synchronizes application code (and configuration) between your local machine and the container running in Kubernetes, akin to what &lt;a href=&#34;https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html&#34; target=&#34;_blank&#34;&gt;oc rsync&lt;/a&gt; does in OpenShift. It aims to improve iteration time for app development by eliminating build and deployment steps.&lt;/p&gt;

&lt;p&gt;Implications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It bypasses container image build and revision control&lt;/li&gt;
&lt;li&gt;Compiled language users have to run builds inside the pod (TBC)&lt;/li&gt;
&lt;li&gt;Two-way sync – remote files are copied to local directory&lt;/li&gt;
&lt;li&gt;Container is restarted each time remote filesystem is updated&lt;/li&gt;
&lt;li&gt;No security features – development only&lt;/li&gt;
&lt;li&gt;Utilizes &lt;a href=&#34;https://github.com/syncthing/syncthing&#34; target=&#34;_blank&#34;&gt;Syncthing&lt;/a&gt;, a Go library for peer-to-peer sync&lt;/li&gt;
&lt;li&gt;Requires a privileged DaemonSet running in the cluster&lt;/li&gt;
&lt;li&gt;Node has to use Docker with overlayfs2 – no other CRI implementations are supported at the time of writing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More info:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vapor-ware/ksync#getting-started&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vapor-ware/ksync/blob/master/docs/architecture.md&#34; target=&#34;_blank&#34;&gt;How It Works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.katacoda.com/vaporio/scenarios/ksync&#34; target=&#34;_blank&#34;&gt;Katacoda scenario to try out ksync in your browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.syncthing.net/specs/&#34; target=&#34;_blank&#34;&gt;Syncthing Specification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;hands-on-walkthroughs&#34;&gt;Hands-on walkthroughs&lt;/h2&gt;

&lt;p&gt;The app we will be using for the hands-on walkthroughs of the tools in the following is a simple &lt;a href=&#34;https://github.com/kubernauts/dok-example-us&#34; target=&#34;_blank&#34;&gt;stock market simulator&lt;/a&gt;, consisting of two microservices:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;stock-gen&lt;/code&gt; microservice is written in Go and generates stock data randomly and exposes it via HTTP endpoint &lt;code&gt;/stockdata&lt;/code&gt;.
‎* A second microservice, &lt;code&gt;stock-con&lt;/code&gt; is a Node.js app that consumes the stream of stock data from &lt;code&gt;stock-gen&lt;/code&gt; and provides an aggregation in form of a moving average via the HTTP endpoint &lt;code&gt;/average/$SYMBOL&lt;/code&gt; as well as a health-check endpoint at &lt;code&gt;/healthz&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall, the default setup of the app looks as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-architecture_preview.png&#34; alt=&#34;Default Setup&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the following we’ll do a hands-on walkthrough for a representative selection of tools discussed above: ksync, Minikube with local build, as well as Skaffold. For each of the tools we do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set up the respective tool incl. preparations for the deployment and local consumption of the &lt;code&gt;stock-con&lt;/code&gt; microservice.&lt;/li&gt;
&lt;li&gt;Perform a code update, that is, change the source code of the &lt;code&gt;/healthz&lt;/code&gt; endpoint in the &lt;code&gt;stock-con&lt;/code&gt; microservice and observe the updates.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that for the target Kubernetes cluster we’ve been using Minikube locally, but you can also a remote cluster for ksync and Skaffold if you want to follow along.&lt;/p&gt;

&lt;h2 id=&#34;walkthrough-ksync&#34;&gt;Walkthrough: ksync&lt;/h2&gt;

&lt;p&gt;As a preparation, install &lt;a href=&#34;https://vapor-ware.github.io/ksync/#installation&#34; target=&#34;_blank&#34;&gt;ksync&lt;/a&gt; and then carry out the following steps to prepare the development setup:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p $(pwd)/ksync
$ kubectl create namespace dok
$ ksync init -n dok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the basic setup completed we&amp;rsquo;re ready to tell ksync’s local client to watch a certain Kubernetes namespace and then we create a spec to define what we want to sync (the directory &lt;code&gt;$(pwd)/ksync&lt;/code&gt; locally with &lt;code&gt;/app&lt;/code&gt; in the container). Note that target pod is specified via the selector parameter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ksync watch -n dok
$ ksync create -n dok --selector=app=stock-con $(pwd)/ksync /app
$ ksync get -n dok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we deploy the stock generator and the stock consumer microservice:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n=dok apply \
      -f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-gen/app.yaml
$ kubectl -n=dok apply \
      -f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-con/app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once both deployments are created and the pods are running, we forward the &lt;code&gt;stock-con&lt;/code&gt; service for local consumption (in a separate terminal session):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get -n dok po --selector=app=stock-con  \
                     -o=custom-columns=:metadata.name --no-headers |  \
                     xargs -IPOD kubectl -n dok port-forward POD 9898:9898
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that we should be able to consume the &lt;code&gt;stock-con&lt;/code&gt; service from our local machine; we do this by regularly checking the response of the &lt;code&gt;healthz&lt;/code&gt; endpoint like so (in a separate terminal session):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch curl localhost:9898/healthz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now change the code in the &lt;code&gt;ksync/stock-con&lt;/code&gt;directory, for example update the &lt;a href=&#34;https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;/healthz&lt;/code&gt; endpoint code in &lt;code&gt;service.js&lt;/code&gt;&lt;/a&gt; by adding a field to the JSON response and observe how the pod gets updated and the response of the &lt;code&gt;curl localhost:9898/healthz&lt;/code&gt; command changes. Overall you should have something like the following in the end:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-ksync_preview.png&#34; alt=&#34;Preview&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;walkthrough-minikube-with-local-build&#34;&gt;Walkthrough: Minikube with local build&lt;/h3&gt;

&lt;p&gt;For the following you will need to have Minikube up and running and we will leverage the Minikube-internal Docker daemon for building images, locally. As a preparation, do the following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ eval $(minikube docker-env)
$ kubectl create namespace dok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we deploy the stock generator and the stock consumer microservice:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n=dok apply -f stock-gen/app.yaml
$ kubectl -n=dok apply -f stock-con/app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once both deployments are created and the pods are running, we forward the &lt;code&gt;stock-con&lt;/code&gt; service for local consumption (in a separate terminal session) and check the response of the &lt;code&gt;healthz&lt;/code&gt; endpoint:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get -n dok po --selector=app=stock-con  \
                     -o=custom-columns=:metadata.name --no-headers |  \
                     xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;amp;
$ watch curl localhost:9898/healthz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now change the code in the &lt;code&gt;stock-con&lt;/code&gt;directory, for example, update the &lt;a href=&#34;https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;/healthz&lt;/code&gt; endpoint code in &lt;code&gt;service.js&lt;/code&gt;&lt;/a&gt; by adding a field to the JSON response. Once you’re done with your code update, the last step is to build a new container image and kick off a new deployment like shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t stock-con:dev -f Dockerfile .
$ kubectl -n dok set image deployment/stock-con *=stock-con:dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Overall you should have something like the following in the end:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-minikube-localdev_preview.png&#34; alt=&#34;Local Preview&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;walkthrough-skaffold&#34;&gt;Walkthrough: Skaffold&lt;/h3&gt;

&lt;p&gt;To perform this walkthrough you first need to install &lt;a href=&#34;https://github.com/GoogleContainerTools/skaffold#installation&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;. Once that is done, you can do the following steps to prepare the development setup:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ kubectl create namespace dok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we deploy the stock generator (but not the stock consumer microservice, that is done via Skaffold):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n=dok apply -f stock-gen/app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that initially we experienced an authentication error when doing &lt;code&gt;skaffold dev&lt;/code&gt; and needed to apply a fix as described in &lt;a href=&#34;https://github.com/GoogleContainerTools/skaffold/issues/322&#34; target=&#34;_blank&#34;&gt;Issue 322&lt;/a&gt;. Essentially it means changing the content of &lt;code&gt;~/.docker/config.json&lt;/code&gt; to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
   &amp;quot;auths&amp;quot;: {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we had to patch &lt;code&gt;stock-con/app.yaml&lt;/code&gt; slightly to make it work with Skaffold:&lt;/p&gt;

&lt;p&gt;Add a &lt;code&gt;namespace&lt;/code&gt; field to both the &lt;code&gt;stock-con&lt;/code&gt; deployment and the service with the value of &lt;code&gt;dok&lt;/code&gt;.
Change the &lt;code&gt;image&lt;/code&gt; field of the container spec to &lt;code&gt;quay.io/mhausenblas/stock-con&lt;/code&gt; since Skaffold manages the container image tag on the fly.&lt;/p&gt;

&lt;p&gt;The resulting &lt;code&gt;app.yaml&lt;/code&gt; file stock-con looks as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    app: stock-con
  name: stock-con
  namespace: dok
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: stock-con
    spec:
      containers:
      - name: stock-con
        image: quay.io/mhausenblas/stock-con
        env:
        - name: DOK_STOCKGEN_HOSTNAME
          value: stock-gen
        - name: DOK_STOCKGEN_PORT
          value: &amp;quot;9999&amp;quot;
        ports:
        - containerPort: 9898
          protocol: TCP
        livenessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /healthz
            port: 9898
        readinessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /healthz
            port: 9898
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: stock-con
  name: stock-con
  namespace: dok
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 9898
  selector:
    app: stock-con
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step before we can start development is to configure Skaffold. So, create a file &lt;code&gt;skaffold.yaml&lt;/code&gt; in the &lt;code&gt;stock-con/&lt;/code&gt; directory with the following content:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: skaffold/v1alpha2
kind: Config
build:
  artifacts:
  - imageName: quay.io/mhausenblas/stock-con
    workspace: .
    docker: {}
  local: {}
deploy:
  kubectl:
    manifests:
      - app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’re ready to kick off the development. For that execute the following in the &lt;code&gt;stock-con/&lt;/code&gt; directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ skaffold dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above command triggers a build of the &lt;code&gt;stock-con&lt;/code&gt; image and then a deployment. Once the pod of the &lt;code&gt;stock-con&lt;/code&gt; deployment is running, we again forward the &lt;code&gt;stock-con&lt;/code&gt; service for local consumption (in a separate terminal session) and check the response of the &lt;code&gt;healthz&lt;/code&gt; endpoint:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get -n dok po --selector&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b8860b&#34;&gt;app&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;stock-con  &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;                     -o&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;custom-columns&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;:metadata.name --no-headers |  &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;                     xargs -IPOD kubectl -n dok port-forward POD &lt;span style=&#34;color:#666&#34;&gt;9898&lt;/span&gt;:9898 &amp;amp;
$ watch curl localhost:9898/healthz&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you now change the code in the &lt;code&gt;stock-con&lt;/code&gt;directory, for example, by updating the &lt;a href=&#34;https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;/healthz&lt;/code&gt; endpoint code in &lt;code&gt;service.js&lt;/code&gt;&lt;/a&gt; by adding a field to the JSON response, you should see Skaffold noticing the change and create a new image as well as deploy it. The resulting screen would look something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-skaffold_preview.png&#34; alt=&#34;Skaffold Preview&#34; /&gt;&lt;/p&gt;

&lt;p&gt;By now you should have a feeling how different tools enable you to develop apps on Kubernetes and if you’re interested to learn more about tools and or methods, check out the following resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Blog post by Shahidh K Muhammed on &lt;a href=&#34;https://blog.hasura.io/draft-vs-gitkube-vs-helm-vs-ksonnet-vs-metaparticle-vs-skaffold-f5aa9561f948&#34; target=&#34;_blank&#34;&gt;Draft vs Gitkube vs Helm vs Ksonnet vs Metaparticle vs Skaffold&lt;/a&gt; (03/2018)&lt;/li&gt;
&lt;li&gt;Blog post by Gergely Nemeth on &lt;a href=&#34;https://nemethgergely.com/using-kubernetes-for-local-development/index.html&#34; target=&#34;_blank&#34;&gt;Using Kubernetes for Local Development&lt;/a&gt;, with a focus on Skaffold (03/2018)&lt;/li&gt;
&lt;li&gt;Blog post by Richard Li on &lt;a href=&#34;https://hackernoon.com/locally-developing-kubernetes-services-without-waiting-for-a-deploy-f63995de7b99&#34; target=&#34;_blank&#34;&gt;Locally developing Kubernetes services (without waiting for a deploy)&lt;/a&gt;, with a focus on Telepresence&lt;/li&gt;
&lt;li&gt;Blog post by Abhishek Tiwari on &lt;a href=&#34;https://abhishek-tiwari.com/local-development-environment-for-kubernetes-using-minikube/&#34; target=&#34;_blank&#34;&gt;Local Development Environment for Kubernetes using Minikube&lt;/a&gt; (09/2017)&lt;/li&gt;
&lt;li&gt;Blog post by Aymen El Amri on &lt;a href=&#34;https://medium.com/devopslinks/using-kubernetes-minikube-for-local-development-c37c6e56e3db&#34; target=&#34;_blank&#34;&gt;Using Kubernetes for Local Development — Minikube&lt;/a&gt; (08/2017)&lt;/li&gt;
&lt;li&gt;Blog post by Alexis Richardson on &lt;a href=&#34;https://www.weave.works/blog/gitops-operations-by-pull-request&#34; target=&#34;_blank&#34;&gt;​GitOps - Operations by Pull Request&lt;/a&gt; (08/2017)&lt;/li&gt;
&lt;li&gt;Slide deck &lt;a href=&#34;https://docs.google.com/presentation/d/1d3PigRVt_m5rO89Ob2XZ16bW8lRSkHHH5k816-oMzZo/&#34; target=&#34;_blank&#34;&gt;GitOps: Drive operations through git&lt;/a&gt;, with a focus on Gitkube by Tirumarai Selvan (03/2018)&lt;/li&gt;
&lt;li&gt;Slide deck &lt;a href=&#34;https://speakerdeck.com/mhausenblas/developing-apps-on-kubernetes&#34; target=&#34;_blank&#34;&gt;Developing apps on Kubernetes&lt;/a&gt;, a talk Michael Hausenblas gave at a CNCF Paris meetup  (04/2018)&lt;/li&gt;
&lt;li&gt;YouTube videos:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QW85Y0Ug3KY&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 029: Developing Apps with Ksync&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=McwwWhCXMxc&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 030: Exploring Skaffold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zezeBAJ_3w8&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 031: Connecting with Telepresence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8B1D7cTMPgA&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 033: Developing with Draft&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Raw responses to the &lt;a href=&#34;https://docs.google.com/spreadsheets/d/12ilRCly2eHKPuicv1P_BD6z__PXAqpiaR-tDYe2eudE/edit&#34; target=&#34;_blank&#34;&gt;Kubernetes Application Survey&lt;/a&gt; 2018 by SIG Apps&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With that we wrap up this post on how to go about developing apps on Kubernetes, we hope you learned something and if you have feedback and/or want to point out a tool that you found useful, please let us know via Twitter: &lt;a href=&#34;https://twitter.com/errordeveloper&#34; target=&#34;_blank&#34;&gt;Ilya&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/mhausenblas&#34; target=&#34;_blank&#34;&gt;Michael&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Zero-downtime Deployment in Kubernetes with Jenkins</title>
      <link>https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/</link>
      <pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/</guid>
      <description>
        
        
        

&lt;p&gt;Ever since we added the &lt;a href=&#34;https://aka.ms/azjenkinsk8s&#34; target=&#34;_blank&#34;&gt;Kubernetes Continuous Deploy&lt;/a&gt; and &lt;a href=&#34;https://aka.ms/azjenkinsacs&#34; target=&#34;_blank&#34;&gt;Azure Container Service&lt;/a&gt; plugins to the Jenkins update center, &amp;ldquo;How do I create zero-downtime deployments&amp;rdquo; is one of our most frequently-asked questions. We created a quickstart template on Azure to demonstrate what zero-downtime deployments can look like. Although our example uses Azure, the concept easily applies to all Kubernetes installations.&lt;/p&gt;

&lt;h2 id=&#34;rolling-update&#34;&gt;Rolling Update&lt;/h2&gt;

&lt;p&gt;Kubernetes supports the RollingUpdate strategy to replace old pods with new ones gradually, while continuing to serve clients without incurring downtime. To perform a RollingUpdate deployment:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set &lt;code&gt;.spec.strategy.type&lt;/code&gt; to &lt;code&gt;RollingUpdate&lt;/code&gt; (the default value).&lt;/li&gt;
&lt;li&gt;Set &lt;code&gt;.spec.strategy.rollingUpdate.maxUnavailable&lt;/code&gt; and &lt;code&gt;.spec.strategy.rollingUpdate.maxSurge&lt;/code&gt; to some reasonable value.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;maxUnavailable&lt;/code&gt;: the maximum number of pods that can be unavailable during the update process. This can be an absolute number or percentage of the replicas count; the default is 25%.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;maxSurge&lt;/code&gt;: the maximum number of pods that can be created over the desired number of pods. Again this can be an absolute number or a percentage of the replicas count; the default is 25%.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Configure the &lt;code&gt;readinessProbe&lt;/code&gt; for your service container to help Kubernetes determine the state of the pods. Kubernetes will only route the client traffic to the pods with a healthy liveness probe.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;ll use deployment of the official Tomcat image to demonstrate this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: tomcat-deployment-rolling-update
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: tomcat
        role: rolling-update
    spec:
      containers:
      - name: tomcat-container
        image: tomcat:${TOMCAT_VERSION}
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /
            port: 8080
  strategy:
    type: RollingUpdate
    rollingUp      maxSurge: 50%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the Tomcat running in the current deployments is version 7, we can replace &lt;code&gt;${TOMCAT_VERSION}&lt;/code&gt; with 8 and apply this to the Kubernetes cluster. With the &lt;a href=&#34;https://aka.ms/azjenkinsk8s&#34; target=&#34;_blank&#34;&gt;Kubernetes Continuous Deploy&lt;/a&gt; or the &lt;a href=&#34;https://aka.ms/azjenkinsacs&#34; target=&#34;_blank&#34;&gt;Azure Container Service&lt;/a&gt; plugin, the value can be fetched from an environment variable which eases the deployment process.&lt;/p&gt;

&lt;p&gt;Behind the scenes, Kubernetes manages the update like so:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-04-30-zero-downtime-deployment-kubernetes-jenkins/deployment-process.png&#34; alt=&#34;Deployment Process&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Initially, all pods are running Tomcat 7 and the frontend Service routes the traffic to these pods.&lt;/li&gt;
&lt;li&gt;During the rolling update, Kubernetes takes down some Tomcat 7 pods and creates corresponding new Tomcat 8 pods. It ensures:

&lt;ul&gt;
&lt;li&gt;at most &lt;code&gt;maxUnavailable&lt;/code&gt; pods in the desired Pods can be unavailable, that is, at least (&lt;code&gt;replicas&lt;/code&gt; - &lt;code&gt;maxUnavailable&lt;/code&gt;) pods should be serving the client traffic, which is 2-1=1 in our case.&lt;/li&gt;
&lt;li&gt;at most maxSurge more pods can be created during the update process, that is 2*50%=1 in our case.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One Tomcat 7 pod is taken down, and one Tomcat 8 pod is created. Kubernetes will not route the traffic to any of them because their readiness probe is not yet successful.&lt;/li&gt;
&lt;li&gt;When the new Tomcat 8 pod is ready as determined by the readiness probe, Kubernetes will start routing the traffic to it. This means during the update process, users may see both the old service and the new service.&lt;/li&gt;
&lt;li&gt;The rolling update continues by taking down Tomcat 7 pods and creating Tomcat 8 pods, and then routing the traffic to the ready pods.&lt;/li&gt;
&lt;li&gt;Finally, all pods are on Tomcat 8.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Rolling Update strategy ensures we always have some Ready backend pods serving client requests, so there&amp;rsquo;s no service downtime. However, some extra care is required:
- During the update, both the old pods and new pods may serve the requests. Without well defined session affinity in the Service layer, a user may be routed to the new pods and later back to the old pods.
- This also requires you to maintain well-defined forward and backward compatibility for both data and the API, which can be challenging.
- It may take a long time before a pod is ready for traffic after it is started. There may be a long window of time where the traffic is served with less backend pods than usual. Generally, this should not be a problem as we tend to do production upgrades when the service is less busy. But this will also extend the time window for issue 1.
- We cannot do comprehensive tests for the new pods being created. Moving application changes from dev / QA environments to production can represent a persistent risk of breaking existing functionality. The readiness probe can do some work to check readiness, however, it should be a lightweight task that can be run periodically, and not suitable to be used as an entry point to start the complete tests.&lt;/p&gt;

&lt;h2 id=&#34;blue-green-deployment&#34;&gt;Blue/green Deployment&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Blue/green deployment quoted from TechTarget&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A blue/green deployment is a change management strategy for releasing software code. Blue/green deployments, which may also be referred to as A/B deployments require two identical hardware environments that are configured exactly the same way. While one environment is active and serving end users, the other environment remains idle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Container technology offers a stand-alone environment to run the desired service, which makes it super easy to create identical environments as required in the blue/green deployment. The loosely coupled Services - ReplicaSets, and the label/selector-based service routing in Kubernetes make it easy to switch between different backend environments. With these techniques, the blue/green deployments in Kubernetes can be done as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Before the deployment, the infrastructure is prepared like so:

&lt;ul&gt;
&lt;li&gt;Prepare the blue deployment and green deployment with &lt;code&gt;TOMCAT_VERSION=7&lt;/code&gt; and &lt;code&gt;TARGET_ROLE&lt;/code&gt; set to blue or green respectively.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: tomcat-deployment-${TARGET_ROLE}
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: tomcat
        role: ${TARGET_ROLE}
    spec:
      containers:
      - name: tomcat-container
        image: tomcat:${TOMCAT_VERSION}
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /
            port: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Prepare the public service endpoint, which initially routes to one of the backend environments, say &lt;code&gt;TARGET_ROLE=blue&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: tomcat-service
  labels:
    app: tomcat
    role: ${TARGET_ROLE}
    env: prod
spec:
  type: LoadBalancer
  selector:
    app: tomcat
    role: ${TARGET_ROLE}
  ports:
    - port: 80
      targetPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Optionally, prepare a test endpoint so that we can visit the backend environments for testing. They are similar to the public service endpoint, but they are intended to be accessed internally by the dev/ops team only.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: tomcat-test-${TARGET_ROLE}
  labels:
    app: tomcat
    role: test-${TARGET_ROLE}
spec:
  type: LoadBalancer
  selector:
    app: tomcat
    role: ${TARGET_ROLE}
  ports:
    - port: 80
      targetPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Update the application in the inactive environment, say green environment. Set &lt;code&gt;TARGET_ROLE=green&lt;/code&gt; and &lt;code&gt;TOMCAT_VERSION=8&lt;/code&gt; in the deployment config to update the green environment.&lt;/li&gt;
&lt;li&gt;Test the deployment via the &lt;code&gt;tomcat-test-green&lt;/code&gt; test endpoint to ensure the green environment is ready to serve client traffic.&lt;/li&gt;
&lt;li&gt;Switch the frontend Service routing to the green environment by updating the Service config with &lt;code&gt;TARGET_ROLE=green&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Run additional tests on the public endpoint to ensure it is working properly.&lt;/li&gt;
&lt;li&gt;Now the blue environment is idle and we can:

&lt;ul&gt;
&lt;li&gt;leave it with the old application so that we can roll back if there&amp;rsquo;s issue with the new application&lt;/li&gt;
&lt;li&gt;update it to make it a hot backup of the active environment&lt;/li&gt;
&lt;li&gt;reduce its replica count to save the occupied resources&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-04-30-zero-downtime-deployment-kubernetes-jenkins/resources.png&#34; alt=&#34;Resources&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As compared to Rolling Update, the blue/green up* The public service is either routed to the old applications, or new applications, but never both at the same time.
* The time it takes for the new pods to be ready does not affect the public service quality, as the traffic will only be routed to the new pods when all of them are tested to be ready.
* We can do comprehensive tests on the new environment before it serves any public traffic. Just keep in mind this is in production, and the tests should not pollute live application data.&lt;/p&gt;

&lt;h2 id=&#34;jenkins-automation&#34;&gt;Jenkins Automation&lt;/h2&gt;

&lt;p&gt;Jenkins provides easy-to-setup workflow to automate your deployments. With &lt;a href=&#34;https://jenkins.io/doc/book/pipeline/&#34; target=&#34;_blank&#34;&gt;Pipeline&lt;/a&gt; support, it is flexible to build the zero-downtime deployment workflow, and visualize the deployment steps.
To facilitate the deployment process for Kubernetes resources, we published the &lt;a href=&#34;https://aka.ms/azjenkinsk8s&#34; target=&#34;_blank&#34;&gt;Kubernetes Continuous Deploy&lt;/a&gt; and the &lt;a href=&#34;https://aka.ms/azjenkinsacs&#34; target=&#34;_blank&#34;&gt;Azure Container Service&lt;/a&gt; plugins built based on the &lt;a href=&#34;https://github.com/fabric8io/kubernetes-client&#34; target=&#34;_blank&#34;&gt;kubernetes-client&lt;/a&gt;. You can deploy the resource to Azure Kubernetes Service (AKS) or the general Kubernetes clusters without the need of kubectl, and it supports variable substitution in the resource configuration so you can deploy environment-specific resources to the clusters without updating the resource config.
We created a Jenkins Pipeline to demonstrate the blue/green deployment to AKS. The flow is like the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-04-30-zero-downtime-deployment-kubernetes-jenkins/jenkins-pipeline.png&#34; alt=&#34;Jenkins Pipeline&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pre-clean: clean workspace.&lt;/li&gt;
&lt;li&gt;SCM: pulling code from the source control management system.&lt;/li&gt;
&lt;li&gt;Prepare Image: prepare the application docker images and upload them to some Docker repository.&lt;/li&gt;
&lt;li&gt;Check Env: determine the active and inactive environment, which drives the following deployment.&lt;/li&gt;
&lt;li&gt;Deploy: deploy the new application resource configuration to the inactive environment. With the Azure Container Service plugin, this can be done with:
&lt;code&gt;
acsDeploy azureCredentialsId: &#39;stored-azure-credentials-id&#39;,
      configFilePaths: &amp;quot;glob/path/to/*/resource-config-*.yml&amp;quot;,
      containerService: &amp;quot;aks-name | AKS&amp;quot;,
      resourceGroupName: &amp;quot;resource-group-name&amp;quot;,
      enableConfigSubstitution: true
&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Verify Staged: verify the deployment to the inactive environment to ensure it is working properly. Again, note this is in the production environment, so be careful not to pollute live application data during tests.&lt;/li&gt;
&lt;li&gt;Confirm: Optionally, send email notifications for manual user approval to proceed with the actual environment switch.&lt;/li&gt;
&lt;li&gt;Switch: Switch the frontend service endpoint routing to the inactive environment. This is just another service deployment to the AKS Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Verify Prod: verify the frontend service endpoint is working properly with the new environment.&lt;/li&gt;
&lt;li&gt;Post-clean: do some post clean on the temporary files.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the Rolling Update strategy, simply deploy the deployment configuration to the Kubernetes cluster, which is a simple, single step.&lt;/p&gt;

&lt;h2 id=&#34;put-it-all-together&#34;&gt;Put It All Together&lt;/h2&gt;

&lt;p&gt;We built a quickstart template on Azure to demonstrate how we can do the zero-downtime deployment to AKS (Kubernetes) with Jenkins. Go to &lt;a href=&#34;https://aka.ms/azjenkinsk8sqs&#34; target=&#34;_blank&#34;&gt;Jenkins Blue-Green Deployment on Kubernetes&lt;/a&gt; and click the button Deploy to Azure to get the working demo. This template will provision:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;An AKS cluster, with the following resources:

&lt;ul&gt;
&lt;li&gt;Two similar deployments representing the environments &amp;ldquo;blue&amp;rdquo; and &amp;ldquo;green&amp;rdquo;. Both are initially set up with the &lt;code&gt;tomcat&lt;/code&gt;:7 image.&lt;/li&gt;
&lt;li&gt;Two test endpoint services (&lt;code&gt;tomcat-test-blue&lt;/code&gt; and &lt;code&gt;tomcat-test-green&lt;/code&gt;), which are connected to the corresponding deployments, and can be used to test if the deployments are ready for production use.&lt;/li&gt;
&lt;li&gt;A production service endpoint (&lt;code&gt;tomcat-service&lt;/code&gt;) which represents the public endpoint that the users will access. Initially it is routing to the &amp;ldquo;blue&amp;rdquo; environment.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A Jenkins master running on an Ubuntu 16.04 VM, with the Azure service principal credentials configured. The Jenkins instance has two sample jobs:

&lt;ul&gt;
&lt;li&gt;AKS Kubernetes Rolling Update Deployment pipeline to demonstrate the Rolling Update deployment to AKS.&lt;/li&gt;
&lt;li&gt;AKS Kubernetes Blue/green Deployment pipeline to demonstrate the blue/green deployment to AKS.&lt;/li&gt;
&lt;li&gt;We didn&amp;rsquo;t include the email confirmation step in the quickstart template. To add that, you need to configure the email SMTP server details in the Jenkins system configuration, and then add a Pipeline stage before Switch:
&lt;code&gt;
stage(&#39;Confirm&#39;) {
mail (to: &#39;to@example.com&#39;,
subject: &amp;quot;Job &#39;${env.JOB_NAME}&#39; (${env.BUILD_NUMBER}) is waiting for input&amp;quot;,
body: &amp;quot;Please go to ${env.BUILD_URL}.&amp;quot;)
input &#39;Ready to go?&#39;
}
&lt;/code&gt;
Follow the &lt;a href=&#34;https://github.com/Azure/azure-quickstart-templates/tree/master/301-jenkins-aks-zero-downtime-deployment#steps&#34; target=&#34;_blank&#34;&gt;Steps&lt;/a&gt; to setup the resources and you can try it out by start the Jenkins build jobs.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Community - Top of the Open Source Charts in 2017</title>
      <link>https://kubernetes.io/blog/2018/04/25/open-source-charts-2017/</link>
      <pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/04/25/open-source-charts-2017/</guid>
      <description>
        
        
        

&lt;p&gt;2017 was a huge year for Kubernetes, and GitHub’s latest &lt;a href=&#34;https://octoverse.github.com&#34; target=&#34;_blank&#34;&gt;Octoverse report&lt;/a&gt; illustrates just how much attention this project has been getting.&lt;/p&gt;

&lt;p&gt;Kubernetes, an &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&#34; target=&#34;_blank&#34;&gt;open source platform for running application containers&lt;/a&gt;, provides a consistent interface that enables developers and ops teams to automate the deployment, management, and scaling of a wide variety of applications on just about any infrastructure.&lt;/p&gt;

&lt;p&gt;Solving these shared challenges by leveraging a wide community of expertise and industrial experience, as Kubernetes does, helps engineers focus on building their own products at the top of the stack, rather than needlessly duplicating work that now exists as a standard part of the “cloud native” toolkit.&lt;/p&gt;

&lt;p&gt;However, achieving these gains via ad-hoc collective organizing is its own unique challenge, one which makes it increasingly difficult to support open source, community-driven efforts through periods of rapid growth.&lt;/p&gt;

&lt;p&gt;Read on to find out how the Kubernetes Community has addressed these scaling challenges to reach the top of the charts in GitHub’s 2017 Octoverse report.&lt;/p&gt;

&lt;h2 id=&#34;most-discussed-on-github&#34;&gt;Most-Discussed on GitHub&lt;/h2&gt;

&lt;p&gt;The top two most-discussed repos of 2017 are both based on Kubernetes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog-logging/2018-04-24-open-source-charts-2017/most-discussed.png&#34; alt=&#34;Most Discussed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of all the open source repositories on GitHub, none received more issue comments than &lt;a href=&#34;https://github.com/kubernetes/kubernetes/&#34; target=&#34;_blank&#34;&gt;kubernetes/kubernetes&lt;/a&gt;. &lt;a href=&#34;http://openshift.com/&#34; target=&#34;_blank&#34;&gt;OpenShift&lt;/a&gt;, a &lt;a href=&#34;https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/&#34; target=&#34;_blank&#34;&gt;CNCF certified distribution of Kubernetes&lt;/a&gt;, took second place.&lt;/p&gt;

&lt;p&gt;Open discussion with ample time for community feedback and review helps build shared infrastructure and establish new standards for cloud native computing.&lt;/p&gt;

&lt;h2 id=&#34;most-reviewed-on-github&#34;&gt;Most Reviewed on GitHub&lt;/h2&gt;

&lt;p&gt;Successfully scaling an open source effort’s communications often leads to better coordination and higher-quality feature delivery. The Kubernetes project’s &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Group (SIG)&lt;/a&gt; structure has helped it become GitHub’s second most reviewed project:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog-logging/2018-04-24-open-source-charts-2017/most-reviews.png&#34; alt=&#34;Most Reviewed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using SIGs to segment and standardize mechanisms for community participation helps channel more frequent reviews from better-qualified community members.&lt;/p&gt;

&lt;p&gt;When managed effectively, active community discussions indicate more than just a highly contentious codebase, or a project with an extensive list of unmet needs.&lt;/p&gt;

&lt;p&gt;Scaling a project’s capacity to handle issues and community interactions helps to expand the conversation.  Meanwhile, large communities come with more diverse use cases and a larger array of support problems to manage. The Kubernetes &lt;a href=&#34;https://github.com/kubernetes/community#sigs&#34; target=&#34;_blank&#34;&gt;SIG organization structure&lt;/a&gt; helps to address the challenges of complex communication at scale.&lt;/p&gt;

&lt;p&gt;SIG meetings provide focused opportunities for users, maintainers, and specialists from various disciplines to collaborate together in support of this community effort.  These investments in organizing help create an environment where it’s easier to prioritize architecture discussion and planning over commit velocity; enabling the project to sustain this kind of scale.&lt;/p&gt;

&lt;h2 id=&#34;join-the-party&#34;&gt;Join the party!&lt;/h2&gt;

&lt;p&gt;You may already be using solutions that are successfully managed and scaled on Kubernetes. For example, GitHub.com, which hosts Kubernetes’ upstream source code, &lt;a href=&#34;https://githubengineering.com/kubernetes-at-github/&#34; target=&#34;_blank&#34;&gt;now runs on Kubernetes&lt;/a&gt; as well!&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/guide/README.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Contributors’ guide&lt;/a&gt; for more information on how to get started as a contributor.&lt;/p&gt;

&lt;p&gt;You can also join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication#weekly-meeting&#34; target=&#34;_blank&#34;&gt;weekly Kubernetes Community meeting&lt;/a&gt; and consider &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md#master-sig-list&#34; target=&#34;_blank&#34;&gt;joining a SIG or two&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Application Survey 2018 Results</title>
      <link>https://kubernetes.io/blog/2018/04/24/kubernetes-application-survey-results-2018/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/04/24/kubernetes-application-survey-results-2018/</guid>
      <description>
        
        
        

&lt;p&gt;Understanding how people use or want to use Kubernetes can help us shape everything from what we build to how we do it. To help us understand how application developers, application operators, and ecosystem tool developers are using and want to use Kubernetes, the Application Definition Working Group recently performed a survey. The survey focused in on these types of user roles and the features and sub-projects owned by the Kubernetes organization. That included kubectl, Dashboard, Minikube, Helm, the Workloads API, etc.&lt;/p&gt;

&lt;p&gt;The results are in and the &lt;a href=&#34;https://docs.google.com/spreadsheets/d/12ilRCly2eHKPuicv1P_BD6z__PXAqpiaR-tDYe2eudE/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;raw data is now available&lt;/a&gt; for everyone.&lt;/p&gt;

&lt;p&gt;There is too much data to summarize in a single blog post and we hope people will be able to find useful information by pouring over the data. Here are some of the highlights that caught our attention.&lt;/p&gt;

&lt;h2 id=&#34;participation&#34;&gt;Participation&lt;/h2&gt;

&lt;p&gt;First, I would like to thank the 380 people who took the survey and provided feedback. We appreciate the time put into to share so much detail.&lt;/p&gt;

&lt;h2 id=&#34;6-8x-response-increase&#34;&gt;6.8x Response Increase&lt;/h2&gt;

&lt;p&gt;In the summer of &lt;a href=&#34;https://kubernetes.io/blog/2016/08/sig-apps-running-apps-in-kubernetes&#34; target=&#34;_blank&#34;&gt;2016 we ran a survey on application usage&lt;/a&gt;. Kubernetes was much newer and the number of people talking about operating applications was much smaller.&lt;/p&gt;

&lt;p&gt;The number of respondents in the past year and 10 months increased at a rate of 6.8 times.&lt;/p&gt;

&lt;h2 id=&#34;where-are-we-in-innovation-lifecycle&#34;&gt;Where Are We In Innovation Lifecycle?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/survey-results/2018-application-survey/minikube-os-usage.png&#34; alt=&#34;Minikube operating system usage&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Minikube is used primarily be people on MacOS and Linux. Yet, according to the 2018 Stack Overflow survey, &lt;a href=&#34;https://insights.stackoverflow.com/survey/2018/#technology-developers-primary-operating-systems&#34; target=&#34;_blank&#34;&gt;almost half of developers use Windows as their primary operating system&lt;/a&gt;. This is where Minikube would run.&lt;/p&gt;

&lt;p&gt;Seeing differences from other data sets is worth looking more deeply at to better understand our audience, where Kubernetes is at, and where it is on the journey it&amp;rsquo;s headed.&lt;/p&gt;

&lt;h2 id=&#34;plenty-of-custom-tooling&#34;&gt;Plenty of Custom Tooling&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/survey-results/2018-application-survey/custom-tooling.png&#34; alt=&#34;Custom Tooling&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Two thirds of respondents work for organizations developing their own tooling to help with application development and operation. We wondered why this might happen so we asked why as a follow-up question. &lt;em&gt;44% of people who took the survey told us why they do it.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;app-management-tools&#34;&gt;App Management Tools&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/survey-results/2018-application-survey/tool-manage-apps.png&#34; alt=&#34;Custom Tooling&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Only 4 tools were in use by more than 10% of those who took the survey with Helm being in use by 64% of them. Many more tools were used by more than 1% of people including those we directly asked about and the space for people to fill in those we didn&amp;rsquo;t ask about. The long tail, captured in the survey, contained more than 80 tools in use.&lt;/p&gt;

&lt;h2 id=&#34;want-to-see-more&#34;&gt;Want To See More?&lt;/h2&gt;

&lt;p&gt;As the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-app-def&#34; target=&#34;_blank&#34;&gt;Application Definition Working Group&lt;/a&gt; is working through the data we&amp;rsquo;re putting observations into a &lt;a href=&#34;http://bit.ly/2qTkuhx&#34; target=&#34;_blank&#34;&gt;Google Slides Document&lt;/a&gt;. This is a living document that will continue to grow while we look over and discuss the data.&lt;/p&gt;

&lt;p&gt;There is &lt;a href=&#34;https://kccnceu18.sched.com/event/DxV4&#34; target=&#34;_blank&#34;&gt;a session at KubeCon where the Application Definition Working Group will be meeting&lt;/a&gt; and discussing the survey. This is a session open to anyone in attendance, if you would like to attend.&lt;/p&gt;

&lt;p&gt;While this working group is doing analysis and sharing it, we want to encourage others to look at the data and share any insights that might be gained.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note, the survey questions were generated by the application definition working group with the help of people working on various sub-projects included in the survey. This is the reason some sub-projects have more and varied questions compared to some others. The survey was shared on social media, on mailing lists, in blog posts, in various meetings, and beyond while collecting information for two weeks.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Local Persistent Volumes for Kubernetes Goes Beta</title>
      <link>https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/</link>
      <pubDate>Fri, 13 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/</guid>
      <description>
        
        
        

&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#local&#34; target=&#34;_blank&#34;&gt;Local Persistent Volumes&lt;/a&gt; beta feature in Kubernetes 1.10 makes it possible to leverage local disks in your StatefulSets. You can specify directly-attached local disks as PersistentVolumes, and use them in StatefulSets with the same PersistentVolumeClaim objects that previously only supported remote volume types.&lt;/p&gt;

&lt;p&gt;Persistent storage is important for running stateful applications, and Kubernetes has supported these workloads with StatefulSets, PersistentVolumeClaims and PersistentVolumes. These primitives have supported remote volume types well, where the volumes can be accessed from any node in the cluster, but did not support local volumes, where the volumes can only be accessed from a specific node. The demand for using local, fast SSDs in replicated, stateful workloads has increased with demand to run more workloads in Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;addressing-hostpath-challenges&#34;&gt;Addressing hostPath challenges&lt;/h2&gt;

&lt;p&gt;The prior mechanism of accessing local storage through hostPath volumes had many challenges. hostPath volumes were difficult to use in production at scale: operators needed to care for local disk management, topology, and scheduling of individual pods when using hostPath volumes, and could not use many Kubernetes features (like StatefulSets). Existing Helm charts that used remote volumes could not be easily ported to use hostPath volumes. The Local Persistent Volumes feature aims to address hostPath volumes’ portability, disk accounting, and scheduling challenges.&lt;/p&gt;

&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Before going into details about how to use Local Persistent Volumes, note that local volumes are not suitable for most applications. Using local storage ties your application to that specific node, making your application harder to schedule. If that node or local volume encounters a failure and becomes inaccessible, then that pod also becomes inaccessible. In addition, many cloud providers do not provide extensive data durability guarantees for local storage, so you could lose all your data in certain scenarios.&lt;/p&gt;

&lt;p&gt;For those reasons, most applications should continue to use highly available, remotely accessible, durable storage.&lt;/p&gt;

&lt;h2 id=&#34;suitable-workloads&#34;&gt;Suitable workloads&lt;/h2&gt;

&lt;p&gt;Some use cases that are suitable for local storage include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Caching of datasets that can leverage data gravity for fast processing&lt;/li&gt;
&lt;li&gt;Distributed storage systems that shard or replicate data across multiple
nodes. Examples include distributed datastores like Cassandra, or distributed
file systems like Gluster or Ceph.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Suitable workloads are tolerant of node failures, data unavailability, and data loss. They provide critical, latency-sensitive infrastructure services to the rest of the cluster, and should run with high priority compared to other workloads.&lt;/p&gt;

&lt;h2 id=&#34;enabling-smarter-scheduling-and-volume-binding&#34;&gt;Enabling smarter scheduling and volume binding&lt;/h2&gt;

&lt;p&gt;An administrator must enable smarter scheduling for local persistent volumes. Before any PersistentVolumeClaims for your local PersistentVolumes are created, a StorageClass must be created with the volumeBindingMode set to “WaitForFirstConsumer”:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This setting tells the PersistentVolume controller to not immediately bind a PersistentVolumeClaim. Instead, the system waits until a Pod that needs to use a volume is scheduled. The scheduler then chooses an appropriate local PersistentVolume to bind to, taking into account the Pod’s other scheduling constraints and policies. This ensures that the initial volume binding is compatible with any Pod resource requirements, selectors, affinity and anti-affinity policies, and more.&lt;/p&gt;

&lt;p&gt;Note that dynamic provisioning is not supported in beta. All local PersistentVolumes must be statically created.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-local-persistent-volume&#34;&gt;Creating a local persistent volume&lt;/h2&gt;

&lt;p&gt;For this initial beta offering, local disks must first be pre-partitioned, formatted, and mounted on the local node by an administrator. Directories on a shared file system are also supported, but must also be created before use.&lt;/p&gt;

&lt;p&gt;Once you set up the local volume, you can create a PersistentVolume for it. In this example, the local volume is mounted at “/mnt/disks/vol1” on node “my-node”:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-local-pv
spec:
  capacity:
    storage: 500Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - my-node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that there’s a new nodeAffinity field in the PersistentVolume object: this is how the Kubernetes scheduler understands that this PersistentVolume is tied to a specific node. nodeAffinity is a required field for local PersistentVolumes.&lt;/p&gt;

&lt;p&gt;When local volumes are manually created like this, the only supported persistentVolumeReclaimPolicy is “Retain”. When the PersistentVolume is released from the PersistentVolumeClaim, an administrator must manually clean up and set up the local volume again for reuse.&lt;/p&gt;

&lt;h2 id=&#34;automating-local-volume-creation-and-deletion&#34;&gt;Automating local volume creation and deletion&lt;/h2&gt;

&lt;p&gt;Manually creating and cleaning up local volumes is a big administrative burden, so we’ve written a simple local volume manager to automate some of these pieces. It’s available in the &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume&#34; target=&#34;_blank&#34;&gt;external-storage repo&lt;/a&gt; as an optional program that you can deploy in your cluster, including instructions and example deployment specs for how to run it.&lt;/p&gt;

&lt;p&gt;To use this, the local volumes must still first be set up and mounted on the local node by an administrator. The administrator needs to mount the local volume into a configurable “discovery directory” that the local volume manager recognizes. Directories on a shared file system are supported, but they must be bind-mounted into the discovery directory.&lt;/p&gt;

&lt;p&gt;This local volume manager monitors the discovery directory, looking for any new mount points. The manager creates a PersistentVolume object with the appropriate storageClassName, path, nodeAffinity, and capacity for any new mount point that it detects. These PersistentVolume objects can eventually be claimed by PersistentVolumeClaims, and then mounted in Pods.&lt;/p&gt;

&lt;p&gt;After a Pod is done using the volume and deletes the PersistentVolumeClaim for it, the local volume manager cleans up the local mount by deleting all files from it, then deleting the PersistentVolume object. This triggers the discovery cycle: a new PersistentVolume is created for the volume and can be reused by a new PersistentVolumeClaim.&lt;/p&gt;

&lt;p&gt;Once the administrator initially sets up the local volume mount, this local volume manager takes over the rest of the PersistentVolume lifecycle without any further administrator intervention required.&lt;/p&gt;

&lt;h2 id=&#34;using-local-volumes-in-a-pod&#34;&gt;Using local volumes in a pod&lt;/h2&gt;

&lt;p&gt;After all that administrator work, how does a user actually mount a local volume into their Pod? Luckily from the user’s perspective, a local volume can be requested in exactly the same way as any other PersistentVolume type: through a PersistentVolumeClaim. Just specify the appropriate StorageClassName for local volumes in the PersistentVolumeClaim object, and the system takes care of the rest!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: example-local-claim
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or in a StatefulSet as a volumeClaimTemplate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StatefulSet
...
 volumeClaimTemplates:
  - metadata:
      name: example-local-claim
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: local-storage
      resources:
        requests:
          storage: 500Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;

&lt;p&gt;The Kubernetes website provides full documentation for &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#local&#34; target=&#34;_blank&#34;&gt;local persistent volumes&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;future-enhancements&#34;&gt;Future enhancements&lt;/h2&gt;

&lt;p&gt;The local persistent volume beta feature is not complete by far. Some notable enhancements under development:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Starting in 1.10, local raw block volumes is available as an alpha feature. This is useful for workloads that need to directly access block devices and manage their own data format.&lt;/li&gt;
&lt;li&gt;Dynamic provisioning of local volumes using LVM is under design and an alpha implementation will follow in a future release. This will eliminate the current need for an administrator to pre-partition, format and mount local volumes, as long as the workload’s performance requirements can tolerate sharing disks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;complementary-features&#34;&gt;Complementary features&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/&#34; target=&#34;_blank&#34;&gt;Pod priority and preemption&lt;/a&gt; is another Kubernetes feature that is complementary to local persistent volumes. When your application uses local storage, it must be scheduled to the specific node where the local volume resides. You can give your local storage workload high priority so if that node ran out of room to run your workload, Kubernetes can preempt lower priority workloads to make room for it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/&#34; target=&#34;_blank&#34;&gt;Pod disruption budget&lt;/a&gt; is also very important for those workloads that must maintain quorum. Setting a disruption budget for your workload ensures that it does not drop below quorum due to voluntary disruption events, such as node drains during upgrade.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature&#34; target=&#34;_blank&#34;&gt;Pod affinity and anti-affinity&lt;/a&gt; ensures that your workloads stay either co-located or spread out across failure domains. If you have multiple local persistent volumes available on a single node, it may be preferable to specify an pod anti-affinity policy to spread your workload across nodes. Note that if you want multiple pods to share the same local persistent volume, you do not need to specify a pod affinity policy. The scheduler understands the locality constraints of the local persistent volume and schedules your pod to the correct node.&lt;/p&gt;

&lt;h2 id=&#34;getting-involved&#34;&gt;Getting involved&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special-Interest-Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors from multiple companies that helped bring this feature to beta, including Cheng Xing (&lt;a href=&#34;https://github.com/verult&#34; target=&#34;_blank&#34;&gt;verult&lt;/a&gt;), David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;), Deyuan Deng (&lt;a href=&#34;https://github.com/ddysher&#34; target=&#34;_blank&#34;&gt;ddysher&lt;/a&gt;), Dhiraj Hedge (&lt;a href=&#34;https://github.com/dhirajh&#34; target=&#34;_blank&#34;&gt;dhirajh&lt;/a&gt;), Ian Chakeres (&lt;a href=&#34;https://github.com/ianchakeres&#34; target=&#34;_blank&#34;&gt;ianchakeres&lt;/a&gt;), Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;), Matthew Wong (&lt;a href=&#34;https://github.com/wongma7&#34; target=&#34;_blank&#34;&gt;wongma7&lt;/a&gt;), Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;), Serguei Bezverkhi (&lt;a href=&#34;https://github.com/sbezverk&#34; target=&#34;_blank&#34;&gt;sbezverk&lt;/a&gt;), and Yuquan Ren (&lt;a href=&#34;https://github.com/nickrenren&#34; target=&#34;_blank&#34;&gt;nickrenren&lt;/a&gt;).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Migrating the Kubernetes Blog</title>
      <link>https://kubernetes.io/blog/2018/04/11/migrating-the-kubernetes-blog/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/04/11/migrating-the-kubernetes-blog/</guid>
      <description>
        
        
        

&lt;p&gt;We recently migrated the Kubernetes Blog from the Blogger platform to GitHub. With the change in platform comes a change in URL: formerly at &lt;a href=&#34;http://blog.kubernetes.io&#34; target=&#34;_blank&#34;&gt;http://blog.kubernetes.io&lt;/a&gt;, the blog now resides at &lt;a href=&#34;https://kubernetes.io/blog&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All existing posts redirect from their former URLs with &lt;code&gt;&amp;lt;rel=canonical&amp;gt;&lt;/code&gt; tags, preserving SEO values.&lt;/p&gt;

&lt;h3 id=&#34;why-and-how-we-migrated-the-blog&#34;&gt;Why and how we migrated the blog&lt;/h3&gt;

&lt;p&gt;Our primary reasons for migrating were to streamline blog submissions and reviews, and to make the overall blog process faster and more transparent. Blogger&amp;rsquo;s web interface made it difficult to provide drafts to multiple reviewers without also granting unnecessary access permissions and compromising security. GitHub&amp;rsquo;s review process offered clear improvements.&lt;/p&gt;

&lt;p&gt;We learned from &lt;a href=&#34;https://www.ybrikman.com&#34; target=&#34;_blank&#34;&gt;Jim Brikman&lt;/a&gt;&amp;rsquo;s experience during &lt;a href=&#34;https://www.ybrikman.com/writing/2015/04/20/migrating-from-blogger-to-github-pages/&#34; target=&#34;_blank&#34;&gt;his own site migration&lt;/a&gt; away from Blogger.&lt;/p&gt;

&lt;p&gt;Our migration was broken into several pull requests, but you can see the work that went into the &lt;a href=&#34;https://github.com/kubernetes/website/pull/7247&#34; target=&#34;_blank&#34;&gt;primary migration PR&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We hope that making blog submissions more accessible will encourage greater community involvement in creating and reviewing blog content.&lt;/p&gt;

&lt;h3 id=&#34;how-to-submit-a-blog-post&#34;&gt;How to Submit a Blog Post&lt;/h3&gt;

&lt;p&gt;You can submit a blog post for consideration one of two ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Submit a Google Doc through the &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSch_phFYMTYlrTDuYziURP6nLMijoXx_f7sLABEU5gWBtxJHQ/viewform&#34; target=&#34;_blank&#34;&gt;blog submission form&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open a pull request against the &lt;a href=&#34;https://github.com/kubernetes/website/tree/master/content/en/blog/_posts&#34; target=&#34;_blank&#34;&gt;website repository&lt;/a&gt; as described &lt;a href=&#34;https://kubernetes.io/docs/home/contribute/create-pull-request/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have a post that you want to remain confidential until your publish date, please submit your post via the Google form. Otherwise, you can choose your submission process based on your comfort level and preferred workflow.&lt;/p&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; Our workflow hasn&amp;rsquo;t changed for confidential advance drafts. Additionally, we&amp;rsquo;ll coordinate publishing for time sensitive posts to ensure that information isn&amp;rsquo;t released prematurely through an open pull request.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;h3 id=&#34;call-for-reviewers&#34;&gt;Call for reviewers&lt;/h3&gt;

&lt;p&gt;The Kubernetes Blog needs more reviewers! If you&amp;rsquo;re interested in contributing to the Kubernetes project and can participate on a regular, weekly basis, send an introductory email to &lt;a href=&#34;k8sblog@linuxfoundation.org&#34; target=&#34;_blank&#34;&gt;k8sblog@linuxfoundation.org&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Container Storage Interface (CSI) for Kubernetes Goes Beta</title>
      <link>https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-kubernetes.png&#34; alt=&#34;Kubernetes Logo&#34; /&gt;
&lt;img src=&#34;https://kubernetes.io/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-logo.png&#34; alt=&#34;CSI Logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Kubernetes implementation of the Container Storage Interface (CSI) is now beta in Kubernetes v1.10. CSI was &lt;a href=&#34;https://kubernetes.io/blog/2018/01/introducing-container-storage-interface&#34; target=&#34;_blank&#34;&gt;introduced as alpha&lt;/a&gt; in Kubernetes v1.9.&lt;/p&gt;

&lt;p&gt;Kubernetes features are generally introduced as alpha and moved to beta (and eventually to stable/GA) over subsequent Kubernetes releases. This process allows Kubernetes developers to get feedback, discover and fix issues, iterate on the designs, and deliver high quality, production grade features.&lt;/p&gt;

&lt;h2 id=&#34;why-introduce-container-storage-interface-in-kubernetes&#34;&gt;Why introduce Container Storage Interface in Kubernetes?&lt;/h2&gt;

&lt;p&gt;Although Kubernetes already provides a powerful volume plugin system that makes it easy to consume different types of block and file storage, adding support for new volume plugins has been challenging. Because volume plugins are currently “in-tree”—volume plugins are part of the core Kubernetes code and shipped with the core Kubernetes binaries—vendors wanting to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) must align themselves with the Kubernetes release process.&lt;/p&gt;

&lt;p&gt;With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Third party storage developers can now write and deploy volume plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This will result in even more options for the storage that backs Kubernetes users’ stateful containerized workloads.&lt;/p&gt;

&lt;h2 id=&#34;what-s-new-in-beta&#34;&gt;What’s new in Beta?&lt;/h2&gt;

&lt;p&gt;With the promotion to beta CSI is now enabled by default on standard Kubernetes deployments instead of being opt-in.&lt;/p&gt;

&lt;p&gt;The move of the Kubernetes implementation of CSI to beta also means:
* Kubernetes is compatible with &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v0.2.0&#34; target=&#34;_blank&#34;&gt;v0.2&lt;/a&gt; of the CSI spec (instead of &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v0.1.0&#34; target=&#34;_blank&#34;&gt;v0.1&lt;/a&gt;)
  * There were breaking changes between the CSI spec v0.1 and v0.2, so existing CSI drivers must be updated to be 0.2 compatible before use with Kubernetes 1.10.0+.
* &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation&#34; target=&#34;_blank&#34;&gt;Mount propagation&lt;/a&gt;, a feature that allows bidirectional mounts between containers and host (a requirement for containerized CSI drivers), has also moved to beta.
* The Kubernetes &lt;code&gt;VolumeAttachment&lt;/code&gt; object, introduced in v1.9 in the storage v1alpha1 group, has been added to the storage v1beta1 group.
* The Kubernetes &lt;code&gt;CSIPersistentVolumeSource&lt;/code&gt; object has been promoted to beta.
A &lt;code&gt;VolumeAttributes&lt;/code&gt; field was added to Kubernetes &lt;code&gt;CSIPersistentVolumeSource&lt;/code&gt; object (in alpha this was passed around via annotations).
* Node authorizer has been updated to limit access to &lt;code&gt;VolumeAttachment&lt;/code&gt; objects from kubelet.
* The Kubernetes &lt;code&gt;CSIPersistentVolumeSource&lt;/code&gt; object and the CSI external-provisioner have been modified to allow passing of secrets to the CSI volume plugin.
* The Kubernetes &lt;code&gt;CSIPersistentVolumeSource&lt;/code&gt; has been modified to allow passing in filesystem type (previously always assumed to be &lt;code&gt;ext4&lt;/code&gt;).
* A new optional call, &lt;code&gt;NodeStageVolume&lt;/code&gt;, has been added to the CSI spec, and the Kubernetes CSI volume plugin has been modified to call &lt;code&gt;NodeStageVolume&lt;/code&gt; during &lt;code&gt;MountDevice&lt;/code&gt; (in alpha this step was a no-op).&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-deploy-a-csi-driver-on-a-kubernetes-cluster&#34;&gt;How do I deploy a CSI driver on a Kubernetes Cluster?&lt;/h2&gt;

&lt;p&gt;CSI plugin authors must provide their own instructions for deploying their plugin on Kubernetes.&lt;/p&gt;

&lt;p&gt;The Kubernetes-CSI implementation team created a &lt;a href=&#34;https://kubernetes-csi.github.io/docs/Example.html&#34; target=&#34;_blank&#34;&gt;sample hostpath CSI driver&lt;/a&gt;. The sample provides a rough idea of what the deployment process for a CSI driver looks like. Production drivers, however, would deploy node components via a DaemonSet and controller components via a StatefulSet rather than a single pod (for example, see the deployment files for the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/compute-persistent-disk-csi-driver/blob/master/deploy/kubernetes/README.md&#34; target=&#34;_blank&#34;&gt;GCE PD driver&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-use-a-csi-volume-in-my-kubernetes-pod&#34;&gt;How do I use a CSI Volume in my Kubernetes pod?&lt;/h2&gt;

&lt;p&gt;Assuming a CSI storage plugin is already deployed on your cluster, you can use it through the familiar Kubernetes storage primitives: &lt;code&gt;PersistentVolumeClaims&lt;/code&gt;, &lt;code&gt;PersistentVolumes&lt;/code&gt;, and &lt;code&gt;StorageClasses&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;CSI is a beta feature in Kubernetes v1.10. Although it is enabled by default, it may require the following flag:
* API server binary and kubelet binaries:
  * &lt;code&gt;--allow-privileged=true&lt;/code&gt;
    * Most CSI plugins will require bidirectional mount propagation, which can only be enabled for privileged pods. Privileged pods are only permitted on clusters where this flag has been set to true (this is the default in some environments like GCE, GKE, and kubeadm).&lt;/p&gt;

&lt;h3 id=&#34;dynamic-provisioning&#34;&gt;Dynamic Provisioning&lt;/h3&gt;

&lt;p&gt;You can enable automatic creation/deletion of volumes for CSI Storage plugins that support dynamic provisioning by creating a &lt;code&gt;StorageClass&lt;/code&gt; pointing to the CSI plugin.&lt;/p&gt;

&lt;p&gt;The following &lt;code&gt;StorageClass&lt;/code&gt;, for example, enables dynamic creation of “&lt;code&gt;fast-storage&lt;/code&gt;” volumes by a CSI volume plugin called “&lt;code&gt;com.example.csi-driver&lt;/code&gt;”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast-storage
provisioner: com.example.csi-driver
parameters:
  type: pd-ssd
  csiProvisionerSecretName: mysecret
  csiProvisionerSecretNamespace: mynamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;New for beta, the &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;default CSI external-provisioner&lt;/a&gt; reserves the parameter keys &lt;code&gt;csiProvisionerSecretName&lt;/code&gt; and &lt;code&gt;csiProvisionerSecretNamespace&lt;/code&gt;. If specified, it fetches the secret and passes it to the CSI driver during provisioning.&lt;/p&gt;

&lt;p&gt;Dynamic provisioning is triggered by the creation of a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object. The following &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, for example, triggers dynamic provisioning using the &lt;code&gt;StorageClass&lt;/code&gt; above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-request-for-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When volume provisioning is invoked, the parameter type: &lt;code&gt;pd-ssd&lt;/code&gt; and the secret any referenced secret(s) are passed to the CSI plugin &lt;code&gt;com.example.csi-driver&lt;/code&gt; via a &lt;code&gt;CreateVolume call&lt;/code&gt;. In response, the external volume plugin provisions a new volume and then automatically create a &lt;code&gt;PersistentVolume&lt;/code&gt; object to represent the new volume. Kubernetes then binds the new &lt;code&gt;PersistentVolume&lt;/code&gt; object to the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, making it ready to use.&lt;/p&gt;

&lt;p&gt;If the fast-storage  StorageClass is marked as “default”, there is no need to include the storageClassName in the PersistentVolumeClaim, it will be used by default.&lt;/p&gt;

&lt;h3 id=&#34;pre-provisioned-volumes&#34;&gt;Pre-Provisioned Volumes&lt;/h3&gt;

&lt;p&gt;You can always expose a pre-existing volume in Kubernetes by manually creating a &lt;code&gt;PersistentVolume&lt;/code&gt; object to represent the existing volume. The following &lt;code&gt;PersistentVolume&lt;/code&gt;, for example, exposes a volume with the name “&lt;code&gt;existingVolumeName&lt;/code&gt;” belonging to a CSI storage plugin called “&lt;code&gt;com.example.csi-driver&lt;/code&gt;”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-manually-created-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: com.example.csi-driver
    volumeHandle: existingVolumeName
    readOnly: false
    fsType: ext4
    volumeAttributes:
      foo: bar
    controllerPublishSecretRef:
      name: mysecret1
      namespace: mynamespace
    nodeStageSecretRef:
      name: mysecret2
      namespace: mynamespace
    nodePublishSecretRef
      name: mysecret3
      namespace: mynamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;attaching-and-mounting&#34;&gt;Attaching and Mounting&lt;/h3&gt;

&lt;p&gt;You can reference a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; that is bound to a CSI volume in any pod or pod template.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
    - name: my-frontend
      image: nginx
      volumeMounts:
      - mountPath: &amp;quot;/var/www/html&amp;quot;
        name: my-csi-volume
  volumes:
    - name: my-csi-volume
      persistentVolumeClaim:
        claimName: my-request-for-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the pod referencing a CSI volume is scheduled, Kubernetes will trigger the appropriate operations against the external CSI plugin (&lt;code&gt;ControllerPublishVolume&lt;/code&gt;, &lt;code&gt;NodeStageVolume&lt;/code&gt;, &lt;code&gt;NodePublishVolume&lt;/code&gt;, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.&lt;/p&gt;

&lt;p&gt;For more details please see the CSI implementation &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md&#34; target=&#34;_blank&#34;&gt;design doc&lt;/a&gt; and &lt;a href=&#34;https://kubernetes-csi.github.io/&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-write-a-csi-driver&#34;&gt;How do I write a CSI driver?&lt;/h2&gt;

&lt;p&gt;CSI Volume Driver deployments on Kubernetes must meet some &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#third-party-csi-volume-drivers&#34; target=&#34;_blank&#34;&gt;minimum requirements&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The minimum requirements document also outlines the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#recommended-mechanism-for-deploying-csi-drivers-on-kubernetes&#34; target=&#34;_blank&#34;&gt;suggested mechanism&lt;/a&gt; for deploying an arbitrary containerized CSI driver on Kubernetes. This mechanism can be used by a Storage Provider to simplify deployment of containerized CSI compatible volume drivers on Kubernetes.&lt;/p&gt;

&lt;p&gt;As part of the suggested deployment process, the Kubernetes team provides the following sidecar (helper) containers:
* &lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher&#34; target=&#34;_blank&#34;&gt;external-attacher&lt;/a&gt;
  * watches Kubernetes &lt;code&gt;VolumeAttachment&lt;/code&gt; objects and triggers &lt;code&gt;ControllerPublish&lt;/code&gt; and &lt;code&gt;ControllerUnpublish&lt;/code&gt; operations against a CSI endpoint
* &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;external-provisioner&lt;/a&gt;
  * watches Kubernetes &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; objects and triggers &lt;code&gt;CreateVolume&lt;/code&gt; and &lt;code&gt;DeleteVolume&lt;/code&gt; operations against a CSI endpoint
* &lt;a href=&#34;https://github.com/kubernetes-csi/driver-registrar&#34; target=&#34;_blank&#34;&gt;driver-registrar&lt;/a&gt;
  * registers the CSI driver with kubelet (in the future) and adds the drivers custom &lt;code&gt;NodeId&lt;/code&gt; (retrieved via &lt;code&gt;GetNodeID&lt;/code&gt; call against the CSI endpoint) to an annotation on the Kubernetes Node API Object
* &lt;a href=&#34;https://github.com/kubernetes-csi/livenessprobe&#34; target=&#34;_blank&#34;&gt;livenessprobe&lt;/a&gt;
  * can be included in a CSI plugin pod to enable the &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34; target=&#34;_blank&#34;&gt;Kubernetes Liveness Probe&lt;/a&gt; mechanism&lt;/p&gt;

&lt;p&gt;Storage vendors can build Kubernetes deployments for their plugins using these components, while leaving their CSI driver completely unaware of Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;where-can-i-find-csi-drivers&#34;&gt;Where can I find CSI drivers?&lt;/h2&gt;

&lt;p&gt;CSI drivers are developed and maintained by third parties. You can find a non-definitive list of some &lt;a href=&#34;https://kubernetes-csi.github.io/docs/Drivers.html&#34; target=&#34;_blank&#34;&gt;sample and production CSI drivers&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-about-flexvolumes&#34;&gt;What about FlexVolumes?&lt;/h2&gt;

&lt;p&gt;As mentioned in the &lt;a href=&#34;https://kubernetes.io/blog/2018/01/introducing-container-storage-interface&#34; target=&#34;_blank&#34;&gt;alpha release blog post&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md&#34; target=&#34;_blank&#34;&gt;FlexVolume plugin&lt;/a&gt; was an earlier attempt to make the Kubernetes volume plugin system extensible. Although it enables third party storage vendors to write drivers “out-of-tree”, because it is an exec based API, FlexVolumes requires files for third party driver binaries (or scripts) to be copied to a special plugin directory on the root filesystem of every node (and, in some cases, master) machine. This requires a cluster admin to have write access to the host filesystem for each node and some external mechanism to ensure that the driver file is recreated if deleted, just to deploy a volume plugin.&lt;/p&gt;

&lt;p&gt;In addition to being difficult to deploy, Flex did not address the pain of plugin dependencies: Volume plugins tend to have many external requirements (on mount and filesystem tools, for example). These dependencies are assumed to be available on the underlying host OS, which is often not the case.&lt;/p&gt;

&lt;p&gt;CSI addresses these issues by not only enabling storage plugins to be developed out-of-tree, but also containerized and deployed via standard Kubernetes primitives.&lt;/p&gt;

&lt;p&gt;If you still have questions about in-tree volumes vs CSI vs Flex, please see the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md&#34; target=&#34;_blank&#34;&gt;Volume Plugin FAQ&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-will-happen-to-the-in-tree-volume-plugins&#34;&gt;What will happen to the in-tree volume plugins?&lt;/h2&gt;

&lt;p&gt;Once CSI reaches stability, we plan to migrate most of the in-tree volume plugins to CSI. Stay tuned for more details as the Kubernetes CSI implementation approaches stable.&lt;/p&gt;

&lt;h2 id=&#34;what-are-the-limitations-of-beta&#34;&gt;What are the limitations of beta?&lt;/h2&gt;

&lt;p&gt;The beta implementation of CSI has the following limitations:
* Block volumes are not supported; only file.
* CSI drivers must be deployed with the provided external-attacher sidecar plugin, even if they don’t implement &lt;code&gt;ControllerPublishVolume&lt;/code&gt;.
* Topology awareness is not supported for CSI volumes, including the ability to share information about where a volume is provisioned (zone, regions, etc.) with the Kubernetes scheduler to allow it to make smarter scheduling decisions, and the ability for the Kubernetes scheduler or a cluster administrator or an application developer to specify where a volume should be provisioned.
* &lt;code&gt;driver-registrar&lt;/code&gt; requires permissions to modify all Kubernetes node API objects which could result in a compromised node gaining the ability to do the same.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI implementation to GA in 1.12.&lt;/p&gt;

&lt;p&gt;The team would like to encourage storage vendors to start developing CSI drivers, deploying them on Kubernetes, and sharing feedback with the team via the Kubernetes Slack channel &lt;a href=&#34;https://kubernetes.slack.com/messages/C8EJ01Z46/details/&#34; target=&#34;_blank&#34;&gt;wg-csi&lt;/a&gt;, the Google group &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-storage-wg-csi&#34; target=&#34;_blank&#34;&gt;kubernetes-sig-storage-wg-csi&lt;/a&gt;, or any of the standard &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact&#34; target=&#34;_blank&#34;&gt;SIG storage communication channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;

&lt;p&gt;In addition to the contributors who have been working on the Kubernetes implementation of CSI since alpha:
* Bradley Childs (&lt;a href=&#34;https://github.com/childsb&#34; target=&#34;_blank&#34;&gt;childsb&lt;/a&gt;)
* Chakravarthy Nelluri (&lt;a href=&#34;https://github.com/chakri-nelluri&#34; target=&#34;_blank&#34;&gt;chakri-nelluri&lt;/a&gt;)
* Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;)
* Luis Pabón (&lt;a href=&#34;https://github.com/lpabon&#34; target=&#34;_blank&#34;&gt;lpabon&lt;/a&gt;)
* Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;saad-ali&lt;/a&gt;)
* Vladimir Vivien (&lt;a href=&#34;https://github.com/vladimirvivien&#34; target=&#34;_blank&#34;&gt;vladimirvivien&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;We offer a huge thank you to the new contributors who stepped up this quarter to help the project reach beta:
* David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;)
* Edison Xiang (&lt;a href=&#34;https://github.com/edisonxiang&#34; target=&#34;_blank&#34;&gt;edisonxiang&lt;/a&gt;)
* Felipe Musse (&lt;a href=&#34;https://github.com/musse&#34; target=&#34;_blank&#34;&gt;musse&lt;/a&gt;)
* Lin Ml (&lt;a href=&#34;https://github.com/mlmhl&#34; target=&#34;_blank&#34;&gt;mlmhl&lt;/a&gt;)
* Lin Youchong (&lt;a href=&#34;https://github.com/linyouchong&#34; target=&#34;_blank&#34;&gt;linyouchong&lt;/a&gt;)
* Pietro Menna (&lt;a href=&#34;https://github.com/pietromenna&#34; target=&#34;_blank&#34;&gt;pietromenna&lt;/a&gt;)
* Serguei Bezverkhi (&lt;a href=&#34;https://github.com/sbezverk&#34; target=&#34;_blank&#34;&gt;sbezverk&lt;/a&gt;)
* Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;)
* Yuquan Ren (&lt;a href=&#34;https://github.com/NickrenREN&#34; target=&#34;_blank&#34;&gt;NickrenREN&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Fixing the Subpath Volume Vulnerability in Kubernetes</title>
      <link>https://kubernetes.io/blog/2018/04/04/fixing-subpath-volume-vulnerability/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/04/04/fixing-subpath-volume-vulnerability/</guid>
      <description>
        
        
        

&lt;p&gt;On March 12, 2018, the Kubernetes Product Security team disclosed &lt;a href=&#34;https://issue.k8s.io/60813&#34; target=&#34;_blank&#34;&gt;CVE-2017-1002101&lt;/a&gt;, which allowed containers using &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath&#34; target=&#34;_blank&#34;&gt;subpath&lt;/a&gt; volume mounts to access files outside of the volume. This means that a container could access any file available on the host, including volumes for other containers that it should not have access to.&lt;/p&gt;

&lt;p&gt;The vulnerability has been fixed and released in the latest Kubernetes patch releases. We recommend that all users upgrade to get the fix. For more details on the impact and how to get the fix, please see the &lt;a href=&#34;https://groups.google.com/forum/#!topic/kubernetes-announce/6sNHO_jyBzE&#34; target=&#34;_blank&#34;&gt;announcement&lt;/a&gt;. (Note, some functional regressions were found after the initial fix and are being tracked in &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/61563&#34; target=&#34;_blank&#34;&gt;issue #61563&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This post presents a technical deep dive on the vulnerability and the solution.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-background&#34;&gt;Kubernetes Background&lt;/h2&gt;

&lt;p&gt;To understand the vulnerability, one must first understand how volume and subpath mounting works in Kubernetes.&lt;/p&gt;

&lt;p&gt;Before a container is started on a node, the kubelet volume manager locally mounts all the volumes specified in the PodSpec under a directory for that Pod on the host system. Once all the volumes are successfully mounted, it constructs the list of volume mounts to pass to the container runtime. Each volume mount contains information that the container runtime needs, the most relevant being:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Path of the volume in the container&lt;/li&gt;
&lt;li&gt;Path of the volume on the host (&lt;code&gt;/var/lib/kubelet/pods/&amp;lt;pod uid&amp;gt;/volumes/&amp;lt;volume type&amp;gt;/&amp;lt;volume name&amp;gt;&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When starting the container, the container runtime creates the path in the container root filesystem, if necessary, and then bind mounts it to the provided host path.&lt;/p&gt;

&lt;p&gt;Subpath mounts are passed to the container runtime just like any other volume. The container runtime does not distinguish between a base volume and a subpath volume, and handles them the same way. Instead of passing the host path to the root of the volume, Kubernetes constructs the host path by appending the Pod-specified subpath (a relative path) to the base volume’s host path.&lt;/p&gt;

&lt;p&gt;For example, here is a spec for a subpath volume mount:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    &amp;lt;snip&amp;gt;
    volumeMounts:
    - mountPath: /mnt/data
      name: my-volume
      subPath: dataset1
  volumes:
  - name: my-volume
    emptyDir: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, when the Pod gets scheduled to a node, the system will:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set up an EmptyDir volume at &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Construct the host path for the subpath mount: &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/ + dataset1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pass the following mount information to the container runtime:

&lt;ul&gt;
&lt;li&gt;Container path: &lt;code&gt;/mnt/data&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host path: &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/dataset1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The container runtime bind mounts &lt;code&gt;/mnt/data&lt;/code&gt; in the container root filesystem to &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/dataset1&lt;/code&gt; on the host.&lt;/li&gt;
&lt;li&gt;The container runtime starts the container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-vulnerability&#34;&gt;The Vulnerability&lt;/h2&gt;

&lt;p&gt;The vulnerability with subpath volumes was discovered by Maxim Ivanov, by making a few observations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Subpath references files or directories that are controlled by the user, not the system.&lt;/li&gt;
&lt;li&gt;Volumes can be shared by containers that are brought up at different times in the Pod lifecycle, including by different Pods.&lt;/li&gt;
&lt;li&gt;Kubernetes passes host paths to the container runtime to bind mount into the container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The basic example below demonstrates the vulnerability. It takes advantage of the observations outlined above by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Using an init container to setup the volume with a symlink.&lt;/li&gt;
&lt;li&gt;Using a regular container to mount that symlink as a subpath later.&lt;/li&gt;
&lt;li&gt;Causing kubelet to evaluate the symlink on the host before passing it into the container runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  initContainers:
  - name: prep-symlink
    image: &amp;quot;busybox&amp;quot;
    command: [&amp;quot;bin/sh&amp;quot;, &amp;quot;-ec&amp;quot;, &amp;quot;ln -s / /mnt/data/symlink-door&amp;quot;]
    volumeMounts:
    - name: my-volume
      mountPath: /mnt/data
  containers:
  - name: my-container
    image: &amp;quot;busybox&amp;quot;
    command: [&amp;quot;/bin/sh&amp;quot;, &amp;quot;-ec&amp;quot;, &amp;quot;ls /mnt/data; sleep 999999&amp;quot;]
    volumeMounts:
    - mountPath: /mnt/data
      name: my-volume
      subPath: symlink-door
  volumes:
  - name: my-volume
    emptyDir: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this example, the system will:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Setup an EmptyDir volume at &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pass the following mount information for the init container to the container runtime:

&lt;ul&gt;
&lt;li&gt;Container path: &lt;code&gt;/mnt/data&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host path: &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The container runtime bind mounts &lt;code&gt;/mnt/data&lt;/code&gt; in the container root filesystem to &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume&lt;/code&gt; on the host.&lt;/li&gt;
&lt;li&gt;The container runtime starts the init container.&lt;/li&gt;
&lt;li&gt;The init container creates a symlink inside the container: &lt;code&gt;/mnt/data/symlink-door&lt;/code&gt; -&amp;gt; &lt;code&gt;/&lt;/code&gt;, and then exits.&lt;/li&gt;
&lt;li&gt;Kubelet starts to prepare the volume mounts for the normal containers.&lt;/li&gt;
&lt;li&gt;It constructs the host path for the subpath volume mount: &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/ + symlink-door&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;And passes the following mount information to the container runtime:

&lt;ul&gt;
&lt;li&gt;Container path: &lt;code&gt;/mnt/data&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host path: &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/symlink-door&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The container runtime bind mounts &lt;code&gt;/mnt/data&lt;/code&gt; in the container root filesystem to &lt;code&gt;/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty~dir/my-volume/symlink-door&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;However, the bind mount resolves symlinks, which in this case, resolves to &lt;code&gt;/&lt;/code&gt; on the host! Now the container can see all of the host’s filesystem through its mount point &lt;code&gt;/mnt/data&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a manifestation of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Symlink_race&#34; target=&#34;_blank&#34;&gt;symlink race&lt;/a&gt;, where a malicious user program can gain access to sensitive data by causing a privileged program (in this case, kubelet) to follow a user-created symlink.&lt;/p&gt;

&lt;p&gt;It should be noted that init containers are not always required for this exploit, depending on the volume type. It is used in the EmptyDir example because EmptyDir volumes cannot be shared with other Pods, and only created when a Pod is created, and destroyed when the Pod is destroyed. For persistent volume types, this exploit can also be done across two different Pods sharing the same volume.&lt;/p&gt;

&lt;h2 id=&#34;the-fix&#34;&gt;The Fix&lt;/h2&gt;

&lt;p&gt;The underlying issue is that the host path for subpaths are untrusted and can point anywhere in the system. The fix needs to ensure that this host path is both:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Resolved and validated to point inside the base volume.&lt;/li&gt;
&lt;li&gt;Not changeable by the user in between the time of validation and when the container runtime bind mounts it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Kubernetes product security team went through many iterations of possible solutions before finally agreeing on a design.&lt;/p&gt;

&lt;h3 id=&#34;idea-1&#34;&gt;Idea 1&lt;/h3&gt;

&lt;p&gt;Our first design was relatively simple. For each subpath mount in each container:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Resolve all the symlinks for the subpath.&lt;/li&gt;
&lt;li&gt;Validate that the resolved path is within the volume.&lt;/li&gt;
&lt;li&gt;Pass the resolved path to the container runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, this design is prone to the classic time-of-check-to-time-of-use (&lt;a href=&#34;https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use&#34; target=&#34;_blank&#34;&gt;TOCTTOU&lt;/a&gt;) problem. In between steps 2) and 3), the user could change the path back to a symlink. The proper solution needs some way to “lock” the path so that it cannot be changed in between validation and bind mounting by the container runtime. All the subsequent ideas use an intermediate bind mount by kubelet to achieve this “lock” step before handing it off to the container runtime. Once a bind mount is performed, the mount source is fixed and cannot be changed.&lt;/p&gt;

&lt;h3 id=&#34;idea-2&#34;&gt;Idea 2&lt;/h3&gt;

&lt;p&gt;We went a bit wild with this idea:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create a working directory under the kubelet’s pod directory. Let’s  call it &lt;code&gt;dir1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Bind mount the base volume to under the working directory, &lt;code&gt;dir1/volume&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Chroot to the working directory &lt;code&gt;dir1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Inside the chroot, bind mount &lt;code&gt;volume/subpath&lt;/code&gt; to &lt;code&gt;subpath&lt;/code&gt;.  This ensures that any symlinks get resolved to inside the chroot environment.&lt;/li&gt;
&lt;li&gt;Exit the chroot.&lt;/li&gt;
&lt;li&gt;On the host again, pass the bind mounted &lt;code&gt;dir1/subpath&lt;/code&gt; to the container runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While this design does ensure that the symlinks cannot point outside of the volume, it was ultimately rejected due to difficulties of implementing the chroot mechanism in 4) across all the various distros and environments that Kubernetes has to support, including containerized kubelets.&lt;/p&gt;

&lt;h3 id=&#34;idea-3&#34;&gt;Idea 3&lt;/h3&gt;

&lt;p&gt;Coming back to earth a little bit, our next idea was to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bind mount the subpath to a working directory under the kubelet’s pod directory.&lt;/li&gt;
&lt;li&gt;Get the source of the bind mount, and validate that it is within the base volume.&lt;/li&gt;
&lt;li&gt;Pass the bind mount to the container runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In theory, this sounded pretty simple, but in reality, 2) was quite difficult to implement correctly. Many scenarios had to be handled where volumes (like EmptyDir) could be on a shared filesystem, on a separate filesystem, on the root filesystem, or not on the root filesystem. NFS volumes ended up handling all bind mounts as a separate mount, instead of as a child to the base volume. There was additional uncertainty about how out-of-tree volume types (that we couldn’t test) would behave.&lt;/p&gt;

&lt;h3 id=&#34;the-solution&#34;&gt;The Solution&lt;/h3&gt;

&lt;p&gt;Given the amount of scenarios and corner cases that had to be handled with the previous design, we really wanted to find a solution that was more generic across all volume types. The final design that we ultimately went with was to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Resolve all the symlinks in the subpath.&lt;/li&gt;
&lt;li&gt;Starting with the base volume, open each path segment one by one, using the &lt;code&gt;openat()&lt;/code&gt; syscall, and disallow symlinks. With each path segment, validate that the current path is within the base volume.&lt;/li&gt;
&lt;li&gt;Bind mount &lt;code&gt;/proc/&amp;lt;kubelet pid&amp;gt;/fd/&amp;lt;final fd&amp;gt;&lt;/code&gt; to a working directory under the kubelet’s pod directory. The proc file is a link to the opened file. If that file gets replaced while kubelet still has it open, then the link will still point to the original file.&lt;/li&gt;
&lt;li&gt;Close the fd and pass the bind mount to the container runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that this solution is different for Windows hosts, where the mounting semantics are different than Linux. In Windows, the design is to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Resolve all the symlinks in the subpath.&lt;/li&gt;
&lt;li&gt;Starting with the base volume, open each path segment one by one with a file lock, and disallow symlinks. With each path segment, validate that the current path is within the base volume.&lt;/li&gt;
&lt;li&gt;Pass the resolved subpath to the container runtime, and start the container.&lt;/li&gt;
&lt;li&gt;After the container has started, unlock and close all the files.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both solutions are able to address all the requirements of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Resolving the subpath and validating that it points to a path inside the base volume.&lt;/li&gt;
&lt;li&gt;Ensuring that the subpath host path cannot be changed in between the time of validation and when the container runtime bind mounts it.&lt;/li&gt;
&lt;li&gt;Being generic enough to support all volume types.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Special thanks to many folks involved with handling this vulnerability:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Maxim Ivanov, who responsibly disclosed the vulnerability to the Kubernetes Product Security team.&lt;/li&gt;
&lt;li&gt;Kubernetes storage and security engineers from Google, Microsoft, and RedHat, who developed, tested, and reviewed the fixes.&lt;/li&gt;
&lt;li&gt;Kubernetes test-infra team, for setting up the private build infrastructure&lt;/li&gt;
&lt;li&gt;Kubernetes patch release managers, for coordinating and handling all the releases.&lt;/li&gt;
&lt;li&gt;All the production release teams that worked to deploy the fix quickly after release.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you find a vulnerability in Kubernetes, please follow our responsible disclosure process and &lt;a href=&#34;https://kubernetes.io/security/#report-a-vulnerability&#34; target=&#34;_blank&#34;&gt;let us know&lt;/a&gt;; we want to do our best to make Kubernetes secure for all users.&lt;/p&gt;

&lt;p&gt;&amp;ndash; Michelle Au, Software Engineer, Google; and Jan Šafránek, Software Engineer, Red Hat&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.10: Stabilizing Storage, Security, and Networking </title>
      <link>https://kubernetes.io/blog/2018/03/26/kubernetes-1.10-stabilizing-storage-security-networking/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/03/26/kubernetes-1.10-stabilizing-storage-security-networking/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor&amp;rsquo;s note: today&amp;rsquo;s post is by the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.10/release_team.md&#34; target=&#34;_blank&#34;&gt;1.10 Release
Team&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.10, our first release
of 2018!&lt;/p&gt;

&lt;p&gt;Today’s release continues to advance maturity, extensibility, and pluggability
of Kubernetes. This newest version stabilizes features in 3 key areas,
including storage, security, and networking. Notable additions in this release
include the introduction of external kubectl credential providers (alpha), the
ability to switch DNS service to CoreDNS at install time (beta), and the move
of Container Storage Interface (CSI) and persistent local volumes to beta.&lt;/p&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;storage-csi-and-local-storage-move-to-beta&#34;&gt;Storage - CSI and Local Storage move to beta&lt;/h2&gt;

&lt;p&gt;This is an impactful release for &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;the Storage Special Interest Group
(SIG)&lt;/a&gt;,
marking the culmination of their work on multiple features. The &lt;a href=&#34;https://github.com/kubernetes/features/issues/178&#34; target=&#34;_blank&#34;&gt;Kubernetes
implementation&lt;/a&gt; of the
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;Container Storage
Interface&lt;/a&gt;
(CSI) moves to beta in this release: installing new volume plugins is now as
easy as deploying a pod. This in turn enables third-party storage providers to
develop their solutions independently outside of the core Kubernetes codebase.
This continues the thread of extensibility within the Kubernetes ecosystem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/121&#34; target=&#34;_blank&#34;&gt;Durable (non-shared) local storage
management&lt;/a&gt; progressed to
beta in this release, making locally attached (non-network attached) storage
available as a persistent volume source. This means higher performance and
lower cost for distributed file systems and databases.&lt;/p&gt;

&lt;p&gt;This release also includes many updates to Persistent Volumes. Kubernetes can
automatically &lt;a href=&#34;https://github.com/kubernetes/features/issues/498&#34; target=&#34;_blank&#34;&gt;prevent deletion of Persistent Volume Claims that are in use by
a pod&lt;/a&gt; (beta) and &lt;a href=&#34;https://github.com/kubernetes/features/issues/499&#34; target=&#34;_blank&#34;&gt;prevent
deletion of a Persistent Volume that is bound to a Persistent Volume Claim
&lt;/a&gt;(beta). This helps ensure
that storage API objects are deleted in the correct order.&lt;/p&gt;

&lt;h2 id=&#34;security-external-credential-providers-alpha&#34;&gt;Security - External credential providers (alpha)&lt;/h2&gt;

&lt;p&gt;Kubernetes, which is
already highly extensible, gains another extension point in 1.10 with
&lt;a href=&#34;https://github.com/kubernetes/features/issues/541&#34; target=&#34;_blank&#34;&gt;external kubectl credential
providers&lt;/a&gt; (alpha). Cloud
providers, vendors, and other platform developers can now release binary
plugins to handle authentication for specific cloud-provider IAM services, or
that integrate with in-house authentication systems that aren’t supported
in-tree, such as Active Directory. This complements the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/&#34; target=&#34;_blank&#34;&gt;Cloud Controller
Manager&lt;/a&gt;
feature added in 1.9.&lt;/p&gt;

&lt;h2 id=&#34;networking-coredns-as-a-dns-provider-beta&#34;&gt;Networking - CoreDNS as a DNS provider (beta)&lt;/h2&gt;

&lt;p&gt;The ability to &lt;a href=&#34;https://github.com/kubernetes/website/pull/7638&#34; target=&#34;_blank&#34;&gt;switch the DNS
service&lt;/a&gt; to CoreDNS at
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/coredns/&#34; target=&#34;_blank&#34;&gt;install time&lt;/a&gt;
is now in beta. CoreDNS has fewer moving parts: it’s a single executable and a
single process, and supports additional use cases.&lt;/p&gt;

&lt;p&gt;Each Special Interest Group (SIG) within the community continues to deliver
the most-requested enhancements, fixes, and functionality for their respective
specialty areas. For a complete list of inclusions by SIG, please visit the
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md#110-release-notes&#34; target=&#34;_blank&#34;&gt;release
notes&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.10 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.10.0&#34; target=&#34;_blank&#34;&gt;download on
GitHub&lt;/a&gt;. To get
started with Kubernetes, check out these i&lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;nteractive
tutorials&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2-day-features-blog-series&#34;&gt;2 Day Features Blog Series&lt;/h2&gt;

&lt;p&gt;If you’re interested in exploring these features
more in depth, check back next week for our 2 Days of Kubernetes series where
we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;p&gt;Day 1 - Container Storage Interface (CSI) for Kubernetes going Beta
Day 2 - Local Persistent Volumes for Kubernetes going Beta&lt;/p&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the effort of hundreds of
individuals who contributed both technical and non-technical content. Special
thanks to the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.10/release_team.md&#34; target=&#34;_blank&#34;&gt;release
team&lt;/a&gt;
led by Jaice Singer DuMars, Kubernetes Ambassador for Microsoft. The 10
individuals on the release team coordinate many aspects of the release, from
documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an
amazing demonstration of collaboration in open source software development.
Kubernetes continues to gain new users at a rapid clip. This growth creates a
positive feedback cycle where more contributors commit code creating a more
vibrant ecosystem.&lt;/p&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining an ambitious project to
visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io/&#34; target=&#34;_blank&#34;&gt;K8s
DevStats&lt;/a&gt; illustrates the breakdown of contributions
from major company contributors, as well as an impressive set of preconfigured
reports on everything from individual contributors to pull request lifecycle
times. Thanks to increased automation, issue count at the end of the release
was only slightly higher than it was at the beginning. This marks a major
shift toward issue manageability. With 75,000+ comments, Kubernetes remains
one of the most actively discussed projects on GitHub.&lt;/p&gt;

&lt;h2 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h2&gt;

&lt;p&gt;According to a &lt;a href=&#34;https://www.cncf.io/blog/2018/03/26/cncf-survey-china/&#34; target=&#34;_blank&#34;&gt;recent CNCF
survey&lt;/a&gt;, more than 49%
of Asia-based respondents use Kubernetes in production, with another 49%
evaluating it for use in production. Established, global organizations are
using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at
massive scale. Recently published user stories from the community include:
1. &lt;strong&gt;Huawei&lt;/strong&gt;, the largest telecommunications equipment manufacturer in the
world, &lt;a href=&#34;https://kubernetes.io/case-studies/huawei/&#34; target=&#34;_blank&#34;&gt;moved its internal IT department’s applications to run on
Kubernetes&lt;/a&gt;. This resulted in the
global deployment cycles decreasing from a week to minutes, and the efficiency
of application delivery improved by tenfold.
1. &lt;strong&gt;Jinjiang Travel International&lt;/strong&gt;, one of the top 5 largest OTA and hotel
companies, use Kubernetes to &lt;a href=&#34;https://www.linux.com/blog/managing-production-systems-kubernetes-chinese-enterprises&#34; target=&#34;_blank&#34;&gt;speed up their software release
velocity&lt;/a&gt;
from hours to just minutes. Additionally, they leverage Kubernetes to increase
the scalability and availability of their online workloads.
1. &lt;strong&gt;Haufe Group&lt;/strong&gt;, the Germany-based media and software company, utilized
Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/haufegroup/&#34; target=&#34;_blank&#34;&gt;deliver a new release in half an
hour&lt;/a&gt; instead of days. The
company is also able to scale down to around half the capacity at night,
saving 30 percent on hardware costs.
1. &lt;strong&gt;BlackRock&lt;/strong&gt;, the world’s largest asset manager, was able to move quickly
using Kubernetes and built an investor research web app from &lt;a href=&#34;https://kubernetes.io/case-studies/blackrock/&#34; target=&#34;_blank&#34;&gt;inception to
delivery in under 100 days&lt;/a&gt;.
Is Kubernetes helping your team? &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;Share your
story&lt;/a&gt;
with the community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;The CNCF is expanding its certification offerings to
include a Certified Kubernetes Application Developer exam. The CKAD exam
certifies an individual&amp;rsquo;s ability to design, build, configure, and expose
cloud native applications for Kubernetes. The CNCF is looking for beta testers
for this new program. More information can be found
&lt;a href=&#34;https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Kubernetes documentation now features &lt;a href=&#34;https://k8s.io/docs/home/&#34; target=&#34;_blank&#34;&gt;user
journeys&lt;/a&gt;: specific pathways for learning based on
who readers are and what readers want to do. Learning Kubernetes is easier
than ever for beginners, and more experienced users can find task journeys
specific to cluster admins and application developers.&lt;/li&gt;
&lt;li&gt;CNCF also offers &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;online
training&lt;/a&gt; that teaches the skills
needed to create and configure a real-world Kubernetes cluster.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;kubecon&#34;&gt;KubeCon&lt;/h2&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/&#34; target=&#34;_blank&#34;&gt;KubeCon +
CloudNativeCon&lt;/a&gt;
is coming to Copenhagen from May 2-4, 2018 and will feature technical
sessions, case studies, developer deep dives, salons and more! Check out the
&lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/program/schedule/&#34; target=&#34;_blank&#34;&gt;schedule&lt;/a&gt;
of speakers and
&lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/attend/register/&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt;
today!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.10 release team on April 10th at
10am PDT to learn about the major features in this release including Local
Persistent Volumes and the Container Storage Interface (CSI). Register
&lt;a href=&#34;https://www.cncf.io/event/webinar-kubernetes-1-10/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining
one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest
Groups&lt;/a&gt;
(SIGs) that align with your interests. Have something you’d like to broadcast
to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;community
meeting&lt;/a&gt;,
and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.
1. Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack
Overflow&lt;/a&gt;
1. Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;
1. Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for
latest updates
1. Chat with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;
1. Share your Kubernetes
&lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Principles of Container-based Application Design</title>
      <link>https://kubernetes.io/blog/2018/03/principles-of-container-app-design/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/03/principles-of-container-app-design/</guid>
      <description>
        
        
        

&lt;p&gt;It&amp;rsquo;s possible nowadays to put almost any application in a container and run it. Creating cloud-native applications, however—containerized applications that are automated and orchestrated effectively by a cloud-native platform such as Kubernetes—requires additional effort. Cloud-native applications anticipate failure; they run and scale reliably even when their infrastructure experiences outages. To offer such capabilities, cloud-native platforms like Kubernetes impose a set of contracts and constraints on applications. These contracts ensure that applications they run conform to certain constraints and allow the platform to automate application management.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve outlined &lt;a href=&#34;https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper&#34; target=&#34;_blank&#34;&gt;seven principles&lt;/a&gt;for containerized applications to follow in order to be fully cloud-native.&lt;/p&gt;

&lt;p&gt;| &amp;mdash;&amp;ndash; |
| &lt;img src=&#34;https://lh5.googleusercontent.com/1XqojkVC0CET1yKCJqZ3-0VWxJ3W8Q74zPLlqnn6eHSJsjHOiBTB7EGUX5o_BOKumgfkxVdgBeLyoyMfMIXwVm9p2QXkq_RRy2mDJG1qEExJDculYL5PciYcWfPAKxF2-DGIdiLw&#34; alt=&#34;&#34; /&gt;  |
| Container Design Principles |&lt;/p&gt;

&lt;p&gt;These seven principles cover both build time and runtime concerns.&lt;/p&gt;

&lt;h4 id=&#34;build-time&#34;&gt;Build time&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single Concern:&lt;/strong&gt; Each container addresses a single concern and does it well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-Containment:&lt;/strong&gt; A container relies only on the presence of the Linux kernel. Additional libraries are added when the container is built.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Image Immutability:&lt;/strong&gt; Containerized applications are meant to be immutable, and once built are not expected to change between different environments.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;runtime&#34;&gt;Runtime&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High Observability:&lt;/strong&gt; Every container must implement all necessary APIs to help the platform observe and manage the application in the best way possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifecycle Conformance:&lt;/strong&gt; A container must have a way to read events coming from the platform and conform by reacting to those events.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Process Disposability:&lt;/strong&gt; Containerized applications must be as ephemeral as possible and ready to be replaced by another container instance at any point in time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Runtime Confinement:&lt;/strong&gt; Every container must declare its resource requirements and restrict resource use to the requirements indicated.
The build time principles ensure that containers have the right granularity, consistency, and structure in place. The runtime principles dictate what functionalities must be implemented in order for containerized applications to possess cloud-native function. Adhering to these principles helps ensure that your applications are suitable for automation in Kubernetes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The white paper is freely available for download:&lt;/p&gt;

&lt;p&gt;To read more about designing cloud-native applications for Kubernetes, check out my &lt;a href=&#34;http://leanpub.com/k8spatterns/&#34; target=&#34;_blank&#34;&gt;Kubernetes Patterns&lt;/a&gt; book.&lt;/p&gt;

&lt;p&gt;— &lt;a href=&#34;http://twitter.com/bibryam&#34; target=&#34;_blank&#34;&gt;Bilgin Ibryam&lt;/a&gt;, Principal Architect, Red Hat&lt;/p&gt;

&lt;p&gt;Twitter:  &lt;br /&gt;
Blog: &lt;a href=&#34;http://www.ofbizian.com/&#34; target=&#34;_blank&#34;&gt;http://www.ofbizian.com&lt;/a&gt;&lt;br /&gt;
Linkedin:&lt;/p&gt;

&lt;p&gt;Bilgin Ibryam (@bibryam) is a principal architect at Red Hat, open source committer at ASF, blogger, author, and speaker. He is the author of Camel Design Patterns and Kubernetes Patterns books. In his day-to-day job, Bilgin enjoys mentoring, training and leading teams to be successful with distributed systems, microservices, containers, and cloud-native applications in general.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Expanding User Support with Office Hours</title>
      <link>https://kubernetes.io/blog/2018/03/expanding-user-support-with-office-hours/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/03/expanding-user-support-with-office-hours/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Today&amp;rsquo;s post is by &lt;a href=&#34;https://twitter.com/castrojo&#34; target=&#34;_blank&#34;&gt;Jorge Castro&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/errordeveloper&#34; target=&#34;_blank&#34;&gt;Ilya Dmitichenko&lt;/a&gt; on Kubernetes office hours.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s developer has an almost overwhelming amount of resources available for learning. Kubernetes development teams use &lt;a href=&#34;https://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;StackOverflow&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/home&#34; target=&#34;_blank&#34;&gt;user documentation&lt;/a&gt;, &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;, and the &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-users&#34; target=&#34;_blank&#34;&gt;mailing lists&lt;/a&gt;. Additionally, the community itself continues to amass an &lt;a href=&#34;https://github.com/ramitsurana/awesome-kubernetes&#34; target=&#34;_blank&#34;&gt;awesome list&lt;/a&gt; of resources.&lt;/p&gt;

&lt;p&gt;One of the challenges of large projects is keeping user resources relevant and useful. While documentation can be useful, great learning also happens in Q&amp;amp;A sessions at conferences, or by learning with someone whose explanation matches your learning style. Consider that learning Kung Fu from Morpheus would be a lot more fun than reading a book about Kung Fu!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-Iy2GaddJp78/WqnFbVUu9FI/AAAAAAAAAM4/xUzhOSIlRDEMMZNl3SzPBd1Pa0T5y0pKQCLcBGAs/s400/24xkey.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We as Kubernetes developers want to create an interactive experience: where Kubernetes users can get their questions answered by experts in real time, or at least referred to the best known documentation or code example.&lt;/p&gt;

&lt;p&gt;Having discussed a few broad ideas, we eventually decided to make &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/events/office-hours.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Office Hours&lt;/a&gt; a live stream where we take user questions from the audience and present them to our panel of contributors and expert users. We run two sessions: one for European time zones, and one for the Americas. These &lt;a href=&#34;https://docs.google.com/document/d/1jHSnRzoOxwd1urgxwbANhNgXjMV8fb0B4NS3ZUL10IY/edit&#34; target=&#34;_blank&#34;&gt;streaming setup guidelines&lt;/a&gt; make office hours extensible—for example, if someone wants to run office hours for Asia/Pacific timezones, or for another CNCF project.&lt;/p&gt;

&lt;p&gt;To give you an idea of what Kubernetes office hours are like, here&amp;rsquo;s Josh Berkus answering a question on running databases on Kubernetes. Despite the popularity of this topic, it&amp;rsquo;s still difficult for a new user to get a constructive answer. Here&amp;rsquo;s an excellent response from Josh:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/embed/Aj0yozuQ0ME?ecver=2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/Aj0yozuQ0ME/0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s often easier to field this kind of question in office hours than it is to ask a developer to write a full-length blog post. [Editor&amp;rsquo;s note: That&amp;rsquo;s legit!] Because we don&amp;rsquo;t have infinite developers with infinite time, this kind of focused communication creates high-bandwidth help while limiting developer commitments to 1 hour per month. This allows a rotating set of experts to share the load without overwhelming any one person.&lt;/p&gt;

&lt;p&gt;We hold office hours the third Wednesday of every month on the &lt;a href=&#34;https://www.youtube.com/c/kubernetescommunity&#34; target=&#34;_blank&#34;&gt;Kubernetes YouTube Channel&lt;/a&gt;. You can post questions on the &lt;a href=&#34;https://kubernetes.slack.com/messages/office-hours&#34; target=&#34;_blank&#34;&gt;#office-hours channel&lt;/a&gt; on Slack, or you can submit your question to Stack Overflow and post a link on Slack. If you post a question in advance, you might get better answers, as volunteers have more time to research and prepare. If a question can&amp;rsquo;t be fully solved during the call, the team will try their best to point you in the right direction and/or ping other people in the community to take a look. &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/events/office-hours.md&#34; target=&#34;_blank&#34;&gt;Check out this page&lt;/a&gt; for more details on what&amp;rsquo;s off- and on topic as well as meeting information for your time zone. We hope to hear your questions soon!&lt;/p&gt;

&lt;p&gt;Special thanks to Amazon, Bitnami, Giant Swarm, Heptio, Liquidweb, Northwestern Mutual, Packet.net, Pivotal, Red Hat, Weaveworks, and VMWare for donating engineering time to office hours.&lt;/p&gt;

&lt;p&gt;And thanks to Alan Pope, Joe Beda, and Charles Butler for technical support in making our livestream better.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: How to Integrate RollingUpdate Strategy for TPR in Kubernetes</title>
      <link>https://kubernetes.io/blog/2018/03/how-to-integrate-rollingupdate-strategy/</link>
      <pubDate>Tue, 13 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/03/how-to-integrate-rollingupdate-strategy/</guid>
      <description>
        
        
        

&lt;p&gt;With Kubernetes, it&amp;rsquo;s easy to manage and scale stateless applications like web apps and API services right out of the box. To date, almost all of the talks about Kubernetes has been about microservices and stateless applications.&lt;/p&gt;

&lt;p&gt;With the popularity of container-based microservice architectures, there is a strong need to deploy and manage RDBMS(Relational Database Management Systems). RDBMS requires experienced database-specific knowledge to correctly scale, upgrade, and re-configure while protecting against data loss or unavailability.&lt;/p&gt;

&lt;p&gt;For example, MySQL (the most popular open source RDBMS) needs to store data in files that are persistent and exclusive to each MySQL database&amp;rsquo;s storage. Each MySQL database needs to be individually distinct, another, more complex is in cluster that need to distinguish one MySQL database from a cluster as a different role, such as master, slave, or shard. High availability and zero data loss are also hard to accomplish when replacing database nodes on failed machines.&lt;/p&gt;

&lt;p&gt;Using powerful Kubernetes API extension mechanisms, we can encode RDBMS domain knowledge into software, named WQ-RDS, running atop Kubernetes like built-in resources.&lt;/p&gt;

&lt;p&gt;WQ-RDS leverages Kubernetes primitive resources and controllers, it deliveries a number of enterprise-grade features and brings a significantly reliable way to automate time-consuming operational tasks like database setup, patching backups, and setting up high availability clusters. WQ-RDS supports mainstream versions of Oracle and MySQL (both compatible with MariaDB).&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s demonstrate how to manage a MySQL sharding cluster.&lt;/p&gt;

&lt;h3 id=&#34;mysql-sharding-cluster&#34;&gt;MySQL Sharding Cluster&lt;/h3&gt;

&lt;p&gt;MySQL Sharding Cluster is a scale-out database architecture. Based on the hash algorithm, the architecture distributes data across all the shards of the cluster. Sharding is entirely transparent to clients: Proxy is able to connect to any Shards in the cluster and issue queries to the correct shards directly.&lt;/p&gt;

&lt;p&gt;| &amp;mdash;&amp;ndash; |
| &lt;img src=&#34;https://lh5.googleusercontent.com/4WiSkxX-XBqARVqQ0No-1tZ31op90LAUkTco3FdIO1mFScNOTVtMCgnjaO8SRUmms-6MAb46CzxlXDhLBqAAAmbx26atJnu4t1FTTALZx_CbUPqrCxjL746DW4TD42-03Ac9VB2c&#34; alt=&#34;&#34; /&gt; |
|&lt;/p&gt;

&lt;p&gt;Note: Each shard corresponds to a single MySQL instance. Currently, WQ-RDS supports a maximum of 64 shards.&lt;/p&gt;

&lt;p&gt;|&lt;/p&gt;

&lt;p&gt;All of the shards are built with Kubernetes Statefulset, Services, Storage Class, configmap, secrets and MySQL. WQ-RDS manages the entire lifecycle of the sharding cluster. Advantages of the sharding cluster are obvious:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scale out queries per second (QPS) and transactions per second (TPS)&lt;/li&gt;
&lt;li&gt;Scale out storage capacity: gain more storage by distributing data to multiple nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;create-a-mysql-sharding-cluster&#34;&gt;Create a MySQL Sharding Cluster&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s create a Kubernetes cluster with 8 shards.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; kubectl create -f mysqlshardingcluster.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, create a MySQL Sharding Cluster including 8 shards.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TPR : MysqlCluster and MysqlDatabase&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl get mysqlcluster  


NAME             KIND

clustershard-c   MysqlCluster.v1.mysql.orain.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MysqlDatabase from clustershard-c0 to clustershard-c7 belongs to MysqlCluster clustershard-c.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8s-master ~]# kubectl get mysqldatabase  

NAME KIND  

clustershard-c0 MysqlDatabase.v1.mysql.orain.com  

clustershard-c1 MysqlDatabase.v1.mysql.orain.com  

clustershard-c2 MysqlDatabase.v1.mysql.orain.com  

clustershard-c3 MysqlDatabase.v1.mysql.orain.com  

clustershard-c4 MysqlDatabase.v1.mysql.orain.com  

clustershard-c5 MysqlDatabase.v1.mysql.orain.com  

clustershard-c6 MysqlDatabase.v1.mysql.orain.com  

clustershard-c7 MysqlDatabase.v1.mysql.orain.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, let&amp;rsquo;s look at two main features: high availability and RollingUpdate strategy.&lt;/p&gt;

&lt;p&gt;To demonstrate, we&amp;rsquo;ll start by running sysbench to generate some load on the cluster. In this example, QPS metrics are generated by MySQL export, collected by Prometheus, and visualized in Grafana.&lt;/p&gt;

&lt;h3 id=&#34;feature-high-availability&#34;&gt;Feature: high availability&lt;/h3&gt;

&lt;p&gt;WQ-RDS handles MySQL instance crashes while protecting against data loss.&lt;/p&gt;

&lt;p&gt;When killing clustershard-c0, WQ-RDS will detect that clustershard-c0 is unavailable and replace clustershard-c0 on failed machine, taking about 35 seconds on average.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/sXqVqfTu6rMWn0mlHLgHHqATe_qsx1tNmMfX60HoTwyhd5HCL4A_ViFBQAZfOoVGioeXcI_XXbzVFUdq2hbKGwS0OXH6PFGqgpZshfBwrT088bz4KqeyTbHpQR2olyzE6eRo1fan&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;zero data loss at same time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/7xnN_sODa-3Ch3ScAUlggCTeYfnE3-wxRaCIHrljHCB7LnXgth8zeCv0gk_UU1jbSDBQuACQ2Mf1FO1-E7GvMWwGKjp7irenAKp4DkHlA5LR9OVuLXqubPFhhksA8kfBUh4Z4OuN&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;feature-rollingupdate-strategy&#34;&gt;Feature : RollingUpdate Strategy&lt;/h3&gt;

&lt;p&gt;MySQL Sharding Cluster brings us not only strong scalability but also some level of maintenance complexity. For example, when updating a MySQL configuration like innodb_buffer_pool_size, a DBA has to perform a number of steps:&lt;/p&gt;

&lt;p&gt;1. Apply change time.&lt;br /&gt;
2. Disable client access to database proxies.&lt;br /&gt;
3. Start a rolling upgrade.&lt;/p&gt;

&lt;p&gt;Rolling upgrades need to proceed in order and are the most demanding step of the process. One cannot continue a rolling upgrade until and unless previous updates to MySQL instances are running and ready.&lt;/p&gt;

&lt;p&gt;4 Verify the cluster.&lt;br /&gt;
5. Enable client access to database proxies.&lt;/p&gt;

&lt;p&gt;Possible problems with a rolling upgrade include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;node reboot&lt;/li&gt;
&lt;li&gt;MySQL instances restart&lt;/li&gt;
&lt;li&gt;human error
Instead, WQ-RDS enables a DBA to perform rolling upgrades automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;statefulset-rollingupdate-in-kubernetes&#34;&gt;StatefulSet RollingUpdate in Kubernetes&lt;/h3&gt;

&lt;p&gt;Kubernetes 1.7 includes a major feature that adds automated updates to StatefulSets and supports a range of update strategies including rolling updates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For more information about &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates&#34; target=&#34;_blank&#34;&gt;StatefulSet RollingUpdate&lt;/a&gt;, see the Kubernetes docs.&lt;/p&gt;

&lt;p&gt;Because TPR (currently CRD) does not support the rolling upgrade strategy, we needed to integrate the RollingUpdate strategy into WQ-RDS. Fortunately, the &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;Kubernetes repo&lt;/a&gt; is a treasure for learning. In the process of implementation, there are some points to share:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;**MySQL Sharding Cluster has **&lt;strong&gt;changed&lt;/strong&gt;: Each StatefulSet has its corresponding ControllerRevision, which records all the revision data and order (like git). Whenever StatefulSet is syncing, StatefulSet Controller will firstly compare it&amp;rsquo;s spec to the latest corresponding ControllerRevision data (similar to git diff). If changed, a new ControllerrRevision will be generated, and the revision number will be incremented by 1. WQ-RDS borrows the process, MySQL Sharding Cluster object will record all the revision and order in ControllerRevision.&lt;/li&gt;
&lt;li&gt;**How to initialize MySQL Sharding Cluster to meet request **&lt;strong&gt;replicas&lt;/strong&gt;: Statefulset supports two &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates&#34; target=&#34;_blank&#34;&gt;Pod management policies&lt;/a&gt;: Parallel and OrderedReady. Because MySQL Sharding Cluster doesn&amp;rsquo;t require ordered creation for its initial processes, we use the Parallel policy to accelerate the initialization of the cluster.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;**How to perform a Rolling **&lt;strong&gt;Upgrade&lt;/strong&gt;: Statefulset recreates pods in strictly decreasing order. The difference is that WQ-RDS updates shards instead of recreating them, as shown below:
&lt;img src=&#34;https://lh6.googleusercontent.com/B4ig8krCsXwvMeBy8NamQi1DrihUEzBcRTHCqhn9kUvlcpPrFoYUNAxn61qh8S2HXcdg31QpOhWSsYHP0jI4QxPkKpZ5oY-k9gFp1eK63qt6rwTMMWMiBs45DObY6rw2R7c0lNPu&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;When RollingUpdate ends&lt;/strong&gt;: Kubernetes signals termination clearly. A rolling update completes when all of a set&amp;rsquo;s Pods have been updated to the updateRevision. The status&amp;rsquo;s currentRevision is set to updateRevision and its updateRevision is set to the empty string. The status&amp;rsquo;s currentReplicas is set to updateReplicas and its updateReplicas are set to 0.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;controller-revision-in-wq-rds&#34;&gt;Controller revision in WQ-RDS&lt;/h3&gt;

&lt;p&gt;Revision information is stored in MysqlCluster.Status and is no different than Statefulset.Status.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
root@k8s-master ~]# kubectl get mysqlcluster -o yaml clustershard-c

apiVersion: v1

items:

\- apiVersion: mysql.orain.com/v1

 kind: MysqlCluster

 metadata:

   creationTimestamp: 2017-10-20T08:19:41Z

   labels:

     AppName: clustershard-crm

     Createdby: orain.com

     DBType: MySQL

   name: clustershard-c

   namespace: default

   resourceVersion: &amp;quot;415852&amp;quot;

   selfLink: /apis/mysql.orain.com/v1/namespaces/default/mysqlclusters/clustershard-c

   uid: 6bb089bb-b56f-11e7-ae02-525400e717a6

 spec:



     dbresourcespec:

       limitedcpu: 1200m

       limitedmemory: 400Mi

       requestcpu: 1000m

       requestmemory: 400Mi



 status:

   currentReplicas: 8

   currentRevision: clustershard-c-648d878965

   replicas: 8

   updateRevision: clustershard-c-648d878965

kind: List

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example-perform-a-rolling-upgrade&#34;&gt;Example: Perform a rolling upgrade&lt;/h3&gt;

&lt;p&gt;Finally, We can now update &amp;ldquo;clustershard-c&amp;rdquo; to update configuration &amp;ldquo;innodb_buffer_pool_size&amp;rdquo; from 6GB to 7GB and reboot.&lt;/p&gt;

&lt;p&gt;The process takes 480 seconds.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/LOxFDdojYxnPvSHDYwivVge6vGImK7uTdyvCsKrxCMF3rIlVkw7mHeNhJiNJwz1aGzVhZXpqrgzHC6pIbkPk3JPAtuSqX9ovAYBzK01BGfzwkXvMGZomAh4L0DahGyD3QB715B-Z&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The upgrade is in monotonically decreasing manner:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/DhFbY3JDh23A_91c04TujxWM9xCX_xq1xOCXHi7XAd75LzKwDtbH6Gr_2VXCscg8AeVCQzw3Inw4M-uvssWq8od4va0wd-fIyClVY63FjRfeU16fQda_XqzBYRIhrG5W3tDnCAwC&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/bAJtLRRl2TqQrfBOooNm9DIEuezoBhT3f-XuOyGxp8sKePzfRaQYcJ7PFvL30xw9jeUpc-3rVw6Qjr46dFRk7mmUsf3oichNEuC-BFwCEtpbxK0_BjSJxtIE4B5xR4CGw1m6Hf0D&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/ALYk-EP_rYibA95nIIo8TKx8BYuSY9w1Pqw4JLEiV89K9i06uBhkTrYWX26FjYtheGKVwwVMTtDKH7UTBovGf8AEpK97T3RT23RSAUTs4GyDFaDOGmlRAczbGLm0UjQglbB_NPdF&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/gmM6UbgVOBWPJBpIMutxeTxGiwtjFv25KAHQw3ebVAF5Kxm-uxkPKEiYKhpwYUTyDe5knYlGmQDDHiN8eefBJx0fbK7jg4IlpG5_DXMUG6rNNFIbpP7Q94ANROIeUfe5JP6t-k37&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/aiczeqRNRls8lh-LYbnx112kgZI2gvaBMimAk74KlLhR3EVicuKAemTKr2eKUSFPjmKbsg_gw_nY1G4YU0-3J1EjDPOhz55UUri47Py-s-jRf0dF-lAKn6TRrF6IvGtv2aldWa3k&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/mwRQP_wXMCzpXsC5sqb0nJ9jU4KdUl4FiUE26gQZMQbrn5zcgqSYZB03CLmGsT2Nuq-7x00W4Ar3IUAh7hxEksQEGl6ugAmY0wo7xjzisNH9VE1qto9Afx8QW2Sr6NR-SBDeJfTt&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/joraaJ-qX-K8zTdAFBJWeOswQQtNeX6yezKGkSM56FNYQT-XYrgsxvNLYBE0askw9huAmJhebCVU4AMvjz4B6xlIjdLwO3vMX7_dWBzkfu05HZ3-NOsFnqg-jvkLknl-ldRzUcFO&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/ayoUAhD-azUjUqjut7iSiW8FBFJpCJZLRJDT9mXJoy4QTutAsGgr4yPvbFumaXasOqpsmJ_zZ2k7nrQl2YrjGqPr83PXe-tXjj9OLc-GYhhtJTzBEeddWpZn5pDyBpdw9I4sD-O0&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;RollingUpgrade is meaningful to database administrators. It provides a more effective way to operator database.&lt;/p&gt;

&lt;p&gt;--Orain Xiong, co-founder, Woqutech&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Spark 2.3 with Native Kubernetes Support</title>
      <link>https://kubernetes.io/blog/2018/03/apache-spark-23-with-native-kubernetes/</link>
      <pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/03/apache-spark-23-with-native-kubernetes/</guid>
      <description>
        
        
        

&lt;h3 id=&#34;kubernetes-and-big-data&#34;&gt;Kubernetes and Big Data&lt;/h3&gt;

&lt;p&gt;The open source community has been working over the past year to enable first-class support for data processing, data analytics and machine learning workloads in Kubernetes. New extensibility features in Kubernetes, such as &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/&#34; target=&#34;_blank&#34;&gt;custom resources&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/#custom-controllers&#34; target=&#34;_blank&#34;&gt;custom controllers&lt;/a&gt;, can be used to create deep integrations with individual applications and frameworks.&lt;/p&gt;

&lt;p&gt;Traditionally, data processing workloads have been run in dedicated setups like the YARN/Hadoop stack. However, unifying the control plane for all workloads on Kubernetes simplifies cluster management and can improve resource utilization.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Bloomberg has invested heavily in machine learning and NLP to give our clients a competitive edge when it comes to the news and financial information that powers their investment decisions. By building our Data Science Platform on top of Kubernetes, we&amp;rsquo;re making state-of-the-art data science tools like Spark, TensorFlow, and our sizable GPU footprint accessible to the company&amp;rsquo;s 5,000+ software engineers in a consistent, easy-to-use way.&amp;rdquo; - Steven Bower, Team Lead, Search and Data Science Infrastructure at Bloomberg&lt;/p&gt;

&lt;h3 id=&#34;introducing-apache-spark-kubernetes&#34;&gt;Introducing Apache Spark + Kubernetes&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/releases/spark-release-2-3-0.html&#34; target=&#34;_blank&#34;&gt;Apache Spark 2.3&lt;/a&gt; with native Kubernetes support combines the best of the two prominent open source projects — Apache Spark, a framework for large-scale data processing; and Kubernetes.&lt;/p&gt;

&lt;p&gt;Apache Spark is an essential tool for data scientists, offering a robust platform for a variety of applications ranging from large scale data transformation to analytics to machine learning. Data scientists are adopting containers en masse to improve their workflows by realizing benefits such as packaging of dependencies and creating reproducible artifacts. Given that Kubernetes is the de facto standard for managing containerized environments, it is a natural fit to have support for Kubernetes APIs within Spark.&lt;/p&gt;

&lt;p&gt;Starting with Spark 2.3, users can run Spark workloads in an existing Kubernetes 1.7+ cluster and take advantage of Apache Spark&amp;rsquo;s ability to manage distributed data processing tasks. Apache Spark workloads can make direct use of Kubernetes clusters for multi-tenancy and sharing through &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&#34; target=&#34;_blank&#34;&gt;Namespaces&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/resource-quotas/&#34; target=&#34;_blank&#34;&gt;Quotas&lt;/a&gt;, as well as administrative features such as &lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/&#34; target=&#34;_blank&#34;&gt;Pluggable Authorization&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34; target=&#34;_blank&#34;&gt;Logging&lt;/a&gt;. Best of all, it requires no changes or new installations on your Kubernetes cluster; simply &lt;a href=&#34;https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images&#34; target=&#34;_blank&#34;&gt;create a container image&lt;/a&gt; and set up the right &lt;a href=&#34;https://spark.apache.org/docs/latest/running-on-kubernetes.html#rbac&#34; target=&#34;_blank&#34;&gt;RBAC roles&lt;/a&gt; for your Spark Application and you&amp;rsquo;re all set.&lt;/p&gt;

&lt;p&gt;Concretely, a native Spark Application in Kubernetes acts as a &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/#custom-controllers&#34; target=&#34;_blank&#34;&gt;custom controller&lt;/a&gt;, which creates Kubernetes resources in response to requests made by the Spark scheduler. In contrast with &lt;a href=&#34;https://kubernetes.io/blog/2016/03/using-Spark-and-Zeppelin-to-process-Big-Data-on-Kubernetes&#34; target=&#34;_blank&#34;&gt;deploying Apache Spark in Standalone Mode&lt;/a&gt; in Kubernetes, the native approach offers fine-grained management of Spark Applications, improved elasticity, and seamless integration with logging and monitoring solutions. The community is also exploring advanced use cases such as managing streaming workloads and leveraging service meshes like &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://1.bp.blogspot.com/-hl4pnOqiH4M/Wp4w9QmzghI/AAAAAAAAAL4/jcWoDOKEp3Y6lCzGxzTOlbvl2Mq1-2YeQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-03-05%2Bat%2B10.10.14%2BPM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To try this yourself on a Kubernetes cluster, simply download the binaries for the official &lt;a href=&#34;https://spark.apache.org/downloads.html&#34; target=&#34;_blank&#34;&gt;Apache Spark 2.3 release&lt;/a&gt;. For example, below, we describe running a simple Spark application to compute the mathematical constant Pi across three Spark executors, each running in a separate pod. Please note that this requires a cluster running Kubernetes 1.7 or above, a &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34;&gt;kubectl&lt;/a&gt; client that is configured to access it, and the necessary &lt;a href=&#34;https://spark.apache.org/docs/latest/running-on-kubernetes.html#rbac&#34; target=&#34;_blank&#34;&gt;RBAC rules&lt;/a&gt; for the default namespace and service account.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl cluster-info  

Kubernetes master is running at https://xx.yy.zz.ww

$ bin/spark-submit

   --master k8s://https://xx.yy.zz.ww

   --deploy-mode cluster

   --name spark-pi

   --class org.apache.spark.examples.SparkPi

   --conf spark.executor.instances=5

   --conf spark.kubernetes.container.image=

   --conf spark.kubernetes.driver.pod.name=spark-pi-driver

   local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To watch Spark resources that are created on the cluster, you can use the following kubectl command in a separate terminal window.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -l &#39;spark-role in (driver, executor)&#39; -w

NAME              READY STATUS  RESTARTS AGE

spark-pi-driver   1/1 Running  0 14s

spark-pi-da1968a859653d6bab93f8e6503935f2-exec-1   0/1 Pending 0 0s

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The results can be streamed during job execution by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
$ kubectl logs -f spark-pi-driver

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the application completes, you should see the computed value of Pi in the driver logs.&lt;/p&gt;

&lt;p&gt;In Spark 2.3, we&amp;rsquo;re starting with support for Spark applications written in Java and Scala with support for resource localization from a variety of data sources including HTTP, GCS, HDFS, and more. We have also paid close attention to failure and recovery semantics for Spark executors to provide a strong foundation to build upon in the future. Get started with &lt;a href=&#34;https://spark.apache.org/docs/latest/running-on-kubernetes.html&#34; target=&#34;_blank&#34;&gt;the open-source documentation&lt;/a&gt; today.&lt;/p&gt;

&lt;h3 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s lots of exciting work to be done in the near future. We&amp;rsquo;re actively working on features such as dynamic resource allocation, in-cluster staging of dependencies, support for PySpark &amp;amp; SparkR, support for Kerberized HDFS clusters, as well as client-mode and popular notebooks&amp;rsquo; interactive execution environments. For people who fell in love with the Kubernetes way of managing applications declaratively, we&amp;rsquo;ve also been working on a &lt;a href=&#34;https://coreos.com/operators/&#34; target=&#34;_blank&#34;&gt;Kubernetes Operator&lt;/a&gt; for spark-submit, which allows users to declaratively specify and submit Spark Applications.&lt;/p&gt;

&lt;p&gt;And we&amp;rsquo;re just getting started! We would love for you to get involved and help us evolve the project further.&lt;/p&gt;

&lt;p&gt;Huge thanks to the Apache Spark and Kubernetes contributors spread across multiple organizations who spent many hundreds of hours working on this effort. We look forward to seeing more of you contribute to the project and help it evolve further.&lt;/p&gt;

&lt;p&gt;Anirudh Ramanathan and Palak Bhatia&lt;br /&gt;
Google&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes: First Beta Version of Kubernetes 1.10 is Here</title>
      <link>https://kubernetes.io/blog/2018/03/first-beta-version-of-kubernetes-1-10/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/03/first-beta-version-of-kubernetes-1-10/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Editor&amp;rsquo;s note: Today&amp;rsquo;s post is by Nick Chase. Nick is Head of Content at &lt;a href=&#34;https://www.mirantis.com/&#34; target=&#34;_blank&#34;&gt;Mirantis&lt;/a&gt;.&lt;/strong&gt;
The Kubernetes community has released the first beta version of Kubernetes 1.10, which means you can now try out some of the new features and give your feedback to the release team ahead of the official release. The release, currently scheduled for March 21, 2018, is targeting the inclusion of more than a dozen brand new alpha features and more mature versions of more than two dozen more.&lt;/p&gt;

&lt;p&gt;Specifically, Kubernetes 1.10 will include production-ready versions of Kubelet TLS Bootstrapping, API aggregation, and more detailed storage metrics.&lt;/p&gt;

&lt;p&gt;Some of these features will look familiar because they emerged at earlier stages in previous releases. Each stage has specific meanings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;stable&lt;/strong&gt;: The same as &amp;ldquo;generally available&amp;rdquo;,  features in this stage have been thoroughly tested and can be used in production environments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;beta&lt;/strong&gt;: The feature has been around long enough that the team is confident that the feature itself is on track to be included as a stable feature, and any API calls aren&amp;rsquo;t going to change. You can use and test these features, but including them in mission-critical production environments is not advised because they are not completely hardened.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;alpha&lt;/strong&gt;: New features generally come in at this stage. These features are still being explored. APIs and options may change in future versions, or the feature itself may disappear. Definitely not for production environments.
You can download the latest release of Kubernetes 1.10 from . To give feedback to the development community, &lt;a href=&#34;https://github.com/kubernetes/kubernetes/milestone/37&#34; target=&#34;_blank&#34;&gt;create an issue in the Kubernetes 1.10 milestone&lt;/a&gt; and tag the appropriate SIG before March 9.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here&amp;rsquo;s what to look for, though you should remember that while this is the current plan as of this writing, there&amp;rsquo;s always a possibility that one or more features may be held for a future release. We&amp;rsquo;ll start with authentication.&lt;/p&gt;

&lt;h3 id=&#34;authentication-sig-auth&#34;&gt;Authentication (SIG-Auth)&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/43&#34; target=&#34;_blank&#34;&gt;Kubelet TLS Bootstrap&lt;/a&gt; (stable): Kubelet TLS bootstrapping is probably the &amp;ldquo;headliner&amp;rdquo; of the Kubernetes 1.10 release as it becomes available for production environments. It provides the ability for a new kubelet to create a certificate signing request, which enables you to add new nodes to your cluster without having to either manually add security certificates or use self-signed certificates that eliminate many of the benefits of having certificates in the first place.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/5&#34; target=&#34;_blank&#34;&gt;Pod Security Policy moves to its own API group&lt;/a&gt; (beta): The beta release of the Pod Security Policy lets administrators decide what contexts pods can run in. In other words, you have the ability to prevent unprivileged users from creating privileged pods &amp;ndash; that is, pods that can perform actions such as writing files or accessing Secrets &amp;ndash; in particular namespaces.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/279&#34; target=&#34;_blank&#34;&gt;Limit node access to API&lt;/a&gt; (beta): Also in beta, you now have the ability to limit calls to the API on a node to just that specific node, and to ensure that a node is only calling its own API, and not those on other nodes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/541&#34; target=&#34;_blank&#34;&gt;External client-go credential providers&lt;/a&gt; (alpha): client-go is the Go language client for accessing the Kubernetes API. This feature adds the ability to add external credential providers. For example, Amazon might want to create its own authenticator to validate interaction with EKS clusters; this feature enables them to do that without having to include their authenticator in the Kubernetes codebase.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/542&#34; target=&#34;_blank&#34;&gt;TokenRequest API&lt;/a&gt; (alpha): The TokenRequest API provides the groundwork for much needed improvements to service account tokens; this feature enables creation of tokens that aren&amp;rsquo;t persisted in the Secrets API, that are targeted for specific audiences (such as external secret stores), have configurable expiries, and are bindable to specific pods.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;networking-sig-network&#34;&gt;Networking (SIG-Network)&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/504&#34; target=&#34;_blank&#34;&gt;Support configurable pod resolv.conf&lt;/a&gt; (beta): You now have the ability to specifically control DNS for a single pod, rather than relying on the overall cluster DNS.&lt;/li&gt;
&lt;li&gt;Although the feature is called &lt;a href=&#34;https://github.com/kubernetes/features/issues/427&#34; target=&#34;_blank&#34;&gt;Switch default DNS plugin to CoreDNS&lt;/a&gt; (beta), that&amp;rsquo;s not actually what will happen in this cycle. The community has been working on the switch from kube-dns, which includes dnsmasq, to CoreDNS, another CNCF project with fewer moving parts, for several releases. In Kubernetes 1.10, the default will still be kube-dns, but when CoreDNS reaches feature parity with kube-dns, the team will look at making it the default.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/536&#34; target=&#34;_blank&#34;&gt;Topology aware routing of services&lt;/a&gt; (alpha): The ability to distribute workloads is one of the advantages of Kubernetes, but one thing that has been missing until now is the ability to keep workloads and services geographically close together for latency purposes. Topology aware routing will help with this problem. (This functionality may be delayed until Kubernetes 1.11.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/539&#34; target=&#34;_blank&#34;&gt;Make NodePort IP address configurable&lt;/a&gt; (alpha): Not having to specify IP addresses in a Kubernetes cluster is great &amp;ndash; until you actually need to know what one of those addresses is ahead of time, such as for setting up database replication or other tasks. You will now have the ability to specifically configure NodePort IP addresses to solve this problem. (This functionality may be delayed until Kubernetes 1.11.)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;kubernetes-apis-sig-api-machinery&#34;&gt;Kubernetes APIs (SIG-API-machinery)&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/263&#34; target=&#34;_blank&#34;&gt;API Aggregation&lt;/a&gt; (stable): Kubernetes makes it possible to extend its API by creating your own functionality and registering your functions so that they can be served alongside the core K8s functionality. This capability will be upgraded to &amp;ldquo;stable&amp;rdquo; in Kubernetes 1.10, so you can use it in production. Additionally, SIG-CLI is adding a feature called &lt;a href=&#34;https://github.com/kubernetes/features/issues/515&#34; target=&#34;_blank&#34;&gt;kubectl get and describe should work well with extensions&lt;/a&gt; (alpha) to make the server, rather than the client, return this information for a smoother user experience.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/516&#34; target=&#34;_blank&#34;&gt;Support for self-hosting authorizer webhook&lt;/a&gt; (alpha): Earlier versions of Kubernetes brought us the authorizer webhooks, which make it possible to customize the enforcement of permissions before commands are executed. Those webhooks, however, have to live somewhere, and this new feature makes it possible to host them in the cluster itself.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;storage-sig-storage&#34;&gt;Storage (SIG-Storage)&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/496&#34; target=&#34;_blank&#34;&gt;Detailed storage metrics of internal state&lt;/a&gt; (stable): With a distributed system such as Kubernetes, it&amp;rsquo;s particularly important to know what&amp;rsquo;s going on inside the system at any given time, either for troubleshooting purposes or simply for automation. This release brings to general availability detailed metrics of what&amp;rsquo;s going in inside the storage systems, including metrics such as mount and unmount time, number of volumes in a particular state, and number of orphaned pod directories. You can find a &lt;a href=&#34;https://docs.google.com/document/d/1Fh0T60T_y888LsRwC51CQHO75b2IZ3A34ZQS71s_F0g/edit#heading=h.ys6pjpbasqdu&#34; target=&#34;_blank&#34;&gt;full list in this design document&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/432&#34; target=&#34;_blank&#34;&gt;Mount namespace propagation&lt;/a&gt; (beta): This feature allows a container to mount a volume as rslave so that host mounts can be seen inside the container, or as rshared so that any mounts from inside the container are reflected in the host&amp;rsquo;s mount namespace. The default for this feature is rslave.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/361&#34; target=&#34;_blank&#34;&gt;Local Ephemeral Storage Capacity Isolation&lt;/a&gt; (beta): Without this feature in place, every pod on a node that is using ephemeral storage is pulling from the same pool, and allocating storage is on a &amp;ldquo;best-effort&amp;rdquo; basis; in other words, a pod never knows for sure how much space it has available. This function provides the ability for a pod to reserve its own storage.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/178&#34; target=&#34;_blank&#34;&gt;Out-of-tree CSI Volume Plugins&lt;/a&gt; (beta): Kubernetes 1.9 announced the release of the Container Storage Interface, which provides a standard way for vendors to provide storage to Kubernetes. This function makes it possible for them to create drivers that live &amp;ldquo;out-of-tree&amp;rdquo;, or out of the normal Kubernetes core. This means that vendors can control their own plugins and don&amp;rsquo;t have to rely on the community for code reviews and approvals.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/121&#34; target=&#34;_blank&#34;&gt;Local Persistent Storage&lt;/a&gt; (beta): This feature enables PersistentVolumes to be created with locally attached disks, and not just network volumes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/498&#34; target=&#34;_blank&#34;&gt;Prevent deletion of Persistent Volume Claims that are used by a pod&lt;/a&gt; (beta) and 7. &lt;a href=&#34;https://github.com/kubernetes/features/issues/499&#34; target=&#34;_blank&#34;&gt;Prevent deletion of Persistent Volume that is bound to a Persistent Volume Claim&lt;/a&gt; (beta): In previous versions of Kubernetes it was possible to delete storage that is in use by a pod, causing massive problems for the pod. These features provide validation that prevents that from happening.&lt;/li&gt;
&lt;li&gt;Running out of storage space on your Persistent Volume? If you are, you can use &lt;a href=&#34;https://github.com/kubernetes/features/issues/531&#34; target=&#34;_blank&#34;&gt;Add support for online resizing of PVs&lt;/a&gt; (alpha) to enlarge the underlying volume it without disrupting existing data. This also works in conjunction with the new &lt;a href=&#34;https://github.com/kubernetes/features/issues/304&#34; target=&#34;_blank&#34;&gt;Add resize support for FlexVolume&lt;/a&gt; (alpha); FlexVolumes are vendor-supported volumes implemented through &lt;a href=&#34;http://leebriggs.co.uk/blog/2017/03/12/kubernetes-flexvolumes.html&#34; target=&#34;_blank&#34;&gt;FlexVolume&lt;/a&gt; plugins.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/490&#34; target=&#34;_blank&#34;&gt;Topology Aware Volume Scheduling&lt;/a&gt; (beta): This feature enables you to specify topology constraints on PersistentVolumes and have those constraints evaluated by the scheduler. It also delays the initial PersistentVolumeClaim binding until the Pod has been scheduled so that the volume binding decision is smarter and considers all Pod scheduling constraints as well. At the moment, this feature is most useful for local persistent volumes, but support for dynamic provisioning is under development.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;node-management-sig-node&#34;&gt;Node management (SIG-Node)&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/281&#34; target=&#34;_blank&#34;&gt;Dynamic Kubelet Configuration&lt;/a&gt; (beta): Kubernetes makes it easy to make changes to existing clusters, such as increasing the number of replicas or making a service available over the network. This feature makes it possible to change Kubernetes itself (or rather, the Kubelet that runs Kubernetes behind the scenes) without bringing down the node on which Kubelet is running.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/292&#34; target=&#34;_blank&#34;&gt;CRI validation test suite&lt;/a&gt; (beta): The Container Runtime Interface (CRI) makes it possible to run containers other than Docker (such as Rkt containers or even virtual machines using Virtlet) on Kubernetes. This features provides a suite of validation tests to make certain that these CRI implementations are compliant, enabling developers to more easily find problems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/495&#34; target=&#34;_blank&#34;&gt;Configurable Pod Process Namespace Sharing&lt;/a&gt; (alpha): Although pods can easily share the Kubernetes namespace, the process, or PID namespace has been a more difficult issue due to lack of support in Docker. This feature enables you to set a parameter on the pod to determine whether containers get their own operating system processes or share a single process.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/547&#34; target=&#34;_blank&#34;&gt;Add support for Windows Container Configuration in CRI&lt;/a&gt; (alpha): The Container Runtime Interface was originally designed with Linux-based containers in mind, and it was impossible to implement support for Windows-based containers using CRI. This feature solves that problem, making it possible to specify a WindowsContainerConfig.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/277&#34; target=&#34;_blank&#34;&gt;Debug Containers&lt;/a&gt; (alpha): It&amp;rsquo;s easy to debug a container if you have the appropriate utilities. But what if you don&amp;rsquo;t? This feature makes it possible to run debugging tools on a container even if those tools weren&amp;rsquo;t included in the original container image.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;other-changes&#34;&gt;Other changes:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Deployment (SIG-Cluster Lifecycle): &lt;a href=&#34;https://github.com/kubernetes/features/issues/88&#34; target=&#34;_blank&#34;&gt;Support out-of-process and out-of-tree cloud providers&lt;/a&gt; (beta): As Kubernetes gains acceptance, more and more cloud providers will want to make it available. To do that more easily, the community is working on extracting provider-specific binaries so that they can be more easily replaced.&lt;/li&gt;
&lt;li&gt;Kubernetes on Azure (SIG-Azure): Kubernetes has a cluster-autoscaler that automatically adds nodes to your cluster if you&amp;rsquo;re running too many workloads, but until now it wasn&amp;rsquo;t available on Azure. The &lt;a href=&#34;https://github.com/kubernetes/features/issues/514&#34; target=&#34;_blank&#34;&gt;Add Azure support to cluster-autoscaler&lt;/a&gt; (alpha) feature aims to fix that. Closely related, the &lt;a href=&#34;https://github.com/kubernetes/features/issues/513&#34; target=&#34;_blank&#34;&gt;Add support for Azure Virtual Machine Scale Sets&lt;/a&gt; (alpha) feature makes use of Azure&amp;rsquo;s own autoscaling capabilities to make resources available.
You can download the Kubernetes 1.10 beta from . Again, if you&amp;rsquo;ve got feedback (and the community hopes you do) please add an issue to the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/milestone/37&#34; target=&#34;_blank&#34;&gt;1.10 milestone&lt;/a&gt; and tag the relevant SIG before March 9.&lt;br /&gt;
_&lt;br /&gt;
(Many thanks to community members Michelle Au, Jan Šafránek, Eric Chiang, Michał Nasiadka, Radosław Pieczonka, Xing Yang, Daniel Smith, sylvain boily, Leo Sunmo, Michal Masłowski, Fernando Ripoll, ayodele abejide, Brett Kochendorfer, Andrew Randall, Casey Davenport, Duffie Cooley, Bryan Venteicher, Mark Ayers, Christopher Luciano, and Sandor Szuecs for their invaluable help in reviewing this article for accuracy.)_&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Reporting Errors from Control Plane to Applications Using Kubernetes Events</title>
      <link>https://kubernetes.io/blog/2018/01/reporting-errors-using-kubernetes-events/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/01/reporting-errors-using-kubernetes-events/</guid>
      <description>
        
        
        &lt;p&gt;At &lt;a href=&#34;https://www.box.com/&#34; target=&#34;_blank&#34;&gt;Box&lt;/a&gt;, we manage several large scale Kubernetes clusters that serve as an internal platform as a service (PaaS) for hundreds of deployed microservices. The majority of those microservices are applications that power box.com for over 80,000 customers. The PaaS team also deploys several services affiliated with the platform infrastructure as the &lt;em&gt;control plane&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One use case of Box’s control plane is &lt;a href=&#34;https://en.wikipedia.org/wiki/Public_key_infrastructure&#34; target=&#34;_blank&#34;&gt;public key infrastructure&lt;/a&gt; (&lt;em&gt;PKI&lt;/em&gt;) processing. In our infrastructure, applications needing a new SSL certificate also need to trigger some processing in the control plane. The majority of our applications are not allowed to generate new SSL certificates due to security reasons. The control plane has a different security boundary and network access, and is therefore allowed to generate certificates.&lt;/p&gt;

&lt;p&gt;| &lt;img src=&#34;https://docs.google.com/a/linuxfoundation.org/drawings/d/snd-Vdn8h65V5wEBwU0KIqg/image?w=624&amp;amp;h=554&amp;amp;rev=303&amp;amp;ac=1&#34; alt=&#34;&#34; /&gt;  |
| Figure1: Block Diagram of the PKI flow |&lt;/p&gt;

&lt;p&gt;If an application needs a new certificate, the application owner explicitly adds a &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/&#34; target=&#34;_blank&#34;&gt;Custom Resource Definition&lt;/a&gt; (CRD) to the application’s Kubernetes config [1]. This CRD specifies parameters for the SSL certificate: &lt;em&gt;name, common name, and others&lt;/em&gt;. A microservice in the control plane watches CRDs and triggers some processing for SSL certificate generation [2]. Once the certificate is ready, the same control plane service sends it to the API server in a Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34; target=&#34;_blank&#34;&gt;Secret&lt;/a&gt; [3]. After that, the application containers access their certificates using Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#secret&#34; target=&#34;_blank&#34;&gt;Secret VolumeMounts&lt;/a&gt; [4]. You can see a working demo of this system in our &lt;a href=&#34;https://github.com/box/error-reporting-with-kubernetes-events&#34; target=&#34;_blank&#34;&gt;example application&lt;/a&gt; on GitHub.&lt;/p&gt;

&lt;p&gt;The rest of this post covers the error scenarios in this “triggered” processing in the control plane. In particular, we are especially concerned with user input errors. Because the SSL certificate parameters come from the application’s config file in a CRD format, what should happen if there is an error in that CRD specification? Even a typo results in a failure of the SSL certificate creation. The error information is available in the control plane even though the root cause is most probably inside the application’s config file. The application owner does not have access to the control plane’s state or logs.&lt;/p&gt;

&lt;p&gt;Providing the right diagnosis to the application owner so she can fix the mistake becomes a serious productivity problem at scale. Box’s rapid migration to microservices results in several new deployments every week. Numerous first time users, who do not know every detail of the infrastructure, need to succeed in deploying their services and troubleshooting problems easily. As the owners of the infrastructure, we do not want to be the bottleneck while reading the errors from the control plane logs and passing them on to application owners. If something in an owner’s configuration causes an error somewhere else, owners need a fully empowering diagnosis. This error data must flow automatically without any human involvement.&lt;/p&gt;

&lt;p&gt;After considerable thought and experimentation, we found that &lt;a href=&#34;https://v1-7.docs.kubernetes.io/docs/api-reference/v1.7/#event-v1-core&#34; target=&#34;_blank&#34;&gt;Kubernetes Events&lt;/a&gt; work great to automatically communicate these kind of errors. If the error information is placed in a pod’s event stream, it shows up in kubectl describe output. Even beginner users can execute kubectl describe pod and obtain an error diagnosis.&lt;/p&gt;

&lt;p&gt;We experimented with a status web page for the control plane service as an alternative to Kubernetes Events. We determined that the status page could update every time after processing an SSL certificate, and that application owners could probe the status page and get the diagnosis from there. After experimenting with a status page initially, we have seen that this does not work as effectively as the Kubernetes Events solution. The status page becomes a new interface to learn for the application owner, a new web address to remember, and one more context switch to a distinct tool during troubleshooting efforts. On the other hand, Kubernetes Events show up cleanly at the kubectl describe output, which is easily recognized by the developers.&lt;/p&gt;

&lt;p&gt;Here is a simplified example showing how we used Kubernetes Events for error reporting across distinct services. We have open sourced a &lt;a href=&#34;https://github.com/box/error-reporting-with-kubernetes-events&#34; target=&#34;_blank&#34;&gt;sample golang application&lt;/a&gt; representative of the previously mentioned control plane service. It watches changes on CRDs and does input parameter checking. If an error is discovered, a Kubernetes Event is generated and the relevant pod’s event stream is updated.&lt;/p&gt;

&lt;p&gt;The sample application executes this &lt;a href=&#34;https://github.com/box/error-reporting-with-kubernetes-events/blob/master/cmd/controlplane/main.go#L201&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt; to setup the Kubernetes Event generation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// eventRecorder returns an EventRecorder type that can be  
// used to post Events to different object&#39;s lifecycles.  
func eventRecorder(  
   kubeClient \*kubernetes.Clientset) (record.EventRecorder, error) {  
   eventBroadcaster := record.NewBroadcaster()  
   eventBroadcaster.StartLogging(glog.Infof)  
   eventBroadcaster.StartRecordingToSink(  
      &amp;amp;typedcorev1.EventSinkImpl{  
         Interface: kubeClient.CoreV1().Events(&amp;quot;&amp;quot;)})  
   recorder := eventBroadcaster.NewRecorder(  
      scheme.Scheme,  
      v1.EventSource{Component: &amp;quot;controlplane&amp;quot;})  
   return recorder, nil  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the one-time setup, the following &lt;a href=&#34;https://github.com/box/error-reporting-with-kubernetes-events/blob/master/cmd/controlplane/main.go#L163&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt; generates events affiliated with pods:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ref, err := reference.GetReference(scheme.Scheme, &amp;amp;pod)  
if err != nil {  
   glog.Fatalf(&amp;quot;Could not get reference for pod %v: %v\n&amp;quot;,  
      pod.Name, err)  
}  
recorder.Event(ref, v1.EventTypeWarning, &amp;quot;pki ServiceName error&amp;quot;,  
   fmt.Sprintf(&amp;quot;ServiceName: %s in pki: %s is not found in&amp;quot;+  
      &amp;quot; allowedNames: %s&amp;quot;, pki.Spec.ServiceName, pki.Name,  
      allowedNames))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Further implementation details can be understood by running the sample application.&lt;/p&gt;

&lt;p&gt;As mentioned previously, here is the relevant kubectl describe output for the application owner.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Events:  
  FirstSeen   LastSeen   Count   From         SubObjectPath   Type      Reason         Message  
  ---------   --------   -----   ----         -------------   --------   ------     
  ....  
  1d      1m      24   controlplane            Warning      pki ServiceName error   ServiceName: appp1 in pki: app1-pki is not found in allowedNames: [app1 app2]  
  ....  

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have demonstrated a practical use case with Kubernetes Events. The automated feedback to programmers in the case of configuration errors has significantly improved our troubleshooting efforts. In the future, we plan to use Kubernetes Events in various other applications under similar use cases. The recently created &lt;a href=&#34;https://github.com/kubernetes/sample-controller&#34; target=&#34;_blank&#34;&gt;sample-controller&lt;/a&gt; example also utilizes Kubernetes Events in a similar scenario. It is great to see there are more sample applications to guide the community. We are excited to continue exploring other use cases for Events and the rest of the Kubernetes API to make development easier for our engineers.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you have a Kubernetes experience you’d like to share, &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;submit your story&lt;/a&gt;. If you use Kubernetes in your organization and want to voice your experience more directly, consider joining the &lt;a href=&#34;https://www.cncf.io/people/end-user-community/&#34; target=&#34;_blank&#34;&gt;CNCF End User Community&lt;/a&gt; that Box and dozens of like-minded companies are part of.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Special thanks for Greg Lyons and Mohit Soni for their contributions.&lt;br /&gt;
Hakan Baba, Sr. Software Engineer, Box&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Core Workloads API GA</title>
      <link>https://kubernetes.io/blog/2018/01/core-workloads-api-ga/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/01/core-workloads-api-ga/</guid>
      <description>
        
        
        

&lt;h2 id=&#34;daemonset-deployment-replicaset-and-statefulset-are-ga&#34;&gt;DaemonSet, Deployment, ReplicaSet, and StatefulSet are GA&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor’s Note: We’re happy to announce that the Core Workloads API is GA in Kubernetes 1.9! This blog post from Kenneth Owens reviews how Core Workloads got to GA from its origins, reveals changes in 1.9, and talks about what you can expect going forward.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;in-the-beginning&#34;&gt;In the Beginning …&lt;/h2&gt;

&lt;p&gt;There were &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/&#34; target=&#34;_blank&#34;&gt;Pods&lt;/a&gt;, tightly coupled containers that share resource requirements, networking, storage, and a lifecycle. Pods were useful, but, as it turns out, users wanted to seamlessly, reproducibly, and automatically create many identical replicas of the same Pod, so we created &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/&#34; target=&#34;_blank&#34;&gt;ReplicationController&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Replication was a step forward, but what users really needed was higher level orchestration of their replicated Pods. They wanted rolling updates, roll backs, and roll overs. So the OpenShift team created &lt;a href=&#34;https://docs.openshift.org/latest/architecture/core_concepts/deployments.html#deployments-and-deployment-configurations&#34; target=&#34;_blank&#34;&gt;DeploymentConfig&lt;/a&gt;. DeploymentConfigs were also useful, and OpenShift users were happy. In order to allow all OSS Kubernetes uses to share in the elation, and to take advantage of &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34; target=&#34;_blank&#34;&gt;set-based label selectors&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&#34; target=&#34;_blank&#34;&gt;ReplicaSet&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;Deployment&lt;/a&gt; were added to the extensions/v1beta1 group version providing rolling updates, roll backs, and roll overs for all Kubernetes users.&lt;/p&gt;

&lt;p&gt;That mostly solved the problem of orchestrating containerized 12 factor apps on Kubernetes, so the community turned its attention to a different problem. Replicating a Pod &amp;lt;n&amp;gt; times isn’t the right hammer for every nail in your cluster. Sometimes, you need to run a Pod on every Node, or on a subset of Nodes (for example, shared side cars like log shippers and metrics collectors, Kubernetes add-ons, and Distributed File Systems). The state of the art was Pods combined with NodeSelectors, or static Pods, but this is unwieldy. After having grown used to the ease of automation provided by Deployments, users demanded the same features for this category of application, so &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34; target=&#34;_blank&#34;&gt;DaemonSet&lt;/a&gt; was added to extension/v1beta1 as well.&lt;/p&gt;

&lt;p&gt;For a time, users were content, until they decided that Kubernetes needed to be able to orchestrate more than just 12 factor apps and cluster infrastructure. Whether your architecture is N-tier, service oriented, or micro-service oriented, your 12 factor apps depend on stateful workloads (for example, RDBMSs, distributed key value stores, and messaging queues) to provide services to end users and other applications. These stateful workloads can have availability and durability requirements that can only be achieved by distributed systems, and users were ready to use Kubernetes to orchestrate the entire stack.&lt;/p&gt;

&lt;p&gt;While Deployments are great for stateless workloads, they don’t provide the right guarantees for the orchestration of distributed systems. These applications can require stable network identities, ordered, sequential deployment, updates, and deletion, and stable, durable storage. &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/upgrade-pet-set-to-stateful-set/&#34; target=&#34;_blank&#34;&gt;PetSet&lt;/a&gt; was added to the apps/v1beta1 group version to address this category of application. Unfortunately, &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/27430&#34; target=&#34;_blank&#34;&gt;we were less than thoughtful with its naming&lt;/a&gt;, and, as we always strive to be an inclusive community, we renamed the kind to &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; target=&#34;_blank&#34;&gt;StatefulSet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we were done.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/0T36knExav8JAr41ict3EVOPOqaIJPMBQrOT2N5jehXw_12jEILD87tKW8BvaK2UCOtCHzS700Oki8Fxja3bF37J3eceanEBjbHpRsATBhC1y3P0mas7DvPeQjt6QmfYuNWDqZVl&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;Or were we?&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-1-8-and-apps-v1beta2&#34;&gt;Kubernetes 1.8 and apps/v1beta2&lt;/h2&gt;

&lt;p&gt;Pod, ReplicationController, ReplicaSet, Deployment, DaemonSet, and StatefulSet came to collectively be known as the core workloads API. We could finally orchestrate all of the things, but the API surface was spread across three groups, had many inconsistencies, and left users wondering about the stability of each of the core workloads kinds. It was time to stop adding new features and focus on consistency and stability.&lt;/p&gt;

&lt;p&gt;Pod and ReplicationController were at GA stability, and even though you can run a workload in a Pod, it’s a nucleus primitive that belongs in core. As Deployments are the recommended way to manage your stateless apps, moving ReplicationController would serve no purpose. In Kubernetes 1.8, we moved all the other core workloads API kinds (Deployment, DaemonSet, ReplicaSet, and StatefulSet) to the apps/v1beta2 group version. This had the benefit of providing a better aggregation across the API surface, and allowing us to break backward compatibility to fix inconsistencies. Our plan was to promote this new surface to GA, wholesale and as is, when we were satisfied with its completeness. The modifications in this release, which are also implemented in apps/v1, are described below.&lt;/p&gt;

&lt;h3 id=&#34;selector-defaulting-deprecated&#34;&gt;Selector Defaulting Deprecated&lt;/h3&gt;

&lt;p&gt;In prior versions of the apps and extensions groups, label selectors of the core workloads API kinds were, when left unspecified, defaulted to a label selector generated from the kind’s template’s labels.&lt;/p&gt;

&lt;p&gt;This was completely incompatible with strategic merge patch and kubectl apply. Moreover, we’ve found that defaulting the value of a field from the value of another field of the same object is an anti-pattern, in general, and particularly dangerous for the API objects used to orchestrate workloads.&lt;/p&gt;

&lt;h3 id=&#34;immutable-selectors&#34;&gt;Immutable Selectors&lt;/h3&gt;

&lt;p&gt;Selector mutation, while allowing for some use cases like promotable Deployment canaries, is not handled gracefully by our workload controllers, and we have always &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#label-selector-updates&#34; target=&#34;_blank&#34;&gt;strongly cautioned users against it&lt;/a&gt;. To provide a consistent, usable, and stable API, selectors were made immutable for all kinds in the workloads API.&lt;/p&gt;

&lt;p&gt;We believe that there are better ways to support features like promotable canaries and orchestrated Pod relabeling, but, if restricted selector mutation is a necessary feature for our users, we can relax immutability in the future without breaking backward compatibility.&lt;/p&gt;

&lt;p&gt;The development of features like promotable canaries, orchestrated Pod relabeling, and restricted selector mutability is driven by demand signals from our users. If you are currently modifying the selectors of your core workload API objects, please tell us about your use case via a GitHub issue, or by participating in SIG apps.&lt;/p&gt;

&lt;h3 id=&#34;default-rolling-updates&#34;&gt;Default Rolling Updates&lt;/h3&gt;

&lt;p&gt;Prior to apps/v1beta2, some kinds defaulted their update strategy to something other than RollingUpdate (e.g. app/v1beta1/StatefulSet uses OnDelete by default). We wanted to be confident that RollingUpdate worked well prior to making it the default update strategy, and we couldn’t change the default behavior in released versions without breaking our promise with respect to backward compatibility. In apps/v1beta2 we enabled RollingUpdate for all core workloads kinds by default.&lt;/p&gt;

&lt;h3 id=&#34;createdby-annotation-deprecated&#34;&gt;CreatedBy Annotation Deprecated&lt;/h3&gt;

&lt;p&gt;The &amp;ldquo;kubernetes.io/created-by&amp;rdquo; was a legacy hold over from the days before garbage collection. Users should use an object’s ControllerRef from its ownerReferences to determine object ownership. We deprecated this feature in 1.8 and removed it in 1.9.&lt;/p&gt;

&lt;h3 id=&#34;scale-subresources&#34;&gt;Scale Subresources&lt;/h3&gt;

&lt;p&gt;A scale subresource was added to all of the applicable kinds in apps/v1beta2 (DaemonSet scales based on its node selector).&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-1-9-and-apps-v1&#34;&gt;Kubernetes 1.9 and apps/v1&lt;/h2&gt;

&lt;p&gt;In Kubernetes 1.9, as planned, we promoted the entire core workloads API surface to GA in the apps/v1 group version. We made a few more changes to make the API consistent, but apps/v1 is mostly identical to apps/v1beta2. The reality is that most users have been treating the beta versions of the core workloads API as GA for some time now. Anyone who is still using ReplicationControllers and shying away from DaemonSets, Deployments, and StatefulSets, due to a perceived lack of stability, should plan migrate their workloads (where applicable) to apps/v1. The minor changes that were made during promotion are described below.&lt;/p&gt;

&lt;h3 id=&#34;garbage-collection-defaults-to-delete&#34;&gt;Garbage Collection Defaults to Delete&lt;/h3&gt;

&lt;p&gt;Prior to apps/v1 the default garbage collection policy for Pods in a DaemonSet, Deployment, ReplicaSet, or StatefulSet, was to orphan the Pods. That is, if you deleted one of these kinds, the Pods that they owned would not be deleted automatically unless cascading deletion was explicitly specified. If you use kubectl, you probably didn’t notice this, as these kinds are scaled to zero prior to deletion. In apps/v1 all core worloads API objects will now, by default, be deleted when their owner is deleted. For most users, this change is transparent.&lt;br /&gt;
Status Conditions&lt;/p&gt;

&lt;p&gt;Prior to apps/v1 only Deployment and ReplicaSet had Conditions in their Status objects. For consistency&amp;rsquo;s sake, either all of the objects or none of them should have conditions. After some debate, we decided that Conditions are useful, and we added Conditions to StatefulSetStatus and DaemonSetStatus. The StatefulSet and DaemonSet controllers currently don’t populate them, but we may choose communicate conditions to clients, via this mechanism, in the future.&lt;/p&gt;

&lt;h3 id=&#34;scale-subresource-migrated-to-autoscale-v1&#34;&gt;Scale Subresource Migrated to autoscale/v1&lt;/h3&gt;

&lt;p&gt;We originally added a scale subresource to the apps group. This was the wrong direction for integration with the autoscaling, and, at some point, we would like to use custom metrics to autoscale StatefulSets. So the apps/v1 group version uses the autoscaling/v1 scale subresource.&lt;/p&gt;

&lt;h2 id=&#34;migration-and-deprecation&#34;&gt;Migration and Deprecation&lt;/h2&gt;

&lt;p&gt;The question most you’re probably asking now is, “What’s my migration path onto apps/v1 and how soon should I plan on migrating?” All of the group versions prior to apps/v1 are deprecated as of Kubernetes 1.9, and all new code should be developed against apps/v1, but, as discussed above, many of our users treat extensions/v1beta1 as if it were GA. We realize this, and the minimum support timelines in our &lt;a href=&#34;https://kubernetes.io/docs/reference/deprecation-policy/&#34; target=&#34;_blank&#34;&gt;deprecation policy&lt;/a&gt; are just that, minimums.&lt;/p&gt;

&lt;p&gt;In future releases, before completely removing any of the group versions, we will disable them by default in the API Server. At this point, you will still be able to use the group version, but you will have to explicitly enable it. We will also provide utilities to upgrade the storage version of the API objects to apps/v1. Remember, all of the versions of the core workloads kinds are bidirectionally convertible. If you want to manually update your core workloads API objects now, you can use &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#convert&#34; target=&#34;_blank&#34;&gt;kubectl convert&lt;/a&gt; to convert manifests between group versions.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;The core workloads API surface is stable, but it’s still software, and software is never complete. We often add features to stable APIs to support new use cases, and we will likely do so for the core workloads API as well. GA stability means that any new features that we do add will be strictly backward compatible with the existing API surface. From this point forward, nothing we do will break our backwards compatibility guarantees. If you’re looking to participate in the evolution of this portion of the API, please feel free to get involved in &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; or to participate in &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-apps&#34; target=&#34;_blank&#34;&gt;SIG Apps&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;ndash;Kenneth Owens, Software Engineer, Google&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://get.k8s.io/&#34; target=&#34;_blank&#34;&gt;Download&lt;/a&gt; Kubernetes&lt;/li&gt;
&lt;li&gt;Get involved with the Kubernetes project on &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Connect with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing client-go version 6</title>
      <link>https://kubernetes.io/blog/2018/01/introducing-client-go-version-6/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/01/introducing-client-go-version-6/</guid>
      <description>
        
        
        

&lt;p&gt;The Kubernetes API server &lt;a href=&#34;https://blog.openshift.com/tag/api-server/&#34; target=&#34;_blank&#34;&gt;exposes a REST interface&lt;/a&gt; consumable by any client. &lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;client-go&lt;/a&gt; is the official client library for the Go programming language. It is used both internally by Kubernetes itself (for example, inside kubectl) as well as by &lt;a href=&#34;https://github.com/search?q=k8s.io%2Fclient-go&amp;amp;type=Code&amp;amp;utf8=%E2%9C%93&#34; target=&#34;_blank&#34;&gt;numerous external consumers&lt;/a&gt;:operators like the &lt;a href=&#34;https://github.com/coreos/etcd-operator&#34; target=&#34;_blank&#34;&gt;etcd-operator&lt;/a&gt; or &lt;a href=&#34;https://github.com/coreos/prometheus-operator&#34; target=&#34;_blank&#34;&gt;prometheus-operator;&lt;/a&gt;higher level frameworks like &lt;a href=&#34;https://github.com/kubeless/kubeless&#34; target=&#34;_blank&#34;&gt;KubeLess&lt;/a&gt; and &lt;a href=&#34;https://openshift.io/&#34; target=&#34;_blank&#34;&gt;OpenShift&lt;/a&gt;; and many more.&lt;/p&gt;

&lt;p&gt;The version 6 update to client-go adds support for Kubernetes 1.9, allowing access to the latest Kubernetes features. While the &lt;a href=&#34;https://github.com/kubernetes/client-go/blob/master/CHANGELOG.md&#34; target=&#34;_blank&#34;&gt;changelog&lt;/a&gt; contains all the gory details, this blog post highlights the most prominent changes and intends to guide on how to upgrade from version 5.&lt;/p&gt;

&lt;p&gt;This blog post is one of a number of efforts to make client-go more accessible to third party consumers. Easier access is a joint effort by a number of people from numerous companies, all meeting in the #client-go-docs channel of the &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Kubernetes Slack&lt;/a&gt;. We are happy to hear feedback and ideas for further improvement, and of course appreciate anybody who wants to contribute.&lt;/p&gt;

&lt;h2 id=&#34;api-group-changes&#34;&gt;API group changes&lt;/h2&gt;

&lt;p&gt;The following API group promotions are part of Kubernetes 1.9:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Workload objects (Deployments, DaemonSets, ReplicaSets, and StatefulSets) have been &lt;a href=&#34;https://kubernetes.io/docs/reference/workloads-18-19/&#34; target=&#34;_blank&#34;&gt;promoted to the apps/v1 API group in Kubernetes 1.9&lt;/a&gt;. client-go follows this transition and allows developers to use the latest version by importing the k8s.io/api/apps/v1 package instead of k8s.io/api/apps/v1beta1 and by using Clientset.AppsV1().&lt;/li&gt;
&lt;li&gt;Admission Webhook Registration has been promoted to the admissionregistration.k8s.io/v1beta1 API group in Kubernetes 1.9. The former ExternalAdmissionHookConfiguration type has been replaced by the incompatible ValidatingWebhookConfiguration and MutatingWebhookConfiguration types. Moreover, the webhook admission payload type AdmissionReview in admission.k8s.io has been promoted to v1beta1. Note that versioned objects are now passed to webhooks. Refer to the admission webhook &lt;a href=&#34;https://kubernetes.io/docs/admin/extensible-admission-controllers/#external-admission-webhooks&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt; for details.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;validation-for-customresources&#34;&gt;Validation for CustomResources&lt;/h2&gt;

&lt;p&gt;In Kubernetes 1.8 we introduced CustomResourceDefinitions (CRD) &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation&#34; target=&#34;_blank&#34;&gt;pre-persistence schema validation&lt;/a&gt; as an alpha feature. With 1.9, the feature got promoted to beta and will be enabled by default. As a client-go user, you will find the API types at k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1beta1.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject&#34; target=&#34;_blank&#34;&gt;OpenAPI v3 schema&lt;/a&gt; can be defined in the CRD spec as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
apiVersion: apiextensions.k8s.io/v1beta1  
kind: CustomResourceDefinition  
metadata: ...  
spec:  
  ...  
  validation:  
    openAPIV3Schema:  
      properties:  
        spec:  
          properties:  
            version:  
                type: string  
                enum:  
                - &amp;quot;v1.0.0&amp;quot;  
                - &amp;quot;v1.0.1&amp;quot;  
            replicas:  
                type: integer  
                minimum: 1  
                maximum: 10

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The schema in the above CRD applies following validations for the instance:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;spec.version must be a string and must be either “v1.0.0” or “v1.0.1”.&lt;/li&gt;
&lt;li&gt;spec.replicas must be an integer and must have a minimum value of 1 and a maximum value of 10.
A CustomResource with invalid values for spec.version (v1.0.2) and spec.replicas (15) will be rejected:&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;
apiVersion: mygroup.example.com/v1  
kind: App  
metadata:  
  name: example-app  
spec:  
  version: &amp;quot;v1.0.2&amp;quot;  
  replicas: 15

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f app.yaml

The App &amp;quot;example-app&amp;quot; is invalid: []: Invalid value: map[string]interface {}{&amp;quot;apiVersion&amp;quot;:&amp;quot;mygroup.example.com/v1&amp;quot;, &amp;quot;kind&amp;quot;:&amp;quot;App&amp;quot;, &amp;quot;metadata&amp;quot;:map[string]interface {}{&amp;quot;creationTimestamp&amp;quot;:&amp;quot;2017-08-31T20:52:54Z&amp;quot;, &amp;quot;uid&amp;quot;:&amp;quot;5c674651-8e8e-11e7-86ad-f0761cb232d1&amp;quot;, &amp;quot;selfLink&amp;quot;:&amp;quot;&amp;quot;, &amp;quot;clusterName&amp;quot;:&amp;quot;&amp;quot;, &amp;quot;name&amp;quot;:&amp;quot;example-app&amp;quot;, &amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;, &amp;quot;deletionTimestamp&amp;quot;:interface {}(nil), &amp;quot;deletionGracePeriodSeconds&amp;quot;:(\*int64)(nil)}, &amp;quot;spec&amp;quot;:map[string]interface {}{&amp;quot;replicas&amp;quot;:15, &amp;quot;version&amp;quot;:&amp;quot;v1.0.2&amp;quot;}}:  
validation failure list:  
spec.replicas in body should be less than or equal to 10  
spec.version in body should be one of [v1.0.0 v1.0.1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that with &lt;a href=&#34;https://kubernetes.io/docs/admin/extensible-admission-controllers/#external-admission-webhooks&#34; target=&#34;_blank&#34;&gt;Admission Webhooks&lt;/a&gt;, Kubernetes 1.9 provides another beta feature to validate objects before they are created or updated. Starting with 1.9, these webhooks also allow mutation of objects (for example, to set defaults or to inject values). Of course, webhooks work with CRDs as well. Moreover, webhooks can be used to implement validations that are not easily expressible with CRD validation. Note that webhooks are harder to implement than CRD validation, so for many purposes, CRD validation is the right tool.&lt;/p&gt;

&lt;h2 id=&#34;creating-namespaced-informers&#34;&gt;Creating namespaced informers&lt;/h2&gt;

&lt;p&gt;Often objects in one namespace or only with certain labels are to be processed in a controller. Informers &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/54660&#34; target=&#34;_blank&#34;&gt;now allow&lt;/a&gt; you to tweak the ListOptions used to query the API server to list and watch objects. Uninitialized objects (for consumption by &lt;a href=&#34;https://kubernetes.io/docs/admin/extensible-admission-controllers/#what-are-initializers&#34; target=&#34;_blank&#34;&gt;initializers&lt;/a&gt;) can be made visible by setting IncludeUnitialized to true. All this can be done using the new NewFilteredSharedInformerFactory constructor for shared informers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
import “k8s.io/client-go/informers”
...  
sharedInformers := informers.NewFilteredSharedInformerFactory(  
 client,  
 30\*time.Minute,   
 “some-namespace”,  
 func(opt \*metav1.ListOptions) {  
  opt.LabelSelector = “foo=bar”  
 },  
)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the corresponding lister will only know about the objects matching the namespace and the given ListOptions. Note that the same restrictions apply for a List or Watch call on a client.&lt;/p&gt;

&lt;p&gt;This &lt;a href=&#34;https://github.com/jetstack/cert-manager/blob/b978faa28c9f0fb0414b5d7293fab7bde65bde76/cmd/controller/app/controller.go#L123&#34; target=&#34;_blank&#34;&gt;production code example&lt;/a&gt; of a cert-manager demonstrates how namespace informers can be used in real code.&lt;/p&gt;

&lt;h2 id=&#34;polymorphic-scale-client&#34;&gt;Polymorphic scale client&lt;/h2&gt;

&lt;p&gt;Historically, only types in the extensions API group would work with autogenerated Scale clients. Furthermore, different API groups use different Scale types for their /scale subresources. To remedy these issues, k8s.io/client-go/scale provides a &lt;a href=&#34;https://github.com/kubernetes/client-go/tree/master/scale&#34; target=&#34;_blank&#34;&gt;polymorphic scale client&lt;/a&gt; to scale different resources in different API groups in a coherent way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
import (


apimeta &amp;quot;k8s.io/apimachinery/pkg/api/meta&amp;quot;

 discocache &amp;quot;k8s.io/client-go/discovery/cached&amp;quot;  
 &amp;quot;k8s.io/client-go/discovery&amp;quot;

&amp;quot;k8s.io/client-go/dynamic&amp;quot;

“k8s.io/client-go/scale”  
)

...

cachedDiscovery := discocache.NewMemCacheClient(client.Discovery())  
restMapper := discovery.NewDeferredDiscoveryRESTMapper(

cachedDiscovery,

apimeta.InterfacesForUnstructured,

)  
scaleKindResolver := scale.NewDiscoveryScaleKindResolver(

client.Discovery(),

)  
scaleClient, err := scale.NewForConfig(

client, restMapper,

dynamic.LegacyAPIPathResolverFunc,

scaleKindResolver,

)
scale, err := scaleClient.Scales(&amp;quot;default&amp;quot;).Get(groupResource, &amp;quot;foo&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The returned scale object is generic and is exposed as the autoscaling/v1.Scale object. It is backed by an internal Scale type, with conversions defined to and from all the special Scale types in the API groups supporting scaling. We planto &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/55168&#34; target=&#34;_blank&#34;&gt;extend this to CustomResources in 1.10&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’re implementing support for the scale subresource, we recommend that you expose the autoscaling/v1.Scale object.&lt;/p&gt;

&lt;h2 id=&#34;type-safe-deepcopy&#34;&gt;Type-safe DeepCopy&lt;/h2&gt;

&lt;p&gt;Deeply copying an object formerly required a call to Scheme.Copy(Object) with the notable disadvantage of losing type safety. A typical piece of code from client-go version 5 required type casting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
newObj, err := runtime.NewScheme().Copy(node)


if err != nil {

    return fmt.Errorf(&amp;quot;failed to copy node %v: %s”, node, err)

}


newNode, ok := newObj.(\*v1.Node)

if !ok {

    return fmt.Errorf(&amp;quot;failed to type-assert node %v&amp;quot;, newObj)


}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/kubernetes/code-generator&#34; target=&#34;_blank&#34;&gt;k8s.io/code-generator&lt;/a&gt;, Copy has now been replaced by a type-safe DeepCopy method living on each object, allowing you to simplify code significantly both in terms of volume and API error surface:&lt;/p&gt;

&lt;p&gt;newNode := node.DeepCopy()&lt;/p&gt;

&lt;p&gt;No error handling is necessary: this call never fails. If and only if the node is nil does DeepCopy() return nil.&lt;/p&gt;

&lt;p&gt;To copy runtime.Objects there is an additional DeepCopyObject() method in the runtime.Object interface.&lt;/p&gt;

&lt;p&gt;With the old method gone for good, clients need to update their copy invocations accordingly.&lt;/p&gt;

&lt;h2 id=&#34;code-generation-and-customresources&#34;&gt;Code generation and CustomResources&lt;/h2&gt;

&lt;p&gt;Using client-go’s dynamic client to access CustomResources is discouraged and superseded by type-safe code using the generators in &lt;a href=&#34;https://github.com/kubernetes/code-generator&#34; target=&#34;_blank&#34;&gt;k8s.io/code-generator&lt;/a&gt;. Check out the &lt;a href=&#34;https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/&#34; target=&#34;_blank&#34;&gt;Deep Dive on the Open Shift blog&lt;/a&gt; to learn about using code generation with client-go.&lt;/p&gt;

&lt;h3 id=&#34;comment-blocks&#34;&gt;Comment Blocks&lt;/h3&gt;

&lt;p&gt;You can now place tags in the comment block just above a type or function, or in the second block above. There is no distinction anymore between these two comment blocks. This used to a be a source of &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/53893&#34; target=&#34;_blank&#34;&gt;subtle errors when using the generators&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// second block above  
// +k8s:some-tag  

// first block above  
// +k8s:another-tag  
type Foo struct {}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;custom-client-methods&#34;&gt;Custom Client Methods&lt;/h3&gt;

&lt;p&gt;You can now use extended tag definitions to create custom verbs . This lets you expand beyond the verbs defined by HTTP. This opens the door to higher levels of customization.&lt;/p&gt;

&lt;p&gt;For example, this block leads to the generation of the method UpdateScale(s *autoscaling.Scale) (*autoscaling.Scale, error):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// genclient:method=UpdateScale,verb=update,subresource=scale,input=k8s.io/kubernetes/pkg/apis/autoscaling.Scale,result=k8s.io/kubernetes/pkg/apis/autoscaling.Scale
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolving-golang-naming-conflicts&#34;&gt;Resolving Golang Naming Conflicts&lt;/h3&gt;

&lt;p&gt;In more complex API groups it’s possible for Kinds, the group name, the Go package name, and the Go group alias name to conflict. This was not handled correctly prior to 1.9. The following tags resolve naming conflicts and make the generated code prettier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// +groupName=example2.example.com  
// +groupGoName=SecondExample
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are usually &lt;a href=&#34;https://github.com/kubernetes/code-generator/blob/release-1.9/_examples/crd/apis/example2/v1/doc.go#L18&#34; target=&#34;_blank&#34;&gt;in the doc.go file of an API package&lt;/a&gt;. The first is used as the CustomResource group name when RESTfully speaking to the API server using HTTP. The second is used in the generated Golang code (for example, in the clientset) to access the group version:&lt;/p&gt;

&lt;p&gt;clientset.SecondExampleV1()&lt;/p&gt;

&lt;p&gt;It’s finally possible to have dots in Go package names. In this section’s example, you would put the groupName snippet into the pkg/apis/example2.example.com directory of your project.&lt;/p&gt;

&lt;h2 id=&#34;example-projects&#34;&gt;Example projects&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.9 includes a number of example projects which can serve as a blueprint for your own projects:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/sample-apiserver&#34; target=&#34;_blank&#34;&gt;k8s.io/sample-apiserver&lt;/a&gt; is a simple user-provided API server that is integrated into a cluster via &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/&#34; target=&#34;_blank&#34;&gt;API aggregation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/sample-controller&#34; target=&#34;_blank&#34;&gt;k8s.io/sample-controller&lt;/a&gt; is a full-featured &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md&#34; target=&#34;_blank&#34;&gt;controller&lt;/a&gt; (also called an operator) with shared informers and a workqueue to process created, changed or deleted objects. It is based on CustomResourceDefinitions and uses &lt;a href=&#34;https://github.com/kubernetes/code-generator&#34; target=&#34;_blank&#34;&gt;k8s.io/code-generator&lt;/a&gt; to generate deepcopy functions, typed clientsets, informers, and listers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;vendoring&#34;&gt;Vendoring&lt;/h2&gt;

&lt;p&gt;In order to update from the previous version 5 to version 6 of client-go, the library itself as well as certain third-party dependencies must be updated. Previously, this process had been tedious due to the fact that a lot of code got refactored or relocated within the existing package layout across releases. Fortunately, far less code had to move in the latest version, which should ease the upgrade procedure for most users.&lt;/p&gt;

&lt;h3 id=&#34;state-of-the-published-repositories&#34;&gt;State of the published repositories&lt;/h3&gt;

&lt;p&gt;In the past &lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;k8s.io/client-go&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes/api&#34; target=&#34;_blank&#34;&gt;k8s.io/api&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kubernetes/apimachinery&#34; target=&#34;_blank&#34;&gt;k8s.io/apimachinery&lt;/a&gt; were updated infrequently. Tags (for example, v4.0.0) were created quite some time after the Kubernetes releases. With the 1.9 release we resumed running a nightly bot that updates all the repositories for public consumption, even before manual tagging. This includes the branches:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;master&lt;/li&gt;
&lt;li&gt;release-1.8 / release-5.0&lt;/li&gt;
&lt;li&gt;release-1.9 / release-6.0
Kubernetes tags (for example, v1.9.1-beta1) are also applied automatically to the published repositories, prefixed with kubernetes- (for example, kubernetes-1.9.1-beta1).&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These tags have limited test coverage, but can be used by early adopters of client-go and the other libraries. Moreover, they help to vendor the correct version of &lt;a href=&#34;https://github.com/kubernetes/api&#34; target=&#34;_blank&#34;&gt;k8s.io/api&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/apimachinery&#34; target=&#34;_blank&#34;&gt;k8s.io/apimachinery&lt;/a&gt;. Note that we only create a v6.0.3-like semantic versioning tag on &lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;k8s.io/client-go&lt;/a&gt;. The corresponding tag for k8s.io/api and k8s.io/apimachinery is kubernetes-1.9.3.&lt;/p&gt;

&lt;p&gt;Also note that only these tags correspond to tested releases of Kubernetes. If you depend on the release branch, e.g., release-1.9, your client is running on unreleased Kubernetes code.&lt;/p&gt;

&lt;h3 id=&#34;state-of-vendoring-of-client-go&#34;&gt;State of vendoring of client-go&lt;/h3&gt;

&lt;p&gt;In general, the list of which dependencies to vendor is automatically generated and written to the file Godeps/Godeps.json. Only the revisions listed there are tested. This means especially that we do not and cannot test the code-base against master branches of our dependencies. This puts us in the following situation depending on the used vendoring tool:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tools/godep&#34; target=&#34;_blank&#34;&gt;godep&lt;/a&gt; reads Godeps/Godeps.json by running godep restore from k8s.io/client-go in your GOPATH. Then use godep save to vendor in your project. godep will choose the correct versions from your GOPATH.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Masterminds/glide&#34; target=&#34;_blank&#34;&gt;glide&lt;/a&gt; reads Godeps/Godeps.json automatically from its dependencies including from k8s.io/client-go, both on init and on update. Hence, glide should be mostly automatic as long as there are no conflicts.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/golang/dep&#34; target=&#34;_blank&#34;&gt;dep&lt;/a&gt; does not currently respect Godeps/Godeps.json in a consistent way, especially not on updates. It is crucial to specify client-go dependencies manually as constraints or overrides, also for non k8s.io/* dependencies. Without those, dep simply chooses the dependency master branches, which can cause problems as they are updated frequently.&lt;/li&gt;
&lt;li&gt;The Kubernetes and golang/dep community are aware of the problems [&lt;a href=&#34;https://github.com/golang/dep/issues/1124&#34; target=&#34;_blank&#34;&gt;issue #1124&lt;/a&gt;, &lt;a href=&#34;https://github.com/golang/dep/issues/1236&#34; target=&#34;_blank&#34;&gt;issue #1236&lt;/a&gt;] and &lt;a href=&#34;https://github.com/kubernetes-dep-experiment/client-go&#34; target=&#34;_blank&#34;&gt;are working together on solutions&lt;/a&gt;. Until then special care must be taken.
Please see client-go’s &lt;a href=&#34;https://github.com/kubernetes/client-go/blob/master/INSTALL.md&#34; target=&#34;_blank&#34;&gt;INSTALL.md&lt;/a&gt; for more details.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;updating-dependencies-golang-dep&#34;&gt;Updating dependencies – golang/dep&lt;/h3&gt;

&lt;p&gt;Even with the deficiencies of golang/dep today, dep is slowly becoming the de-facto standard in the Go ecosystem. With the necessary care and the awareness of the missing features, dep can be (and is!) used successfully. Here’s a demonstration of how to update a project with client-go 5 to the latest version 6 using dep:&lt;/p&gt;

&lt;p&gt;(If you are still running client-go version 4 and want to play it safe by not skipping a release, now is a good time to check out &lt;a href=&#34;https://medium.com/@andy.goldstein/upgrading-kubernetes-client-go-from-v4-to-v5-bbd5025fe381&#34; target=&#34;_blank&#34;&gt;this excellent blog post&lt;/a&gt; describing how to upgrade to version 5, put together by our friends at Heptio.)&lt;/p&gt;

&lt;p&gt;Before starting, it is important to understand that client-go depends on two other Kubernetes projects: &lt;a href=&#34;https://github.com/kubernetes/apimachinery&#34; target=&#34;_blank&#34;&gt;k8s.io/apimachinery&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/api&#34; target=&#34;_blank&#34;&gt;k8s.io/api&lt;/a&gt;. In addition, if you are using CRDs, you probably also depend on &lt;a href=&#34;https://github.com/kubernetes/apiextensions-apiserver&#34; target=&#34;_blank&#34;&gt;k8s.io/apiextensions-apiserver&lt;/a&gt; for the CRD client. The first exposes lower-level API mechanics (such as schemes, serialization, and type conversion), the second holds API definitions, and the third provides APIs related to CustomResourceDefinitions. In order for client-go to operate correctly, it needs to have its companion libraries vendored in correspondingly matching versions. Each library repository provides a branch named release-&lt;em&gt;&amp;lt;version&amp;gt;&lt;/em&gt; where &lt;em&gt;&amp;lt;version&amp;gt;&lt;/em&gt; refers to a particular Kubernetes version; for client-go version 6, it is imperative to refer to the &lt;em&gt;release&lt;/em&gt;-1.9 branch on each repository.&lt;/p&gt;

&lt;p&gt;Assuming the latest version 5 patch release of client-go being vendored through dep, the Gopkg.toml manifest file should look something like this (possibly using branches instead of versions):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;




[[constraint]]


  name = &amp;quot;k8s.io/api&amp;quot;

  version = &amp;quot;kubernetes-1.8.1&amp;quot;


[[constraint]]

  name = &amp;quot;k8s.io/apimachinery&amp;quot;

  version = &amp;quot;kubernetes-1.8.1&amp;quot;


[[constraint]]

  name = &amp;quot;k8s.io/apiextensions-apiserver&amp;quot;

  version = &amp;quot;kubernetes-1.8.1&amp;quot;


[[constraint]]

  name = &amp;quot;k8s.io/client-go&amp;quot;




  version = &amp;quot;5.0.1&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that some of the libraries could be missing if they are not actually needed by the client.&lt;/p&gt;

&lt;p&gt;Upgrading to client-go version 6 means bumping the version and tag identifiers as following ( &lt;strong&gt;emphasis&lt;/strong&gt; given):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;




[constraint]]


  name = &amp;quot;k8s.io/api&amp;quot;

  version = &amp;quot;kubernetes-1.9.0&amp;quot;


[[constraint]]

  name = &amp;quot;k8s.io/apimachinery&amp;quot;

  version = &amp;quot;kubernetes-1.9.0&amp;quot;


[[constraint]]

  name = &amp;quot;k8s.io/apiextensions-apiserver&amp;quot;

  version = &amp;quot;kubernetes-1.9.0&amp;quot;


[[constraint]]

  name = &amp;quot;k8s.io/client-go&amp;quot;




  version = &amp;quot;6.0.0&amp;quot;



&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result of the upgrade can be found &lt;a href=&#34;https://github.com/ncdc/client-go-4-to-5/tree/v5-to-v6&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A note of caution: dep cannot capture the complete set of dependencies in a reliable and reproducible fashion as described above. This means that for a 100% future-proof project you have to add constraints (or even overrides) to many other packages listed in client-go’s Godeps/Godeps.json. Be prepared to add them if something breaks. We are working with the golang/dep community to make this an easier and more smooth experience.&lt;/p&gt;

&lt;p&gt;Finally, we need to tell dep to upgrade to the specified versions by executing dep ensure. If everything goes well, the output of the command invocation should be empty, with the only indication that it was successful being a number of updated files inside the vendor folder.&lt;/p&gt;

&lt;p&gt;If you are using CRDs, you probably also use code-generation. The following block for Gopkg.toml will add the required code-generation packages to your project:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
required = [  
  &amp;quot;k8s.io/code-generator/cmd/client-gen&amp;quot;,  
  &amp;quot;k8s.io/code-generator/cmd/conversion-gen&amp;quot;,  
  &amp;quot;k8s.io/code-generator/cmd/deepcopy-gen&amp;quot;,  
  &amp;quot;k8s.io/code-generator/cmd/defaulter-gen&amp;quot;,  
  &amp;quot;k8s.io/code-generator/cmd/informer-gen&amp;quot;,  
  &amp;quot;k8s.io/code-generator/cmd/lister-gen&amp;quot;,  
]


[[constraint]]

  branch = &amp;quot;kubernetes-1.9.0&amp;quot;


  name = &amp;quot;k8s.io/code-generator&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Whether you would also like to prune unneeded packages (such as test files) through dep or commit the changes into the VCS at this point is up to you &amp;ndash; but from an upgrade perspective, you should now be ready to harness all the fancy new features that Kubernetes 1.9 brings through client-go.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Extensible Admission is Beta</title>
      <link>https://kubernetes.io/blog/2018/01/extensible-admission-is-beta/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/01/extensible-admission-is-beta/</guid>
      <description>
        
        
        

&lt;p&gt;In this post we review a feature, available in the Kubernetes API server, that allows you to implement arbitrary control decisions and which has matured considerably in Kubernetes 1.9.&lt;/p&gt;

&lt;p&gt;The admission stage of API server processing is one of the most powerful tools for securing a Kubernetes cluster by restricting the objects that can be created, but it has always been limited to compiled code. In 1.9, we promoted webhooks for admission to beta, allowing you to leverage admission from outside the API server process.&lt;/p&gt;

&lt;h2 id=&#34;what-is-admission&#34;&gt;What is Admission?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/admission-controllers/#what-are-they&#34; target=&#34;_blank&#34;&gt;Admission&lt;/a&gt; is the phase of &lt;a href=&#34;https://blog.openshift.com/kubernetes-deep-dive-api-server-part-1/&#34; target=&#34;_blank&#34;&gt;handling an API server request&lt;/a&gt; that happens before a resource is persisted, but after authorization. Admission gets access to the same information as authorization (user, URL, etc) and the complete body of an API request (for most requests).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-p8WGg2BATsY/WlfywbD_tAI/AAAAAAAAAJw/mDqZV0dB4_Y0gXXQp_1tQ7CtMRSd6lHVwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-01-11%2Bat%2B3.22.07%2BPM.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://2.bp.blogspot.com/-p8WGg2BATsY/WlfywbD_tAI/AAAAAAAAAJw/mDqZV0dB4_Y0gXXQp_1tQ7CtMRSd6lHVwCK4BGAYYCw/s640/Screen%2BShot%2B2018-01-11%2Bat%2B3.22.07%2BPM.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The admission phase is composed of individual plugins, each of which are narrowly focused and have semantic knowledge of what they are inspecting. Examples include: PodNodeSelector (influences scheduling decisions), PodSecurityPolicy (prevents escalating containers), and ResourceQuota (enforces resource allocation per namespace).&lt;/p&gt;

&lt;p&gt;Admission is split into two phases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Mutation, which allows modification of the body content itself as well as rejection of an API request.&lt;/li&gt;
&lt;li&gt;Validation, which allows introspection queries and rejection of an API request.
An admission plugin can be in both phases, but all mutation happens before validation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;mutation&#34;&gt;Mutation&lt;/h3&gt;

&lt;p&gt;The mutation phase of admission allows modification of the resource content before it is persisted. Because the same field can be mutated multiple times while in the admission chain, the order of the admission plugins in the mutation matters.&lt;/p&gt;

&lt;p&gt;One example of a mutating admission plugin is the &lt;code&gt;PodNodeSelector&lt;/code&gt; plugin, which uses an annotation on a namespace &lt;code&gt;namespace.annotations[“scheduler.alpha.kubernetes.io/node-selector”]&lt;/code&gt; to find a label selector and add it to the &lt;code&gt;pod.spec.nodeselector&lt;/code&gt; field. This positively restricts which nodes the pods in a particular namespace can land on, as opposed to taints, which provide negative restriction (also with an admission plugin).&lt;/p&gt;

&lt;h3 id=&#34;validation&#34;&gt;Validation&lt;/h3&gt;

&lt;p&gt;The validation phase of admission allows the enforcement of invariants on particular API resources. The validation phase runs after all mutators finish to ensure that the resource isn’t going to change again.&lt;/p&gt;

&lt;p&gt;One example of a validation admission plugin is also the &lt;code&gt;PodNodeSelector&lt;/code&gt; plugin, which ensures that all pods’ &lt;code&gt;spec.nodeSelector&lt;/code&gt; fields are constrained by the node selector restrictions on the namespace. Even if a mutating admission plugin tries to change the &lt;code&gt;spec.nodeSelector&lt;/code&gt; field after the PodNodeSelector runs in the mutating chain, the PodNodeSelector in the validating chain prevents the API resource from being created because it fails validation.&lt;/p&gt;

&lt;h2 id=&#34;what-are-admission-webhooks&#34;&gt;What are admission webhooks?&lt;/h2&gt;

&lt;p&gt;Admission webhooks allow a Kubernetes installer or a cluster-admin to add mutating and validating admission plugins to the admission chain of &lt;code&gt;kube-apiserver&lt;/code&gt; as well as any extensions apiserver based on k8s.io/apiserver 1.9, like &lt;a href=&#34;https://github.com/kubernetes/metrics&#34; target=&#34;_blank&#34;&gt;metrics&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes-incubator/service-catalog&#34; target=&#34;_blank&#34;&gt;service-catalog&lt;/a&gt;, or &lt;a href=&#34;https://github.com/openshift/kube-projects&#34; target=&#34;_blank&#34;&gt;kube-projects&lt;/a&gt;, without recompiling them. Both kinds of admission webhooks run at the end of their respective chains and have the same powers and limitations as compiled admission plugins.&lt;/p&gt;

&lt;h3 id=&#34;what-are-they-good-for&#34;&gt;What are they good for?&lt;/h3&gt;

&lt;p&gt;Webhook admission plugins allow for mutation and validation of any resource on any API server, so the possible applications are vast. Some common use-cases include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Mutation of resources like pods. Istio has talked about doing this to inject side-car containers into pods. You could also write a plugin which forcefully resolves image tags into image SHAs.&lt;/li&gt;
&lt;li&gt;Name restrictions. On multi-tenant systems, reserving namespaces has emerged as a use-case.&lt;/li&gt;
&lt;li&gt;Complex CustomResource validation. Because the entire object is visible, a clever admission plugin can perform complex validation on dependent fields (A requires B) and even external resources (compare to LimitRanges).&lt;/li&gt;
&lt;li&gt;Security response. If you forced image tags into image SHAs, you could write an admission plugin that prevents certain SHAs from running.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;registration&#34;&gt;Registration&lt;/h3&gt;

&lt;p&gt;Webhook admission plugins of both types are registered in the API, and all API servers (kube-apiserver and all extension API servers) share a common config for them. During the registration process, a webhook admission plugin describes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How to connect to the webhook admission server&lt;/li&gt;
&lt;li&gt;How to verify the webhook admission server (Is it really the server I expect?)&lt;/li&gt;
&lt;li&gt;Where to send the data at that server (which URL path)&lt;/li&gt;
&lt;li&gt;Which resources and which HTTP verbs it will handle&lt;/li&gt;
&lt;li&gt;What an API server should do on connection failures (for example, if the admission webhook server goes down)
&lt;code&gt;
1 apiVersion: admissionregistration.k8s.io/v1beta1  
2 kind: ValidatingWebhookConfiguration  
3 metadata:  
4   name: namespacereservations.admission.online.openshift.io  
5 webhooks:  
6 - name: namespacereservations.admission.online.openshift.io  
7   clientConfig:  
8     service:  
9       namespace: default  
10      name: kubernetes  
11     path: /apis/admission.online.openshift.io/v1alpha1/namespacereservations  
12    caBundle: KUBE\_CA\_HERE  
13  rules:  
14  - operations:  
15    - CREATE  
16    apiGroups:  
17    - &amp;quot;&amp;quot;  
18    apiVersions:  
19    - &amp;quot;\*&amp;quot;  
20    resources:  
21    - namespaces  
22  failurePolicy: Fail
&lt;/code&gt;
Line 6: &lt;code&gt;name&lt;/code&gt; - the name for the webhook itself. For mutating webhooks, these are sorted to provide ordering.&lt;br /&gt;
Line 7: &lt;code&gt;clientConfig&lt;/code&gt; - provides information about how to connect to, trust, and send data to the webhook admission server.&lt;br /&gt;
Line 13: &lt;code&gt;rules&lt;/code&gt; - describe when an API server should call this admission plugin. In this case, only for creates of namespaces. You can specify any resource here so specifying creates of &lt;code&gt;serviceinstances.servicecatalog.k8s.io&lt;/code&gt; is also legal.&lt;br /&gt;
Line 22: &lt;code&gt;failurePolicy&lt;/code&gt; - says what to do if the webhook admission server is unavailable. Choices are “Ignore” (fail open) or “Fail” (fail closed). Failing open makes for unpredictable behavior for all clients.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;authentication-and-trust&#34;&gt;Authentication and trust&lt;/h3&gt;

&lt;p&gt;Because webhook admission plugins have a lot of power (remember, they get to see the API resource content of any request sent to them and might modify them for mutating plugins), it is important to consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How individual API servers verify their connection to the webhook admission server&lt;/li&gt;
&lt;li&gt;How the webhook admission server authenticates precisely which API server is contacting it&lt;/li&gt;
&lt;li&gt;Whether that particular API server has authorization to make the request
There are three major categories of connection:&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;From kube-apiserver or extension-apiservers to externally hosted admission webhooks (webhooks not hosted in the cluster)&lt;/li&gt;
&lt;li&gt;From kube-apiserver to self-hosted admission webhooks&lt;/li&gt;
&lt;li&gt;From extension-apiservers to self-hosted admission webhooks
To support these categories, the webhook admission plugins accept a kubeconfig file which describes how to connect to individual servers. For interacting with externally hosted admission webhooks, there is really no alternative to configuring that file manually since the authentication/authorization and access paths are owned by the server you’re hooking to.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the self-hosted category, a cleverly built webhook admission server and topology can take advantage of the safe defaulting built into the admission plugin and have a secure, portable, zero-config topology that works from any API server.&lt;/p&gt;

&lt;h3 id=&#34;simple-secure-portable-zero-config-topology&#34;&gt;Simple, secure, portable, zero-config topology&lt;/h3&gt;

&lt;p&gt;If you build your webhook admission server to also be an extension API server, it becomes possible to aggregate it as a normal API server. This has a number of advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Your webhook becomes available like any other API under default kube-apiserver service &lt;code&gt;kubernetes.default.svc&lt;/code&gt; (e.g. &lt;a href=&#34;https://kuberentes.default.svc/apis/admission.example.com/v1/mymutatingadmissionreviews&#34; target=&#34;_blank&#34;&gt;https://kubernetes.default.svc/apis/admission.example.com/v1/mymutatingadmissionreviews&lt;/a&gt;). Among other benefits, you can test using &lt;code&gt;kubectl&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Your webhook automatically (without any config) makes use of the in-cluster authentication and authorization provided by kube-apiserver. You can restrict access to your webhook with normal RBAC rules.&lt;/li&gt;
&lt;li&gt;Your extension API servers and kube-apiserver automatically (without any config) make use of their in-cluster credentials to communicate with the webhook.&lt;/li&gt;
&lt;li&gt;Extension API servers do not leak their service account token to your webhook because they go through kube-apiserver, which is a secure front proxy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/FeXoJLmbhf5exSBQu6Wxd2sIEqzkKPbRA_iv6T2QmJbhRsO4FyPtgAAbHdAmuTrE0jVEUzftfxcPndN8ACzstfsX9XTFdQFrioS1srvYgVP3l99R6x-vvd3RfBA4eWttaKRWj6iA&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
_Source: &lt;a href=&#34;https://drive.google.com/a/redhat.com/file/d/12nC9S2fWCbeX_P8nrmL6NgOSIha4HDNp&#34; target=&#34;_blank&#34;&gt;https://drive.google.com/a/redhat.com/file/d/12nC9S2fWCbeX_P8nrmL6NgOSIha4HDNp&lt;/a&gt;_&lt;/p&gt;

&lt;p&gt;In short: a secure topology makes use of all security mechanisms of API server aggregation and additionally requires no additional configuration.&lt;/p&gt;

&lt;p&gt;Other topologies are possible but require additional manual configuration as well as a lot of effort to create a secure setup, especially when extension API servers like service catalog come into play. The topology above is zero-config and portable to every Kubernetes cluster.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-write-a-webhook-admission-server&#34;&gt;How do I write a webhook admission server?&lt;/h3&gt;

&lt;p&gt;Writing a full server complete with authentication and authorization can be intimidating. To make it easier, there are projects based on Kubernetes 1.9 that provide a library for building your webhook admission server in 200 lines or less. Take a look at the &lt;a href=&#34;https://github.com/openshift/generic-admission-server&#34; target=&#34;_blank&#34;&gt;generic-admission-apiserver&lt;/a&gt; and the &lt;a href=&#34;https://github.com/openshift/kubernetes-namespace-reservation&#34; target=&#34;_blank&#34;&gt;kubernetes-namespace-reservation&lt;/a&gt; projects for the library and an example of how to build your own secure and portable webhook admission server.&lt;/p&gt;

&lt;p&gt;With the admission webhooks introduced in 1.9 we’ve made Kubernetes even more adaptable to your needs. We hope this work, driven by both Red Hat and Google, will enable many more workloads and support ecosystem components. (Istio is one example.) Now is a good time to give it a try!&lt;/p&gt;

&lt;p&gt;If you’re interested in giving feedback or contributing to this area, join us in the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-api-machinery&#34; target=&#34;_blank&#34;&gt;SIG API machinery&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Introducing Container Storage Interface (CSI) Alpha for Kubernetes </title>
      <link>https://kubernetes.io/blog/2018/01/introducing-container-storage-interface/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/01/introducing-container-storage-interface/</guid>
      <description>
        
        
        

&lt;p&gt;One of the key differentiators for Kubernetes has been a powerful &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/&#34; target=&#34;_blank&#34;&gt;volume plugin system&lt;/a&gt; that enables many different types of storage systems to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Automatically create storage when required.&lt;/li&gt;
&lt;li&gt;Make storage available to containers wherever they’re scheduled.&lt;/li&gt;
&lt;li&gt;Automatically delete the storage when no longer needed.
Adding support for new storage systems to Kubernetes, however, has been challenging.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Kubernetes 1.9 introduces an &lt;a href=&#34;https://github.com/kubernetes/features/issues/178&#34; target=&#34;_blank&#34;&gt;alpha implementation of the Container Storage Interface (CSI)&lt;/a&gt; which makes installing new volume plugins as easy as deploying a pod. It also enables third-party storage providers to develop solutions without the need to add to the core Kubernetes codebase.&lt;/p&gt;

&lt;p&gt;Because the feature is alpha in 1.9, it must be explicitly enabled. Alpha features are not recommended for production usage, but are a good indication of the direction the project is headed (in this case, towards a more extensible and standards based Kubernetes storage ecosystem).&lt;/p&gt;

&lt;h3 id=&#34;why-kubernetes-csi&#34;&gt;Why Kubernetes CSI?&lt;/h3&gt;

&lt;p&gt;Kubernetes volume plugins are currently “in-tree”, meaning they’re linked, compiled, built, and shipped with the core kubernetes binaries. Adding support for a new storage system to Kubernetes (a volume plugin) requires checking code into the core Kubernetes repository. But aligning with the Kubernetes release process is painful for many plugin developers.&lt;/p&gt;

&lt;p&gt;The existing &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md&#34; target=&#34;_blank&#34;&gt;Flex Volume plugin&lt;/a&gt; attempted to address this pain by exposing an exec based API for external volume plugins. Although it enables third party storage vendors to write drivers out-of-tree, in order to deploy the third party driver files it requires access to the root filesystem of node and master machines.&lt;/p&gt;

&lt;p&gt;In addition to being difficult to deploy, Flex did not address the pain of plugin dependencies: Volume plugins tend to have many external requirements (on mount and filesystem tools, for example). These dependencies are assumed to be available on the underlying host OS which is often not the case (and installing them requires access to the root filesystem of node machine).&lt;/p&gt;

&lt;p&gt;CSI addresses all of these issues by enabling storage plugins to be developed out-of-tree, containerized, deployed via standard Kubernetes primitives, and consumed through the Kubernetes storage primitives users know and love (PersistentVolumeClaims, PersistentVolumes, StorageClasses).&lt;/p&gt;

&lt;h3 id=&#34;what-is-csi&#34;&gt;What is CSI?&lt;/h3&gt;

&lt;p&gt;The goal of CSI is to establish a standardized mechanism for Container Orchestration Systems (COs) to expose arbitrary storage systems to their containerized workloads. The CSI specification emerged from cooperation between community members from various Container Orchestration Systems (COs)&amp;ndash;including Kubernetes, Mesos, Docker, and Cloud Foundry. The specification is developed, independent of Kubernetes, and maintained at &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;https://github.com/container-storage-interface/spec/blob/master/spec.md&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Kubernetes v1.9 exposes an alpha implementation of the CSI specification enabling CSI compatible volume drivers to be deployed on Kubernetes and consumed by Kubernetes workloads.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-deploy-a-csi-driver-on-a-kubernetes-cluster&#34;&gt;How do I deploy a CSI driver on a Kubernetes Cluster?&lt;/h3&gt;

&lt;p&gt;CSI plugin authors will provide their own instructions for deploying their plugin on Kubernetes.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-use-a-csi-volume&#34;&gt;How do I use a CSI Volume?&lt;/h3&gt;

&lt;p&gt;Assuming a CSI storage plugin is already deployed on your cluster, you can use it through the familiar Kubernetes storage primitives: PersistentVolumeClaims, PersistentVolumes, and StorageClasses.&lt;/p&gt;

&lt;p&gt;CSI is an alpha feature in Kubernetes v1.9. To enable it, set the following flags:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CSI is an alpha feature in Kubernetes v1.9. To enable it, set the following flags:

API server binary:
--feature-gates=CSIPersistentVolume=true
--runtime-config=storage.k8s.io/v1alpha1=true
API server binary and kubelet binaries:
--feature-gates=MountPropagation=true
--allow-privileged=true
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dynamic-provisioning&#34;&gt;Dynamic Provisioning&lt;/h3&gt;

&lt;p&gt;You can enable automatic creation/deletion of volumes for CSI Storage plugins that support dynamic provisioning by creating a StorageClass pointing to the CSI plugin.&lt;/p&gt;

&lt;p&gt;The following StorageClass, for example, enables dynamic creation of “fast-storage” volumes by a CSI volume plugin called “com.example.team/csi-driver”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass

apiVersion: storage.k8s.io/v1

metadata:

  name: fast-storage

provisioner: com.example.team/csi-driver

parameters:

  type: pd-ssd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To trigger dynamic provisioning, create a PersistentVolumeClaim object. The following PersistentVolumeClaim, for example, triggers dynamic provisioning using the StorageClass above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1

kind: PersistentVolumeClaim

metadata:

  name: my-request-for-storage

spec:

  accessModes:

  - ReadWriteOnce

  resources:

    requests:

      storage: 5Gi

  storageClassName: fast-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When volume provisioning is invoked, the parameter “type: pd-ssd” is passed to the CSI plugin “com.example.team/csi-driver” via a “CreateVolume” call. In response, the external volume plugin provisions a new volume and then automatically create a PersistentVolume object to represent the new volume. Kubernetes then binds the new PersistentVolume object to the PersistentVolumeClaim, making it ready to use.&lt;/p&gt;

&lt;p&gt;If the “fast-storage” StorageClass is marked default, there is no need to include the storageClassName in the PersistentVolumeClaim, it will be used by default.&lt;/p&gt;

&lt;h3 id=&#34;pre-provisioned-volumes&#34;&gt;Pre-Provisioned Volumes&lt;/h3&gt;

&lt;p&gt;You can always expose a pre-existing volume in Kubernetes by manually creating a PersistentVolume object to represent the existing volume. The following PersistentVolume, for example, exposes a volume with the name “existingVolumeName” belonging to a CSI storage plugin called “com.example.team/csi-driver”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1

kind: PersistentVolume

metadata:

  name: my-manually-created-pv

spec:

  capacity:

    storage: 5Gi

  accessModes:

    - ReadWriteOnce

  persistentVolumeReclaimPolicy: Retain

  csi:

    driver: com.example.team/csi-driver

    volumeHandle: existingVolumeName

    readOnly: false
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;attaching-and-mounting&#34;&gt;Attaching and Mounting&lt;/h3&gt;

&lt;p&gt;You can reference a PersistentVolumeClaim that is bound to a CSI volume in any pod or pod template.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Pod

apiVersion: v1

metadata:

  name: my-pod

spec:

  containers:

    - name: my-frontend

      image: dockerfile/nginx

      volumeMounts:

      - mountPath: &amp;quot;/var/www/html&amp;quot;

        name: my-csi-volume

  volumes:

    - name: my-csi-volume

      persistentVolumeClaim:

        claimName: my-request-for-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the pod referencing a CSI volume is scheduled, Kubernetes will trigger the appropriate operations against the external CSI plugin (ControllerPublishVolume, NodePublishVolume, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.&lt;/p&gt;

&lt;p&gt;For more details please see the CSI implementation &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md&#34; target=&#34;_blank&#34;&gt;design doc&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes-csi/docs/wiki/Setup&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-create-a-csi-driver&#34;&gt;How do I create a CSI driver?&lt;/h3&gt;

&lt;p&gt;Kubernetes is as minimally prescriptive on the packaging and deployment of a CSI Volume Driver as possible. The minimum requirements for deploying a CSI Volume Driver on Kubernetes are documented &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#third-party-csi-volume-drivers&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The minimum requirements document also contains a &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#recommended-mechanism-for-deploying-csi-drivers-on-kubernetes&#34; target=&#34;_blank&#34;&gt;section&lt;/a&gt; outlining the suggested mechanism for deploying an arbitrary containerized CSI driver on Kubernetes. This mechanism can be used by a Storage Provider to simplify deployment of containerized CSI compatible volume drivers on Kubernetes.&lt;/p&gt;

&lt;p&gt;As part of this recommended deployment process, the Kubernetes team provides the following sidecar (helper) containers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher&#34; target=&#34;_blank&#34;&gt;external-attacher&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sidecar container that watches Kubernetes VolumeAttachment objects and triggers ControllerPublish and ControllerUnpublish operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;external-provisioner&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sidecar container that watches Kubernetes PersistentVolumeClaim objects and triggers CreateVolume and DeleteVolume operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/driver-registrar&#34; target=&#34;_blank&#34;&gt;driver-registrar&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sidecar container that registers the CSI driver with kubelet (in the future), and adds the drivers custom NodeId (retrieved via GetNodeID call against the CSI endpoint) to an annotation on the Kubernetes Node API Object&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Storage vendors can build Kubernetes deployments for their plugins using these components, while leaving their CSI driver completely unaware of Kubernetes.&lt;/p&gt;

&lt;h3 id=&#34;where-can-i-find-csi-drivers&#34;&gt;Where can I find CSI drivers?&lt;/h3&gt;

&lt;p&gt;CSI drivers are developed and maintained by third-parties. You can find example CSI drivers &lt;a href=&#34;https://github.com/kubernetes-csi/drivers&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, but these are provided purely for illustrative purposes, and are not intended to be used for production workloads.&lt;/p&gt;

&lt;h3 id=&#34;what-about-flex&#34;&gt;What about Flex?&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md&#34; target=&#34;_blank&#34;&gt;Flex Volume plugin&lt;/a&gt; exists as an exec based mechanism to create “out-of-tree” volume plugins. Although it has some drawbacks (mentioned above), the Flex volume plugin coexists with the new CSI Volume plugin. SIG Storage will continue to maintain the Flex API so that existing third-party Flex drivers (already deployed in production clusters) continue to work. In the future, new volume features will only be added to CSI, not Flex.&lt;/p&gt;

&lt;h3 id=&#34;what-will-happen-to-the-in-tree-volume-plugins&#34;&gt;What will happen to the in-tree volume plugins?&lt;/h3&gt;

&lt;p&gt;Once CSI reaches stability, we plan to migrate most of the in-tree volume plugins to CSI. Stay tuned for more details as the Kubernetes CSI implementation approaches stable.&lt;/p&gt;

&lt;h3 id=&#34;what-are-the-limitations-of-alpha&#34;&gt;What are the limitations of alpha?&lt;/h3&gt;

&lt;p&gt;The alpha implementation of CSI has the following limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The credential fields in CreateVolume, NodePublishVolume, and ControllerPublishVolume calls are not supported.&lt;/li&gt;
&lt;li&gt;Block volumes are not supported; only file.&lt;/li&gt;
&lt;li&gt;Specifying filesystems is not supported, and defaults to ext4.&lt;/li&gt;
&lt;li&gt;CSI drivers must be deployed with the provided “external-attacher,” even if they don’t implement “ControllerPublishVolume”.&lt;/li&gt;
&lt;li&gt;Kubernetes scheduler topology awareness is not supported for CSI volumes: in short, sharing information about where a volume is provisioned (zone, regions, etc.) to allow k8s scheduler to make smarter scheduling decisions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h3&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI implementation to beta in either 1.10 or 1.11.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-get-involved&#34;&gt;How Do I Get Involved?&lt;/h3&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. A huge thank you to Vladimir Vivien (&lt;a href=&#34;https://github.com/vladimirvivien&#34; target=&#34;_blank&#34;&gt;vladimirvivien&lt;/a&gt;), Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;), Chakravarthy Nelluri (&lt;a href=&#34;https://github.com/chakri-nelluri&#34; target=&#34;_blank&#34;&gt;chakri-nelluri&lt;/a&gt;), Bradley Childs (&lt;a href=&#34;https://github.com/childsb&#34; target=&#34;_blank&#34;&gt;childsb&lt;/a&gt;), Luis Pabón (&lt;a href=&#34;https://github.com/lpabon&#34; target=&#34;_blank&#34;&gt;lpabon&lt;/a&gt;), and Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;saad-ali&lt;/a&gt;) for their tireless efforts in bringing CSI to life in Kubernetes.&lt;/p&gt;

&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special-Interest-Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes v1.9 releases beta support for Windows Server Containers</title>
      <link>https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/</link>
      <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/</guid>
      <description>
        
        
        

&lt;p&gt;With the release of Kubernetes v1.9, our mission of ensuring Kubernetes works well everywhere and for everyone takes a great step forward. We’ve advanced support for Windows Server to beta along with continued feature and functional advancements on both the Kubernetes and Windows platforms. SIG-Windows has been working since March of 2016 to open the door for many Windows-specific applications and workloads to run on Kubernetes, significantly expanding the implementation scenarios and the enterprise reach of Kubernetes.&lt;/p&gt;

&lt;p&gt;Enterprises of all sizes have made significant investments in .NET and Windows based applications. Many enterprise portfolios today contain .NET and Windows, with Gartner claiming that &lt;a href=&#34;http://www.gartner.com/document/3446217&#34; target=&#34;_blank&#34;&gt;80%&lt;/a&gt; of enterprise apps run on Windows. According to StackOverflow Insights, 40% of professional developers use the .NET programming languages (including .NET Core).&lt;/p&gt;

&lt;p&gt;But why is all this information important? It means that enterprises have both legacy and new born-in-the-cloud (microservice) applications that utilize a wide array of programming frameworks. There is a big push in the industry to modernize existing/legacy applications to containers, using an approach similar to “lift and shift”. Modernizing existing applications into containers also provides added flexibility for new functionality to be introduced in additional Windows or Linux containers. Containers are becoming the de facto standard for packaging, deploying, and managing both existing and microservice applications. IT organizations are looking for an easier and homogenous way to orchestrate and manage containers across their Linux and Windows environments. Kubernetes v1.9 now offers beta support for Windows Server containers, making it the clear choice for orchestrating containers of any kind.&lt;/p&gt;

&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;

&lt;p&gt;Alpha support for Windows Server containers in Kubernetes was great for proof-of-concept projects and visualizing the road map for support of Windows in Kubernetes. The alpha release had significant drawbacks, however, and lacked many features, especially in networking. SIG-Windows, Microsoft, Cloudbase Solutions, Apprenda, and other community members banded together to create a comprehensive beta release, enabling Kubernetes users to start evaluating and using Windows.&lt;/p&gt;

&lt;p&gt;Some key feature improvements for Windows Server containers on Kubernetes include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Improved support for pods! Multiple Windows Server containers in a pod can now share the network namespace using network compartments in Windows Server. This feature brings the concept of a pod to parity with Linux-based containers&lt;/li&gt;
&lt;li&gt;Reduced network complexity by using a single network endpoint per pod&lt;/li&gt;
&lt;li&gt;Kernel-Based load-balancing using the Virtual Filtering Platform (VFP) Hyper-v Switch Extension (analogous to Linux iptables)&lt;/li&gt;
&lt;li&gt;Container Runtime Interface (CRI) pod and node level statistics. Windows Server containers can now be profiled for Horizontal Pod Autoscaling using performance metrics gathered from the pod and the node&lt;/li&gt;
&lt;li&gt;Support for kubeadm commands to add Windows Server nodes to a Kubernetes environment. Kubeadm simplifies the provisioning of a Kubernetes cluster, and with the support for Windows Server, you can use a single tool to deploy Kubernetes in your infrastructure&lt;/li&gt;
&lt;li&gt;Support for ConfigMaps, Secrets, and Volumes. These are key features that allow you to separate, and in some cases secure, the configuration of the containers from the implementation
The crown jewels of Kubernetes 1.9 Windows support, however, are the networking enhancements. With the release of Windows Server 1709, Microsoft has enabled key networking capabilities in the operating system and the Windows Host Networking Service (HNS) that paved the way to produce a number of CNI plugins that work with Windows Server containers in Kubernetes. The Layer-3 routed and network overlay plugins that are supported with Kubernetes 1.9 are listed below:&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;Upstream L3 Routing - IP routes configured in upstream ToR&lt;/li&gt;
&lt;li&gt;Host-Gateway - IP routes configured on each host&lt;/li&gt;
&lt;li&gt;Open vSwitch (OVS) &amp;amp; Open Virtual Network (OVN) with Overlay - Supports STT and Geneve tunneling types
You can read more about each of their &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/windows/&#34; target=&#34;_blank&#34;&gt;configuration, setup, and runtime capabilities&lt;/a&gt; to make an informed selection for your networking stack in Kubernetes.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Even though you have to continue running the Kubernetes Control Plane and Master Components in Linux, you are now able to introduce Windows Server as a Node in Kubernetes. As a community, this is a huge milestone and achievement. We will now start seeing .NET, .NET Core, ASP.NET, IIS, Windows Services, Windows executables and many more windows-based applications in Kubernetes.&lt;/p&gt;

&lt;h3 id=&#34;what-s-coming-next&#34;&gt;What’s coming next&lt;/h3&gt;

&lt;p&gt;A lot of work went into this beta release, but the community realizes there are more areas of investment needed before we can release Windows support as GA (General Availability) for production workloads. Some keys areas of focus for the first two quarters of 2018 include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Continue to make progress in the area of networking. Additional CNI plugins are under development and nearing completion&lt;/li&gt;
&lt;li&gt;Overlay - win-overlay (vxlan or IP-in-IP encapsulation using Flannel)&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Win-l2bridge (host-gateway)&amp;nbsp;&lt;/li&gt;
&lt;li&gt;OVN using cloud networking - without overlays&lt;/li&gt;
&lt;li&gt;Support for Kubernetes network policies in ovn-kubernetes&lt;/li&gt;
&lt;li&gt;Support for Hyper-V Isolation&lt;/li&gt;
&lt;li&gt;Support for StatefulSet functionality for stateful applications&lt;/li&gt;
&lt;li&gt;Produce installation artifacts and documentation that work on any infrastructure and across many public cloud providers like Microsoft Azure, Google Cloud, and Amazon AWS&lt;/li&gt;
&lt;li&gt;Continuous Integration/Continuous Delivery (CI/CD) infrastructure for SIG-Windows&lt;/li&gt;
&lt;li&gt;Scalability and Performance testing
Even though we have not committed to a timeline for GA, SIG-Windows estimates a GA release in the first half of 2018.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h3&gt;

&lt;p&gt;As we continue to make progress towards General Availability of this feature in Kubernetes, we welcome you to get involved, contribute code, provide feedback, deploy Windows Server containers to your Kubernetes cluster, or simply join our community.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you want to get started on deploying Windows Server containers in Kubernetes, read our getting started guide at &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/windows/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/getting-started-guides/windows/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;We meet every other Tuesday at 12:30 Eastern Standard Time (EST) at &lt;a href=&#34;https://zoom.us/my/sigwindows&#34; target=&#34;_blank&#34;&gt;https://zoom.us/my/sigwindows&lt;/a&gt;. All our meetings are recorded on youtube and referenced at &lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4&#34; target=&#34;_blank&#34;&gt;https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chat with us on Slack at &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-windows&#34; target=&#34;_blank&#34;&gt;https://kubernetes.slack.com/messages/sig-windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Find us on GitHub at &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-windows&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/community/tree/master/sig-windows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you,&lt;/p&gt;

&lt;p&gt;Michael Michael (@michmike77)&lt;br /&gt;
SIG-Windows Lead&lt;br /&gt;
Senior Director of Product Management, Apprenda&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Five Days of Kubernetes 1.9 </title>
      <link>https://kubernetes.io/blog/2018/01/five-days-of-kubernetes-19/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2018/01/five-days-of-kubernetes-19/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 1.9 is live, made possible by hundreds of contributors pushing thousands of commits in this latest releases.&lt;/p&gt;

&lt;p&gt;The community has tallied around 32,300 commits in the main repo and continues rapid growth outside of the main repo, which signals growing maturity and stability for the project. The community has logged more than 90,700 commits across all repos and 7,800 commits across all repos for v1.8.0 to v1.9.0 alone.&lt;/p&gt;

&lt;p&gt;With the help of our growing community of 1,400 plus contributors, we issued more than 4,490 PRs and pushed more than 7,800 commits to deliver Kubernetes 1.9 with many notable updates, including enhancements for the workloads and stateful application support areas. This all points to increased extensibility and standards-based Kubernetes ecosystem.&lt;/p&gt;

&lt;p&gt;While many improvements have been contributed, we highlight key features in this series of in-depth posts listed below. &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;Follow along&lt;/a&gt; and see what’s new and improved with workloads, Windows support and more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Day 1:&lt;/strong&gt; 5 Days of Kubernetes 1.9&lt;br /&gt;
&lt;strong&gt;Day 2:&lt;/strong&gt; Windows and Docker support for Kubernetes (beta)&lt;br /&gt;
&lt;strong&gt;Day 3:&lt;/strong&gt; Storage, CSI framework (alpha)&lt;br /&gt;
&lt;strong&gt;Day 4:&lt;/strong&gt; &amp;nbsp;Web Hook and Mission Critical, Dynamic Admission Control&lt;br /&gt;
&lt;strong&gt;Day 5:&lt;/strong&gt; Introducing client-go version 6&lt;br /&gt;
&lt;strong&gt;Day 6:&lt;/strong&gt; Workloads API&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Connect&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Connect with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Get involved with the Kubernetes project on &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Introducing Kubeflow - A Composable, Portable, Scalable ML Stack Built for Kubernetes </title>
      <link>https://kubernetes.io/blog/2017/12/introducing-kubeflow-composable/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/12/introducing-kubeflow-composable/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Today’s post is by David Aronchick and Jeremy Lewi, a PM and Engineer on the Kubeflow project, a new open source Github repo dedicated to making using machine learning (ML) stacks on Kubernetes easy, fast and extensible.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-and-machine-learning&#34;&gt;Kubernetes and Machine Learning&lt;/h2&gt;

&lt;p&gt;Kubernetes has quickly become the hybrid solution for deploying complicated workloads anywhere. While it started with just stateless services, customers have begun to move complex workloads to the platform, taking advantage of rich APIs, reliability and performance provided by Kubernetes. One of the fastest growing use cases is to use Kubernetes as the deployment platform of choice for machine learning.&lt;/p&gt;

&lt;p&gt;Building any production-ready machine learning system involves various components, often mixing vendors and hand-rolled solutions. Connecting and managing these services for even moderately sophisticated setups introduces huge barriers of complexity in adopting machine learning. Infrastructure engineers will often spend a significant amount of time manually tweaking deployments and hand rolling solutions before a single model can be tested.&lt;/p&gt;

&lt;p&gt;Worse, these deployments are so tied to the clusters they have been deployed to that these stacks are immobile, meaning that moving a model from a laptop to a highly scalable cloud cluster is effectively impossible without significant re-architecture. All these differences add up to wasted effort and create opportunities to introduce bugs at each transition.&lt;/p&gt;

&lt;h2 id=&#34;introducing-kubeflow&#34;&gt;Introducing Kubeflow&lt;/h2&gt;

&lt;p&gt;To address these concerns, we’re announcing the creation of the Kubeflow project, a new open source Github repo dedicated to making using ML stacks on Kubernetes easy, fast and extensible. This repository contains:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;JupyterHub to create &amp;amp; manage interactive Jupyter notebooks&lt;/li&gt;
&lt;li&gt;A Tensorflow &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/&#34; target=&#34;_blank&#34;&gt;Custom Resource&lt;/a&gt; (CRD) that can be configured to use CPUs or GPUs, and adjusted to the size of a cluster with a single setting&lt;/li&gt;
&lt;li&gt;A TF Serving container
Because this solution relies on Kubernetes, it runs wherever Kubernetes runs. Just spin up a cluster and go!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;using-kubeflow&#34;&gt;Using Kubeflow&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s suppose you are working with two different Kubernetes clusters: a local &lt;a href=&#34;https://github.com/kubernetes/minikube&#34; target=&#34;_blank&#34;&gt;minikube&lt;/a&gt; cluster; and a &lt;a href=&#34;https://docs.google.com/forms/d/1JNnoUe1_3xZvAogAi16DwH6AjF2eu08ggED24OGO7Xc/viewform?edit_requested=true&#34; target=&#34;_blank&#34;&gt;GKE cluster with GPUs&lt;/a&gt;; and that you have two &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#define-clusters-users-and-contexts&#34; target=&#34;_blank&#34;&gt;kubectl contexts&lt;/a&gt; defined named minikube and gke.&lt;/p&gt;

&lt;p&gt;First we need to initialize our &lt;a href=&#34;https://github.com/ksonnet&#34; target=&#34;_blank&#34;&gt;ksonnet&lt;/a&gt; application and install the Kubeflow packages. (To use ksonnet, you must first install it on your operating system - the instructions for doing so are &lt;a href=&#34;https://github.com/ksonnet/ksonnet&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     ks init my-kubeflow  
     cd my-kubeflow  
     ks registry add kubeflow \  
     github.com/google/kubeflow/tree/master/kubeflow  
     ks pkg install kubeflow/core  
     ks pkg install kubeflow/tf-serving  
     ks pkg install kubeflow/tf-job  
     ks generate core kubeflow-core --name=kubeflow-core
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now define &lt;a href=&#34;https://ksonnet.io/docs/concepts#environment&#34; target=&#34;_blank&#34;&gt;environments&lt;/a&gt; corresponding to our two clusters.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     kubectl config use-context minikube  
     ks env add minikube  

     kubectl config use-context gke  
     ks env add gke  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we’re done! Now just create the environments on your cluster. First, on minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     ks apply minikube -c kubeflow-core  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And to create it on our multi-node GKE cluster for quicker training:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     ks apply gke -c kubeflow-core  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By making it easy to deploy the same rich ML stack everywhere, the drift and rewriting between these environments is kept to a minimum.&lt;/p&gt;

&lt;p&gt;To access either deployments, you can execute the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     kubectl port-forward tf-hub-0 8100:8000  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then open up &lt;a href=&#34;http://127.0.0.1:8100&#34; target=&#34;_blank&#34;&gt;http://127.0.0.1:8100&lt;/a&gt; to access JupyterHub. To change the environment used by kubectl, use either of these commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     # To access minikube  
     kubectl config use-context minikube  

     # To access GKE  
     kubectl config use-context gke  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you execute apply you are launching on K8s&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;JupyterHub for launching and managing Jupyter notebooks on K8s&lt;/li&gt;
&lt;li&gt;A &lt;a href=&#34;https://github.com/tensorflow/k8s&#34; target=&#34;_blank&#34;&gt;TF CRD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s suppose you want to submit a training job. Kubeflow provides ksonnet &lt;a href=&#34;https://ksonnet.io/docs/concepts#prototype&#34; target=&#34;_blank&#34;&gt;prototypes&lt;/a&gt; that make it easy to define &lt;a href=&#34;https://ksonnet.io/docs/concepts#component&#34; target=&#34;_blank&#34;&gt;components&lt;/a&gt;. The tf-job prototype makes it easy to create a job for your code but for this example, we&amp;rsquo;ll use the tf-cnn prototype which runs &lt;a href=&#34;https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks&#34; target=&#34;_blank&#34;&gt;TensorFlow&amp;rsquo;s CNN benchmark&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To submit a training job, you first generate a new job from a prototype:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     ks generate tf-cnn cnn --name=cnn  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By default the tf-cnn prototype uses 1 worker and no GPUs which is perfect for our minikube cluster so we can just submit it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     ks apply minikube -c cnn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On GKE, we’ll want to tweak the prototype to take advantage of the multiple nodes and GPUs. First, let’s list all the parameters available:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     # To see a list of parameters  
     ks prototype list tf-job  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s adjust the parameters to take advantage of GPUs and access to multiple nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     ks param set --env=gke cnn num\_gpus 1  
     ks param set --env=gke cnn num\_workers 1  

     ks apply gke -c cnn  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note how we set those parameters so they are used only when you deploy to GKE. Your minikube parameters are unchanged!&lt;/p&gt;

&lt;p&gt;After training, you &lt;a href=&#34;https://www.tensorflow.org/serving/serving_basic&#34; target=&#34;_blank&#34;&gt;export your model&lt;/a&gt; to a serving location.&lt;/p&gt;

&lt;p&gt;Kubeflow also includes a serving package as well. In a separate example, we trained a standard Inception model, and stored the trained model in a bucket we’ve created called ‘gs://kubeflow-models’ with the path ‘/inception’.&lt;/p&gt;

&lt;p&gt;To deploy a the trained model for serving, execute the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     ks generate tf-serving inception --name=inception  
     ---namespace=default --model\_path=gs://kubeflow-models/inception  
     ks apply gke -c inception  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This highlights one more option in Kubeflow - the ability to pass in inputs based on your deployment. This command creates a tf-serving service on the GKE cluster, and makes it available to your application.&lt;/p&gt;

&lt;p&gt;For more information about of deploying and monitoring TensorFlow training jobs and TensorFlow models please refer to the &lt;a href=&#34;https://github.com/google/kubeflow/blob/master/user_guide.md&#34; target=&#34;_blank&#34;&gt;user guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;kubeflow-ksonnet&#34;&gt;Kubeflow + ksonnet&lt;/h2&gt;

&lt;p&gt;One choice we want to call out is the use of the ksonnet project. We think working with multiple environments (dev, test, prod) will be the norm for most Kubeflow users. By making environments a first class concept, ksonnet makes it easy for Kubeflow users to easily move their workloads between their different environments.&lt;/p&gt;

&lt;p&gt;Particularly now that &lt;a href=&#34;https://blog.heptio.com/ksonnet-intro-43f6183a97a6&#34; target=&#34;_blank&#34;&gt;Helm is integrating ksonnet&lt;/a&gt; with the next version of their platform, we felt like it was the perfect choice for us. More information about ksonnet can be found in the ksonnet &lt;a href=&#34;https://ksonnet.io/&#34; target=&#34;_blank&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also want to thank the team at &lt;a href=&#34;https://heptio.com/&#34; target=&#34;_blank&#34;&gt;Heptio&lt;/a&gt; for expediting features critical to Kubeflow&amp;rsquo;s use of ksonnet.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;We are in the midst of building out a community effort right now, and we would love your help! We’ve already been collaborating with many teams - &lt;a href=&#34;https://caicloud.io/article_detail/5a3b58fce928ca1c69e1aa70&#34; target=&#34;_blank&#34;&gt;CaiCloud&lt;/a&gt;, &lt;a href=&#34;https://blog.openshift.com/machine-learning-openshift-kubernetes/&#34; target=&#34;_blank&#34;&gt;Red Hat &amp;amp; OpenShift&lt;/a&gt;, &lt;a href=&#34;https://tutorials.ubuntu.com/tutorial/get-started-kubeflow&#34; target=&#34;_blank&#34;&gt;Canonical&lt;/a&gt;, &lt;a href=&#34;https://www.weave.works/blog/kubeflow-and-weave-cloud&#34; target=&#34;_blank&#34;&gt;Weaveworks&lt;/a&gt;, &lt;a href=&#34;http://container-solutions.com/tensorflow-on-kubernetes-kubeflow/&#34; target=&#34;_blank&#34;&gt;Container Solutions&lt;/a&gt; and many others. &lt;a href=&#34;https://coreos.com/&#34; target=&#34;_blank&#34;&gt;CoreOS&lt;/a&gt;, for example, is already seeing the promise of Kubeflow:&lt;/p&gt;

&lt;p&gt;“The Kubeflow project was a needed advancement to make it significantly easier to set up and productionize machine learning workloads on Kubernetes, and we anticipate that it will greatly expand the opportunity for even more enterprises to embrace the platform. We look forward to working with the project members in providing tight integration of Kubeflow with Tectonic, the enterprise Kubernetes platform.” &amp;ndash; Reza Shafii, VP of product, CoreOS&lt;/p&gt;

&lt;p&gt;If you’d like to try out Kubeflow right now right in your browser, we’ve partnered with &lt;a href=&#34;https://www.katacoda.com/&#34; target=&#34;_blank&#34;&gt;Katacoda&lt;/a&gt; to make it super easy. You can try it &lt;a href=&#34;https://www.katacoda.com/kubeflow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;And we’re just getting started! We would love for you to help. How you might ask? Well…&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Please join the&lt;a href=&#34;https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg&#34; target=&#34;_blank&#34;&gt;slack channel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Please join the&lt;a href=&#34;https://groups.google.com/forum/#!forum/kubeflow-discuss&#34; target=&#34;_blank&#34;&gt;kubeflow-discuss&lt;/a&gt; email list&lt;/li&gt;
&lt;li&gt;Please subscribe to the&lt;a href=&#34;http://twitter.com/kubeflow&#34; target=&#34;_blank&#34;&gt;Kubeflow twitter&lt;/a&gt; account&lt;/li&gt;
&lt;li&gt;Please download and run kubeflow, and submit bugs!
Thank you for your support so far, we could not be more excited!&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Jeremy Lewi &amp;amp; David Aronchick&lt;/em&gt;
Google&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 1.9: Apps Workloads GA and Expanded Ecosystem </title>
      <link>https://kubernetes.io/blog/2017/12/kubernetes-19-workloads-expanded-ecosystem/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/12/kubernetes-19-workloads-expanded-ecosystem/</guid>
      <description>
        
        
        

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.9, our fourth and final release this year.&lt;/p&gt;

&lt;p&gt;Today’s release continues the evolution of an increasingly rich feature set, more robust stability, and even greater community contributions. As the fourth release of the year, it gives us an opportunity to look back at the progress made in key areas. Particularly notable is the advancement of the Apps Workloads API to stable. This removes any reservations potential adopters might have had about the functional stability required to run mission-critical workloads. Another big milestone is the beta release of Windows support, which opens the door for many Windows-specific applications and workloads to run in Kubernetes, significantly expanding the implementation scenarios and enterprise readiness of Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;workloads-api-ga&#34;&gt;Workloads API GA&lt;/h2&gt;

&lt;p&gt;We’re excited to announce General Availability (GA) of the &lt;a href=&#34;https://kubernetes.io/docs/reference/workloads-18-19/&#34; target=&#34;_blank&#34;&gt;apps/v1 Workloads API&lt;/a&gt;, which is now enabled by default. The Apps Workloads API groups the DaemonSet, Deployment, ReplicaSet, and StatefulSet APIs together to form the foundation for long-running stateless and stateful workloads in Kubernetes. Note that the Batch Workloads API (Job and CronJob) is not part of this effort and will have a separate path to GA stability.&lt;/p&gt;

&lt;p&gt;Deployment and ReplicaSet, two of the most commonly used objects in Kubernetes, are now stabilized after more than a year of real-world use and feedback. &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-apps&#34; target=&#34;_blank&#34;&gt;SIG Apps&lt;/a&gt; has applied the lessons from this process to all four resource kinds over the last several release cycles, enabling DaemonSet and StatefulSet to join this graduation. The v1 (GA) designation indicates production hardening and readiness, and comes with the guarantee of long-term backwards compatibility.&lt;/p&gt;

&lt;h2 id=&#34;windows-support-beta&#34;&gt;Windows Support (beta)&lt;/h2&gt;

&lt;p&gt;Kubernetes was originally developed for Linux systems, but as our users are realizing the benefits of container orchestration at scale, we are seeing demand for Kubernetes to run Windows workloads. Work to support Windows Server in Kubernetes began in earnest about 12 months ago. &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-windows&#34; target=&#34;_blank&#34;&gt;SIG-Windows&lt;/a&gt;has now promoted this feature to beta status, which means that we can evaluate it for &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/windows/&#34; target=&#34;_blank&#34;&gt;usage&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;storage-enhancements&#34;&gt;Storage Enhancements&lt;/h2&gt;

&lt;p&gt;From the first release, Kubernetes has supported multiple options for persistent data storage, including commonly-used NFS or iSCSI, along with native support for storage solutions from the major public and private cloud providers. As the project and ecosystem grow, more and more storage options have become available for Kubernetes. Adding volume plugins for new storage systems, however, has been a challenge.&lt;/p&gt;

&lt;p&gt;Container Storage Interface (CSI) is a cross-industry standards initiative that aims to lower the barrier for cloud native storage development and ensure compatibility. &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;SIG-Storage&lt;/a&gt; and the &lt;a href=&#34;https://github.com/container-storage-interface/community&#34; target=&#34;_blank&#34;&gt;CSI Community&lt;/a&gt; are collaborating to deliver a single interface for provisioning, attaching, and mounting storage compatible with Kubernetes.&lt;/p&gt;

&lt;p&gt;Kubernetes 1.9 introduces an &lt;a href=&#34;https://github.com/kubernetes/features/issues/178&#34; target=&#34;_blank&#34;&gt;alpha implementation&lt;/a&gt; of the Container Storage Interface (CSI), which will make installing new volume plugins as easy as deploying a pod, and enable third-party storage providers to develop their solutions without the need to add to the core Kubernetes codebase.&lt;/p&gt;

&lt;p&gt;Because the feature is alpha in 1.9, it must be explicitly enabled and is not recommended for production usage, but it indicates the roadmap working toward a more extensible and standards-based Kubernetes storage ecosystem.&lt;/p&gt;

&lt;h2 id=&#34;additional-features&#34;&gt;Additional Features&lt;/h2&gt;

&lt;p&gt;Custom Resource Definition (CRD) Validation, now graduating to beta and enabled by default, helps CRD authors give clear and immediate feedback for invalid objects&lt;/p&gt;

&lt;p&gt;SIG Node hardware accelerator moves to alpha, enabling GPUs and consequently machine learning and other high performance workloads&lt;/p&gt;

&lt;p&gt;CoreDNS alpha makes it possible to install CoreDNS with standard tools&lt;/p&gt;

&lt;p&gt;IPVS mode for kube-proxy goes beta, providing better scalability and performance for large clusters&lt;/p&gt;

&lt;p&gt;Each Special Interest Group (SIG) in the community continues to deliver the most requested user features for their area. For a complete list, please visit the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v190&#34; target=&#34;_blank&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.9 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.9.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/kubernetes-basics/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://github.com/kubernetes/features/blob/master/release-1.9/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Anthony Yeh, Software Engineer at Google. The 14 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process has become an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has embarked on an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io/&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors. Open issues remained relatively stable over the course of the release, while forks rose approximately 20%, as did individuals starring the various project repositories. Approver volume has risen slightly since the last release, but a lull is commonplace during the last quarter of the year. With 75,000+ comments, Kubernetes remains one of the most actively discussed projects on GitHub.&lt;/p&gt;

&lt;h2 id=&#34;user-highlights&#34;&gt;User highlights&lt;/h2&gt;

&lt;p&gt;According to the l&lt;a href=&#34;https://www.cncf.io/blog/2017/12/06/cloud-native-technologies-scaling-production-applications&#34; target=&#34;_blank&#34;&gt;atest survey conducted by CNCF&lt;/a&gt;, 61 percent of organizations are evaluating and 83 percent are using Kubernetes in production. Example of user stories from the community include:&lt;/p&gt;

&lt;p&gt;BlaBlaCar, the world’s largest long distance carpooling community connects 40 million members across 22 countries. The company has about 3,000 pods, with &lt;a href=&#34;https://kubernetes.io/case-studies/blablacar/&#34; target=&#34;_blank&#34;&gt;1,200 of them running on Kubernetes&lt;/a&gt;, leading to improved website availability for customers.&lt;/p&gt;

&lt;p&gt;Pokémon GO, the popular free-to-play, location-based augmented reality game developed by Niantic for iOS and Android devices, has its application logic running on Google Container Engine powered by Kubernetes. This was the &lt;a href=&#34;https://cloudplatform.googleblog.com/2016/09/bringing-Pokemon-GO-to-life-on-Google-Cloud.html&#34; target=&#34;_blank&#34;&gt;largest Kubernetes deployment&lt;/a&gt; ever on Google Container Engine.&lt;/p&gt;

&lt;p&gt;Is Kubernetes helping your team? &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;Share your story&lt;/a&gt; with the community.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem updates&lt;/h2&gt;

&lt;p&gt;Announced on November 13, the &lt;a href=&#34;https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes Conformance Program&lt;/a&gt; ensures that Certified Kubernetes™ products deliver consistency and portability. Thirty-two Certified Kubernetes Distributions and Platforms are &lt;a href=&#34;https://kubernetes.io/partners/#dist&#34; target=&#34;_blank&#34;&gt;now available&lt;/a&gt;. Development of the certification program involved close collaboration between CNCF and the rest of the Kubernetes community, especially the Testing and Architecture Special Interest Groups (SIGs). The Kubernetes Architecture SIG is the final arbiter of the definition of API conformance for the program. The program also includes strong guarantees that commercial providers of Kubernetes will continue to release new versions to ensure that customers can take advantage of the rapid pace of ongoing development.&lt;/p&gt;

&lt;p&gt;CNCF also offers &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;online training&lt;/a&gt; that teaches the skills needed to create and configure a real-world Kubernetes cluster.&lt;/p&gt;

&lt;h2 id=&#34;kubecon&#34;&gt;KubeCon&lt;/h2&gt;

&lt;p&gt;For recorded sessions from the largest Kubernetes gathering, &lt;a href=&#34;http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-north-america&#34; target=&#34;_blank&#34;&gt;KubeCon + CloudNativeCon&lt;/a&gt; in Austin from December 6-8, 2017, visit &lt;a href=&#34;https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA&#34; target=&#34;_blank&#34;&gt;YouTube/CNCF&lt;/a&gt;. The premiere Kubernetes event will be back May 2-4, 2018 in Copenhagen and will feature technical sessions, case studies, developer deep dives, salons and more! &lt;a href=&#34;http://events.linuxfoundation.org/events/kubecon-and-cloudnativecon-europe/program/cfpguide&#34; target=&#34;_blank&#34;&gt;CFP&lt;/a&gt; closes January 12, 2018.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.9 release team on &lt;strong&gt;January 9th from 10am-11am PT&lt;/strong&gt; to learn about the major features in this release as they demo some of the highlights in the areas of Windows and Docker support, storage, admission control, and the workloads API.&amp;nbsp;&lt;a href=&#34;https://zoom.us/webinar/register/WN_oVjQMwyzQFOmWsfVzDsa2A&#34; target=&#34;_blank&#34;&gt;Register here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get involved:&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Chat with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Using eBPF in Kubernetes</title>
      <link>https://kubernetes.io/blog/2017/12/using-ebpf-in-kubernetes/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/12/using-ebpf-in-kubernetes/</guid>
      <description>
        
        
        

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Kubernetes provides a high-level API and a set of components that hides almost all of the intricate and—to some of us—interesting details of what happens at the systems level. Application developers are not required to have knowledge of the machines&amp;rsquo; IP tables, cgroups, namespaces, seccomp, or, nowadays, even the &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes&#34; target=&#34;_blank&#34;&gt;container runtime&lt;/a&gt; that their application runs on top of. But underneath, Kubernetes and the technologies upon which it relies (for example, the container runtime) heavily leverage core Linux functionalities.&lt;/p&gt;

&lt;p&gt;This article focuses on a core Linux functionality increasingly used in networking, security and auditing, and tracing and monitoring tools. This functionality is called &lt;a href=&#34;http://man7.org/linux/man-pages/man2/bpf.2.html&#34; target=&#34;_blank&#34;&gt;extended Berkeley Packet Filter&lt;/a&gt; (eBPF)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;In this article we use both acronyms: eBPF and BPF. The former is used for the extended BPF functionality, and the latter for &amp;ldquo;classic&amp;rdquo; BPF functionality.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-is-bpf&#34;&gt;What is BPF?&lt;/h2&gt;

&lt;p&gt;BPF is a mini-VM residing in the Linux kernel that runs BPF programs. Before running, BPF programs are loaded with the &lt;a href=&#34;http://man7.org/linux/man-pages/man2/bpf.2.html&#34; target=&#34;_blank&#34;&gt;bpf()&lt;/a&gt; syscall and are validated for safety: checking for loops, code size, etc. BPF programs are attached to kernel objects and executed when events happen on those objects—for example, when a network interface emits a packet.&lt;/p&gt;

&lt;h2 id=&#34;bpf-superpowers&#34;&gt;BPF Superpowers&lt;/h2&gt;

&lt;p&gt;BPF programs are event-driven by definition, an incredibly powerful concept, and executes code in the kernel when an event occurs. &lt;a href=&#34;http://www.brendangregg.com/bio.html&#34; target=&#34;_blank&#34;&gt;Netflix&amp;rsquo;s Brendan Gregg&lt;/a&gt; refers to BPF as a &lt;a href=&#34;http://www.brendangregg.com/blog/2016-03-05/linux-bpf-superpowers.html&#34; target=&#34;_blank&#34;&gt;Linux superpower&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-e-in-ebpf&#34;&gt;The &amp;lsquo;e&amp;rsquo; in eBPF&lt;/h2&gt;

&lt;p&gt;Traditionally, BPF could only be attached to sockets for socket filtering. BPF’s first use case was in &lt;code&gt;tcpdump&lt;/code&gt;. When you run &lt;code&gt;tcpdump&lt;/code&gt; the filter is compiled into a BPF program and attached to a raw &lt;code&gt;AF_PACKET&lt;/code&gt; socket in order to print out filtered packets.&lt;/p&gt;

&lt;p&gt;But over the years, eBPF added the ability to attach to &lt;a href=&#34;https://github.com/torvalds/linux/blob/v4.14/include/uapi/linux/bpf.h#L117-L133&#34; target=&#34;_blank&#34;&gt;other kernel objects&lt;/a&gt;. In addition to socket filtering, some supported attach points are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kprobes (and userspace equivalents uprobes)&lt;/li&gt;
&lt;li&gt;Tracepoints&lt;/li&gt;
&lt;li&gt;Network schedulers or qdiscs for classification or action (tc)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;XDP (eXpress Data Path)
This and other, newer features like in-kernel helper functions and shared data-structures (maps) that can be used to communicate with user space, extend BPF’s capabilities.&lt;/p&gt;

&lt;h2 id=&#34;existing-use-cases-of-ebpf-with-kubernetes&#34;&gt;Existing Use Cases of eBPF with Kubernetes&lt;/h2&gt;

&lt;p&gt;Several open-source Kubernetes tools already use eBPF and many use cases warrant a closer look, especially in areas such as networking, monitoring and security tools.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dynamic-network-control-and-visibility-with-cilium&#34;&gt;Dynamic Network Control and Visibility with Cilium&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cilium/cilium&#34; target=&#34;_blank&#34;&gt;Cilium&lt;/a&gt; is a networking project that makes heavy use of eBPF superpowers to route and filter network traffic for container-based systems. By using eBPF, Cilium can dynamically generate and apply rules—even at the device level with XDP—without making changes to the Linux kernel itself.&lt;/p&gt;

&lt;p&gt;The Cilium Agent runs on each host. Instead of managing IP tables, it translates network policy definitions to BPF programs that are loaded into the kernel and attached to a container&amp;rsquo;s virtual ethernet device. These programs are executed—rules applied—on each packet that is sent or received.&lt;/p&gt;

&lt;p&gt;This diagram shows how the Cilium project works:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/Xe8qee5yYsJton2NHFLOhHevxdbpCHHPPgttOLP18ZWtoUJp9ChFKtKJiTxqNFn8zQPRJu4BdtG7xc24vlGkD2gtfbkCuHq_eU3Tx6z2m6ld4iYGEZv-MsSCcJ3jAcJO2HkMc_d_&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Depending on what network rules are applied, BPF programs may be attached with &lt;a href=&#34;http://man7.org/linux/man-pages/man8/tc.8.html&#34; target=&#34;_blank&#34;&gt;tc&lt;/a&gt; or &lt;a href=&#34;https://www.iovisor.org/technology/xdp&#34; target=&#34;_blank&#34;&gt;XDP&lt;/a&gt;. By using XDP, Cilium can attach the BPF programs at the lowest possible point, which is also the most performant point in the networking software stack.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;d like to learn more about how Cilium uses eBPF, take a look at the project&amp;rsquo;s &lt;a href=&#34;http://cilium.readthedocs.io/en/latest/bpf/&#34; target=&#34;_blank&#34;&gt;BPF and XDP reference guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;tracking-tcp-connections-in-weave-scope&#34;&gt;Tracking TCP Connections in Weave Scope&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/weaveworks/scope&#34; target=&#34;_blank&#34;&gt;Weave Scope&lt;/a&gt; is a tool for monitoring, visualizing and interacting with container-based systems. For our purposes, we&amp;rsquo;ll focus on how Weave Scope gets the TCP connections.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/47C76UqCrrDr5O8wand6jESyFzx1SP4SQ_jVWiAhN5ctAEefz9e0orgmu0Q_2681QhcxJDfMQbn3HcRZYZN_QiPjKfXMo5Kt6XuXPjRGAoc_j2x7yC_9Un5JIoVt1Aa-DCHl-DUu&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Weave Scope employs an agent that runs on each node of a cluster. The agent monitors the system, generates a report and sends it to the app server. The app server compiles the reports it receives and presents the results in the Weave Scope UI.&lt;/p&gt;

&lt;p&gt;To accurately draw connections between containers, the agent attaches a BPF program to kprobes that track socket events: opening and closing connections. The BPF program, &lt;a href=&#34;https://github.com/weaveworks/tcptracer-bpf&#34; target=&#34;_blank&#34;&gt;tcptracer-bpf&lt;/a&gt;, is compiled into an ELF object file and loaded using &lt;a href=&#34;https://github.com/iovisor/gobpf&#34; target=&#34;_blank&#34;&gt;gopbf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(As a side note, Weave Scope also has a plugin that make use of eBPF: &lt;a href=&#34;https://github.com/weaveworks-plugins/scope-http-statistics&#34; target=&#34;_blank&#34;&gt;HTTP statistics&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;To learn more about how this works and why it&amp;rsquo;s done this way, read &lt;a href=&#34;https://www.weave.works/blog/improving-performance-reliability-weave-scope-ebpf/&#34; target=&#34;_blank&#34;&gt;this extensive post&lt;/a&gt; that the &lt;a href=&#34;https://kinvolk.io/&#34; target=&#34;_blank&#34;&gt;Kinvolk&lt;/a&gt; team wrote for the &lt;a href=&#34;https://www.weave.works/blog/&#34; target=&#34;_blank&#34;&gt;Weaveworks Blog&lt;/a&gt;. You can also watch &lt;a href=&#34;https://www.youtube.com/watch?v=uTTFUpT0Sfw&amp;amp;list=PLWYdJViL9Eio5o5j4Uth_-Mt0FPrYXNwx&#34; target=&#34;_blank&#34;&gt;a recent talk&lt;/a&gt; about the topic.&lt;/p&gt;

&lt;h2 id=&#34;limiting-syscalls-with-seccomp-bpf&#34;&gt;Limiting syscalls with seccomp-bpf&lt;/h2&gt;

&lt;p&gt;Linux has more than 300 system calls (read, write, open, close, etc.) available for use—or misuse. Most applications only need a small subset of syscalls to function properly. &lt;a href=&#34;https://en.wikipedia.org/wiki/Seccomp&#34; target=&#34;_blank&#34;&gt;seccomp&lt;/a&gt; is a Linux security facility used to limit the set of syscalls that an application can use, thereby limiting potential misuse.&lt;/p&gt;

&lt;p&gt;The original implementation of seccomp was highly restrictive. Once applied, if an application attempted to do anything beyond reading and writing to files it had already opened, seccomp sent a &lt;code&gt;SIGKILL&lt;/code&gt; signal.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.yadutaf.fr/2014/05/29/introduction-to-seccomp-bpf-linux-syscall-filter/&#34; target=&#34;_blank&#34;&gt;seccomp-bpf&lt;/a&gt; enables more complex filters and a wider range of actions. Seccomp-bpf, also known as seccomp mode 2, allows for applying custom filters in the form of BPF programs. When the BPF program is loaded, the filter is applied to each syscall and the appropriate action is taken (Allow, Kill, Trap, etc.).&lt;/p&gt;

&lt;p&gt;seccomp-bpf is widely used in Kubernetes tools and exposed in Kubernetes itself. For example, seccomp-bpf is used in Docker to apply custom &lt;a href=&#34;https://docs.docker.com/engine/security/seccomp/&#34; target=&#34;_blank&#34;&gt;seccomp security profiles&lt;/a&gt;, in rkt to apply &lt;a href=&#34;https://github.com/rkt/rkt/blob/5fadf0f1f444cdfde40d57e1d199b6dd6371594c/Documentation/seccomp-guide.md&#34; target=&#34;_blank&#34;&gt;seccomp isolators&lt;/a&gt;, and in Kubernetes itself in its &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34; target=&#34;_blank&#34;&gt;Security Context&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But in all of these cases the use of BPF is hidden behind &lt;a href=&#34;https://github.com/seccomp/libseccomp&#34; target=&#34;_blank&#34;&gt;libseccomp&lt;/a&gt;. Behind the scenes, libseccomp generates BPF code from rules provided to it. Once generated, the BPF program is loaded and the rules applied.&lt;/p&gt;

&lt;h2 id=&#34;potential-use-cases-for-ebpf-with-kubernetes&#34;&gt;Potential Use Cases for eBPF with Kubernetes&lt;/h2&gt;

&lt;p&gt;eBPF is a relatively new Linux technology. As such, there are many uses that remain unexplored. eBPF itself is also evolving: new features are being added in eBPF that will enable new use cases that aren’t currently possible. In the following sections, we&amp;rsquo;re going to look at some of these that have only recently become possible and ones on the horizon. Our hope is that these features will be leveraged by open source tooling.&lt;/p&gt;

&lt;h2 id=&#34;pod-and-container-level-network-statistics&#34;&gt;Pod and container level network statistics&lt;/h2&gt;

&lt;p&gt;BPF socket filtering is nothing new, but BPF socket filtering per cgroup is. Introduced in Linux 4.10, &lt;a href=&#34;https://lwn.net/Articles/698073/&#34; target=&#34;_blank&#34;&gt;cgroup-bpf&lt;/a&gt; allows attaching eBPF programs to cgroups. Once attached, the program is executed for all packets entering or exiting any process in the cgroup.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;http://man7.org/linux/man-pages/man7/cgroups.7.html&#34; target=&#34;_blank&#34;&gt;cgroup&lt;/a&gt; is, amongst other things, a hierarchical grouping of processes. In Kubernetes, this grouping is found at the container level. One idea for making use of cgroup-bpf, is to install BPF programs that collect detailed per-pod and/or per-container network statistics.&lt;/p&gt;

&lt;p&gt;Generally, such statistics are collected by periodically checking the relevant file in Linux&amp;rsquo;s &lt;code&gt;/sys&lt;/code&gt; directory or using Netlink. By using BPF programs attached to cgroups for this, we can get much more detailed statistics: for example, how many packets/bytes on tcp port 443, or how many packets/bytes from IP 10.2.3.4. In general, because BPF programs have a kernel context, they can safely and efficiently deliver more detailed information to user space.&lt;/p&gt;

&lt;p&gt;To explore the idea, the Kinvolk team implemented a proof-of-concept: &lt;a href=&#34;https://github.com/kinvolk/cgnet&#34; target=&#34;_blank&#34;&gt;https://github.com/kinvolk/cgnet&lt;/a&gt;. This project attaches a BPF program to each cgroup and exports the information to &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are of course other interesting possibilities, like doing actual packet filtering. But the obstacle currently standing in the way of this is having cgroup v2 support—required by cgroup-bpf—in &lt;a href=&#34;https://github.com/opencontainers/runc/issues/654&#34; target=&#34;_blank&#34;&gt;Docker&lt;/a&gt; and Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;application-applied-lsm&#34;&gt;Application-applied LSM&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Linux_Security_Modules&#34; target=&#34;_blank&#34;&gt;Linux Security Modules&lt;/a&gt; (LSM) implements a generic framework for security policies in the Linux kernel. &lt;a href=&#34;https://wiki.centos.org/HowTos/SELinux&#34; target=&#34;_blank&#34;&gt;SELinux&lt;/a&gt; and &lt;a href=&#34;https://wiki.ubuntu.com/AppArmor&#34; target=&#34;_blank&#34;&gt;AppArmor&lt;/a&gt; are examples of these. Both of these implement rules at a system-global scope, placing the onus on the administrator to configure the security policies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://landlock.io/&#34; target=&#34;_blank&#34;&gt;Landlock&lt;/a&gt; is another LSM under development that would co-exist with SELinux and AppArmor. An initial patchset has been submitted to the Linux kernel and is in an early stage of development. The main difference with other LSMs is that Landlock is designed to allow unprivileged applications to build their own sandbox, effectively restricting themselves instead of using a global configuration. With Landlock, an application can load a BPF program and have it executed when the process performs a specific action. For example, when the application opens a file with the open() system call, the kernel will execute the BPF program, and, depending on what the BPF program returns, the action will be accepted or denied.&lt;/p&gt;

&lt;p&gt;In some ways, it is similar to seccomp-bpf: using a BPF program, seccomp-bpf allows unprivileged processes to restrict what system calls they can perform. Landlock will be more powerful and provide more flexibility. Consider the following system call:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C  
fd = open(“myfile.txt”, O\_RDWR);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first argument is a “char *”, a pointer to a memory address, such as &lt;code&gt;0xab004718&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With seccomp, a BPF program only has access to the parameters of the syscall but cannot dereference the pointers, making it impossible to make security decisions based on a file. seccomp also uses classic BPF, meaning it cannot make use of eBPF maps, the mechanism for interfacing with user space. This restriction means security policies cannot be changed in seccomp-bpf based on a configuration in an eBPF map.&lt;/p&gt;

&lt;p&gt;BPF programs with Landlock don’t receive the arguments of the syscalls but a reference to a kernel object. In the example above, this means it will have a reference to the file, so it does not need to dereference a pointer, consider relative paths, or perform chroots.&lt;/p&gt;

&lt;h2 id=&#34;use-case-landlock-in-kubernetes-based-serverless-frameworks&#34;&gt;Use Case: Landlock in Kubernetes-based serverless frameworks&lt;/h2&gt;

&lt;p&gt;In Kubernetes, the unit of deployment is a pod. Pods and containers are the main unit of isolation. In serverless frameworks, however, the main unit of deployment is a function. Ideally, the unit of deployment equals the unit of isolation. This puts serverless frameworks like &lt;a href=&#34;https://github.com/kubeless/kubeless&#34; target=&#34;_blank&#34;&gt;Kubeless&lt;/a&gt; or &lt;a href=&#34;https://github.com/openfaas/faas&#34; target=&#34;_blank&#34;&gt;OpenFaaS&lt;/a&gt; into a predicament: optimize for unit of isolation or deployment?&lt;/p&gt;

&lt;p&gt;To achieve the best possible isolation, each function call would have to happen in its own container—ut what&amp;rsquo;s good for isolation is not always good for performance. Inversely, if we run function calls within the same container, we increase the likelihood of collisions.&lt;/p&gt;

&lt;p&gt;By using Landlock, we could isolate function calls from each other within the same container, making a temporary file created by one function call inaccessible to the next function call, for example. Integration between Landlock and technologies like Kubernetes-based serverless frameworks would be a ripe area for further exploration.&lt;/p&gt;

&lt;h2 id=&#34;auditing-kubectl-exec-with-ebpf&#34;&gt;Auditing kubectl-exec with eBPF&lt;/h2&gt;

&lt;p&gt;In Kubernetes 1.7 the &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&#34; target=&#34;_blank&#34;&gt;audit proposal&lt;/a&gt; started making its way in. It&amp;rsquo;s currently pre-stable with plans to be stable in the 1.10 release. As the name implies, it allows administrators to log and audit events that take place in a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;While these events log Kubernetes events, they don&amp;rsquo;t currently provide the level of visibility that some may require. For example, while we can see that someone has used &lt;code&gt;kubectl exec&lt;/code&gt; to enter a container, we are not able to see what commands were executed in that session. With eBPF one can attach a BPF program that would record any commands executed in the &lt;code&gt;kubectl exec&lt;/code&gt; session and pass those commands to a user-space program that logs those events. We could then play that session back and know the exact sequence of events that took place.&lt;/p&gt;

&lt;h2 id=&#34;learn-more-about-ebpf&#34;&gt;Learn more about eBPF&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;re interested in learning more about eBPF, here are some resources:
- A comprehensive &lt;a href=&#34;https://qmonnet.github.io/whirl-offload/2016/09/01/dive-into-bpf/&#34; target=&#34;_blank&#34;&gt;reading list about eBPF&lt;/a&gt; for doing just that
- &lt;a href=&#34;https://github.com/iovisor/bcc&#34; target=&#34;_blank&#34;&gt;BCC&lt;/a&gt; (BPF Compiler Collection) provides tools for working with eBPF as well as many example tools making use of BCC.
- Some videos&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JRFNIKUROPE&#34; target=&#34;_blank&#34;&gt;BPF: Tracing and More&lt;/a&gt; by Brendan Gregg&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CcGtDMm1SJA&#34; target=&#34;_blank&#34;&gt;Cilium - Container Security and Networking Using BPF and XDP&lt;/a&gt; by Thomas Graf&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=T3Wcuj8fy5o&#34; target=&#34;_blank&#34;&gt;Using BPF in Kubernetes&lt;/a&gt; by Alban Crequy&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We are just starting to see the Linux superpowers of eBPF being put to use in Kubernetes tools and technologies. We will undoubtedly see increased use of eBPF. What we have highlighted here is just a taste of what you might expect in the future. What will be really exciting is seeing how these technologies will be used in ways that we have not yet thought about. Stay tuned!&lt;/p&gt;

&lt;p&gt;The Kinvolk team will be hanging out at the Kinvolk booth at KubeCon in Austin. Come by to talk to us about all things, Kubernetes, Linux, container runtimes and yeah, eBPF.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  PaddlePaddle Fluid: Elastic Deep Learning on Kubernetes </title>
      <link>https://kubernetes.io/blog/2017/12/paddle-paddle-fluid-elastic-learning/</link>
      <pubDate>Wed, 06 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/12/paddle-paddle-fluid-elastic-learning/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;em&gt;Editor&amp;rsquo;s note: Today&amp;rsquo;s post is a joint post from the deep learning team at Baidu and the etcd team at CoreOS.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;paddlepaddle-fluid-elastic-deep-learning-on-kubernetes&#34;&gt;PaddlePaddle Fluid: Elastic Deep Learning on Kubernetes&lt;/h2&gt;

&lt;p&gt;Two open source communities—PaddlePaddle, the deep learning framework originated in Baidu, and Kubernetes®, the most famous containerized application scheduler—are announcing the Elastic Deep Learning (EDL) feature in PaddlePaddle’s new release codenamed Fluid.&lt;/p&gt;

&lt;p&gt;Fluid EDL includes a &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md&#34; target=&#34;_blank&#34;&gt;Kubernetes controller&lt;/a&gt;, &lt;a href=&#34;https://github.com/PaddlePaddle/cloud/tree/develop/doc/autoscale&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;PaddlePaddle auto-scaler&lt;/em&gt;&lt;/a&gt;, which changes the number of processes of distributed jobs according to the idle hardware resource in the cluster, and a new fault-tolerable architecture as described in the &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/cluster_train/README.md&#34; target=&#34;_blank&#34;&gt;PaddlePaddle design doc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Industrial deep learning requires significant computation power. Research labs and companies often build GPU clusters managed by SLURM, MPI, or SGE. These clusters either run a submitted job if it requires less than the idle resource, or pend the job for an unpredictably long time. This approach has its drawbacks: in an example with 99 available nodes and a submitted job that requires 100, the job has to wait without using any of the available nodes. Fluid works with Kubernetes to power elastic deep learning jobs, which often lack optimal resources, by helping to expose potential algorithmic problems as early as possible.&lt;/p&gt;

&lt;p&gt;Another challenge is that industrial users tend to run deep learning jobs as a subset stage of the complete data pipeline, including the web server and log collector. Such general-purpose clusters require priority-based elastic scheduling. This makes it possible to run more processes in the web server job and less in deep learning during periods of high web traffic, then prioritize deep learning when web traffic is low. Fluid talks to Kubernetes&amp;rsquo; API server to understand the global picture and orchestrate the number of processes affiliated with various jobs.&lt;/p&gt;

&lt;p&gt;In both scenarios, PaddlePaddle jobs are tolerant to a process spikes and decreases. We achieved this by implementing the new design, which introduces a master process in addition to the old PaddlePaddle architecture as described in a &lt;a href=&#34;https://kubernetes.io/blog/2017/02/run-deep-learning-with-paddlepaddle-on-kubernetes&#34; target=&#34;_blank&#34;&gt;previous blog post&lt;/a&gt;. In the new design, as long as there are three processes left in a job, it continues. In extreme cases where all processes are killed, the job can be restored and resume.&lt;/p&gt;

&lt;p&gt;We tested Fluid EDL for two use cases: 1) the Kubernetes cluster runs only PaddlePaddle jobs; and 2) the cluster runs PaddlePaddle and Nginx jobs.&lt;/p&gt;

&lt;p&gt;In the first test, we started up to 20 PaddlePaddle jobs one by one with a 10-second interval. Each job has 60 trainers and 10 parameter server processes, and will last for hours. We repeated the experiment 20 times: 10 with FluidEDL turned off and 10 with FluidEDL turned on. In Figure one, solid lines correspond to the first 10 experiments and dotted lines the rest. In the upper part of the figure, we see that the number of pending jobs increments monotonically without EDL. However, when EDL is turned on, resources are evenly distributed to all jobs. Fluid EDL kills some existing processes to make room for new jobs and jobs coming in at a later point in time. In both cases, the cluster is equally utilized (see lower part of figure).&lt;/p&gt;

&lt;p&gt;| &lt;a href=&#34;https://1.bp.blogspot.com/-sp_sVZvhMbU/WiYgXMLQKuI/AAAAAAAAAIM/uc_3iT9BZmAtQGiGGSErgueHK71uWMBCACEwYBhgL/s1600/figure-1.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://1.bp.blogspot.com/-sp_sVZvhMbU/WiYgXMLQKuI/AAAAAAAAAIM/uc_3iT9BZmAtQGiGGSErgueHK71uWMBCACEwYBhgL/s640/figure-1.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt; |
| &lt;em&gt;Figure 1. Fluid EDL evenly distributes resource among jobs.&lt;/em&gt;&lt;br /&gt;
 |&lt;/p&gt;

&lt;p&gt;In the second test, each experiment ran 400 Nginx pods, which has higher priority than the six PaddlePaddle jobs. Initially, each PaddlePaddle job had 15 trainers and 10 parameter servers. We killed 100 Nginx pods every 90 seconds until 100 left, and then we started to increase the number of Nginx jobs by 100 every 90 seconds. The upper part of Figure 2 shows this process. The middle of the diagram shows that Fluid EDL automatically started some PaddlePaddle processes by decreasing Nginx pods, and killed PaddlePaddle processes by increasing Nginx pods later on. As a result, the cluster maintains around 90% utilization as shown in the bottom of the figure. When Fluid EDL was turned off, there were no PaddlePaddle processes autoincrement, and the utilization fluctuated with the varying number of Nginx pods.&lt;/p&gt;

&lt;p&gt;| &lt;a href=&#34;https://4.bp.blogspot.com/-gOMFfnaygSU/WiYgXO_KJ0I/AAAAAAAAAII/lMLjTGNGYhsovwKornCzMZBhEdMdPI5HACLcBGAs/s1600/figure-2.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-gOMFfnaygSU/WiYgXO_KJ0I/AAAAAAAAAII/lMLjTGNGYhsovwKornCzMZBhEdMdPI5HACLcBGAs/s640/figure-2.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt; |
| &lt;em&gt;Figure 2. Fluid changes PaddlePaddle processes with the change of Nginx processes.&lt;/em&gt; |&lt;/p&gt;

&lt;p&gt;We continue to work on FluidEDL and welcome comments and contributions. Visit the &lt;a href=&#34;https://github.com/PaddlePaddle/cloud&#34; target=&#34;_blank&#34;&gt;PaddlePaddle repo&lt;/a&gt;, where you can find the &lt;a href=&#34;https://github.com/PaddlePaddle/cloud/blob/develop/doc/autoscale/README.md&#34; target=&#34;_blank&#34;&gt;design doc&lt;/a&gt;, a &lt;a href=&#34;https://github.com/PaddlePaddle/cloud/blob/develop/doc/autoscale/example/autoscale.md&#34; target=&#34;_blank&#34;&gt;simple tutorial&lt;/a&gt;, and &lt;a href=&#34;https://github.com/PaddlePaddle/cloud/tree/develop/doc/autoscale/experiment&#34; target=&#34;_blank&#34;&gt;experiment details&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Xu Yan (Baidu Research)&lt;/li&gt;
&lt;li&gt;Helin Wang (Baidu Research)&lt;/li&gt;
&lt;li&gt;Yi Wu (Baidu Research)&lt;/li&gt;
&lt;li&gt;Xi Chen (Baidu Research)&lt;/li&gt;
&lt;li&gt;Weibao Gong (Baidu Research)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Xiang Li (CoreOS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yi Wang (Baidu Research)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Autoscaling in Kubernetes </title>
      <link>https://kubernetes.io/blog/2017/11/autoscaling-in-kubernetes/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/11/autoscaling-in-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes allows developers to automatically adjust cluster sizes and the number of pod replicas based on current traffic and load. These adjustments reduce the amount of unused nodes, saving money and resources. In this talk, Marcin Wielgus of Google walks you through the current state of pod and node autoscaling in Kubernetes: .how it works, and how to use it, including best practices for deployments in production applications.&lt;/p&gt;

&lt;p&gt;Enjoyed this talk? Join us for more exciting sessions on scaling and automating your Kubernetes clusters at KubeCon in Austin on December 6-8. &lt;a href=&#34;https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006&#34; target=&#34;_blank&#34;&gt;Register Now&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Be sure to check out &lt;a href=&#34;http://sched.co/CU64&#34; target=&#34;_blank&#34;&gt;Automating and Testing Production Ready Kubernetes Clusters in the Public Cloud&lt;/a&gt; by Ron Lipke, Senior Developer, Platform as a Service, Gannet/USA Today Network.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Certified Kubernetes Conformance Program: Launch Celebration Round Up </title>
      <link>https://kubernetes.io/blog/2017/11/certified-kubernetes-conformance/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/11/certified-kubernetes-conformance/</guid>
      <description>
        
        
        &lt;p&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-YasPeoIh8tA/Wg28rH4dzXI/AAAAAAAAAHg/Hfk2dnUoav4XMefGyjzMWdJMZbu1QJFagCK4BGAYYCw/s1600/certified_kubernetes_color.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://1.bp.blogspot.com/-YasPeoIh8tA/Wg28rH4dzXI/AAAAAAAAAHg/Hfk2dnUoav4XMefGyjzMWdJMZbu1QJFagCK4BGAYYCw/s200/certified_kubernetes_color.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;This week the CNCFⓇ &lt;a href=&#34;https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/&#34; target=&#34;_blank&#34;&gt;certified the first group&lt;/a&gt; of KubernetesⓇ&amp;nbsp;offerings under the &lt;a href=&#34;https://www.cncf.io/certification/software-conformance/&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes Conformance Program&lt;/a&gt;. These first certifications follow a &lt;a href=&#34;https://kubernetes.io/blog/2017/10/software-conformance-certification&#34; target=&#34;_blank&#34;&gt;beta phase&lt;/a&gt; during which we invited participants to submit conformance results. The community response was overwhelming: CNCF certified offerings from 32 vendors!&lt;/p&gt;

&lt;p&gt;The new Certified Kubernetes Conformance Program gives enterprise organizations the confidence that workloads running on any Certified Kubernetes distribution or platform will work correctly on other Certified Kubernetes distributions or platforms. A Certified Kubernetes product guarantees that the complete Kubernetes API functions as specified, so users can rely on a seamless, stable experience.&lt;/p&gt;

&lt;p&gt;Here’s what the world had to say about the Certified Kubernetes Conformance Program.&lt;/p&gt;

&lt;p&gt;Press coverage:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://venturebeat.com/2017/11/13/ibm-google-microsoft-and-33-more-partner-to-ensure-kubernetes-workload-portability/&#34; target=&#34;_blank&#34;&gt;IBM, Google, Microsoft, and 33 more partner to ensure Kubernetes workload portability&lt;/a&gt;, VentureBeat.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://techcrunch.com/2017/11/13/the-cncf-just-got-36-companies-to-agree-to-a-kubernetes-certification-standard/&#34; target=&#34;_blank&#34;&gt;The CNCF just got 36 companies to agree to a Kubernetes certification standard&lt;/a&gt;, TechCrunch&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://thenewstack.io/cncf-introduces-c/&#34; target=&#34;_blank&#34;&gt;CNCF Ensures Kubernetes Interoperability with a New Cert Program&lt;/a&gt;, The New Stack&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sdtimes.com/cloud-native-launches-certified-kubernetes-program/&#34; target=&#34;_blank&#34;&gt;Cloud Native launches Certified Kubernetes program&lt;/a&gt;, SD Times&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rcrwireless.com/20171113/interoperability-standards-congealing-via-certified-kubernetes-program-tag27&#34; target=&#34;_blank&#34;&gt;CNCF offers Kubernetes Software Conformance Certification program&lt;/a&gt; RCR Wireless&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://virtualizationreview.com/articles/2017/11/13/new-kubernetes-certification-program-announced.aspx&#34; target=&#34;_blank&#34;&gt;New Kubernetes Certification Program Announced&lt;/a&gt; Virtualization Review&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.geekwire.com/2017/key-cloud-computing-group-launches-interoperability-certification-kubernetes/&#34; target=&#34;_blank&#34;&gt;Key Cloud Computing Group Launches Interoperability Certification for Kubernetes&lt;/a&gt; GeekWire&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://betanews.com/2017/11/13/kubernetes-certification-cloud/&#34; target=&#34;_blank&#34;&gt;New Kubernetes Certification Program Helps Deliver Consistency in the Cloud&lt;/a&gt; BetaNews&lt;/li&gt;
&lt;li&gt;​&lt;a href=&#34;http://www.zdnet.com/article/kubernetes-vendors-agree-on-standardization/&#34; target=&#34;_blank&#34;&gt;Kubernetes vendors agree on standardization&lt;/a&gt;, ZDNet&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.sdxcentral.com/articles/news/cncf-kubernetes-conformance-program-provides-seal-of-approval/2017/11/&#34; target=&#34;_blank&#34;&gt;CNCF Launches Kubernetes Conformance Certification Program&lt;/a&gt;, sdxcentral
Community blog round-up:&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloudplatform.googleblog.com/2017/11/introducing-Certified-Kubernetes-and-Google-Kubernetes-Engine.html&#34; target=&#34;_blank&#34;&gt;Introducing Certified Kubernetes (and Google Kubernetes Engine!)&lt;/a&gt;, Google&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.heptio.com/certified-kubernetes-a-key-step-forward-for-the-open-source-ecosystem-1f845df65898&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes: A key step forward for the open source ecosystem&lt;/a&gt;, Heptio&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://developer.ibm.com/code/2017/11/13/is-kubernetes-crossing-the-chasm-yes/&#34; target=&#34;_blank&#34;&gt;Is Kubernetes “crossing the chasm”? Yes!&lt;/a&gt;, IBM&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://coreos.com/blog/coreos-tectonic-is-certified-kubernetes&#34; target=&#34;_blank&#34;&gt;CoreOS Tectonic is Certified Kubernetes&lt;/a&gt;, CoreOS&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blogs.vmware.com/cloudnative/2017/11/13/vmware-pivotal-container-service-achieves-kubernetes-certification/&#34; target=&#34;_blank&#34;&gt;VMware Pivotal Container Service Achieves Kubernetes Certification&lt;/a&gt;, VMWare Pivotal&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.huaweicloud.com/en-us/news/1510655878651.html&#34; target=&#34;_blank&#34;&gt;Huawei Cloud Container Engine gained first wave of Certificated Kubernetes Qualification&lt;/a&gt;, Huawei&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloudfoundry.org/cloud-foundry-container-runtime-gets-kubernetes-certified/&#34; target=&#34;_blank&#34;&gt;Cloud Foundry Container Runtime Gets Kubernetes-Certified&lt;/a&gt;, Cloud Foundry&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.suse.com/communities/blog/suse-caas-platform-2-certified-kubernetes-conformance-software-certification/&#34; target=&#34;_blank&#34;&gt;SUSE CaaS Platform 2 certified under Kubernetes Conformance Software Certification&lt;/a&gt;, SUSE&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://mesosphere.com/blog/kubernetes-on-mesosphere-dcos-now-certified-by-cncf/&#34; target=&#34;_blank&#34;&gt;Kubernetes on Mesosphere DC/OS now certified by CNCF&lt;/a&gt;, Mesosphere&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://rancher.com/joining-k8s-conformance-program/&#34; target=&#34;_blank&#34;&gt;Rancher joins the CNCF Kubernetes Software Conformance Certification program&lt;/a&gt;, Rancher&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.stackpoint.io/stackpointcloud-becomes-first-multi-cloud-cncf-certified-kubernetes-offering-3c7983a71c5f&#34; target=&#34;_blank&#34;&gt;StackPointCloud Becomes First Multi-Cloud CNCF Certified Kubernetes Offering&lt;/a&gt;, StackPointCloud
Visit &lt;a href=&#34;https://www.cncf.io/certification/software-conformance/&#34; target=&#34;_blank&#34;&gt;https://www.cncf.io/certification/software-conformance&lt;/a&gt; for more information about the Certified Kubernetes Conformance Program, and learn how you can join a growing list of Certified Kubernetes providers.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;“Cloud Native Computing Foundation”, “CNCF” and “Kubernetes” are registered trademarks of The Linux Foundation in the United States and other countries. “Certified Kubernetes” and the Certified Kubernetes design are trademarks of The Linux Foundation in the United States and other countries.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes is Still Hard (for Developers) </title>
      <link>https://kubernetes.io/blog/2017/11/kubernetes-is-still-hard-for-developers/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/11/kubernetes-is-still-hard-for-developers/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes has made the Ops experience much easier, but how does the developer experience compare? Ops teams can deploy a Kubernetes cluster in a matter of minutes. But developers need to understand a host of new concepts before beginning to work with Kubernetes. This can be a tedious and manual process, but it doesn’t have to be. In this talk, &lt;a href=&#34;https://twitter.com/michellenoorali&#34; target=&#34;_blank&#34;&gt;Michelle Noorali&lt;/a&gt;, co-lead of SIG-Apps, reimagines the Kubernetes developer experience. She shares her top 3 tips for building a successful developer experience including:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A framework for thinking about cloud native applications&lt;/li&gt;
&lt;li&gt;An integrated experience for debugging and fine-tuning cloud native applicationsA way to get a cloud native application out the door quickly
Interested in learning how far the Kubernetes developer experience has come? Join us at KubeCon in Austin on December 6-8.&amp;nbsp;&lt;a href=&#34;https://goo.gl/TK9ET3&#34; target=&#34;_blank&#34;&gt;Register Now &amp;gt;&amp;gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;http://sched.co/CUCC&#34; target=&#34;_blank&#34;&gt;Check out Michelle’s keynote&lt;/a&gt; to learn about exciting new updates from CNCF projects.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Securing Software Supply Chain with Grafeas </title>
      <link>https://kubernetes.io/blog/2017/11/securing-software-supply-chain-grafeas/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/11/securing-software-supply-chain-grafeas/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor&amp;rsquo;s note: This post is written by Kelsey Hightower, Staff Developer Advocate at Google, and Sandra Guo, Product Manager at Google.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes has evolved to support increasingly complex classes of applications, enabling the development of two major industry trends: hybrid cloud and microservices. With increasing complexity in production environments, customers—especially enterprises—are demanding better ways to manage their software supply chain with more centralized visibility and control over production deployments.&lt;/p&gt;

&lt;p&gt;On October 12th, Google and partners &lt;a href=&#34;https://cloudplatform.googleblog.com/2017/10/introducing-grafeas-open-source-api-.html&#34; target=&#34;_blank&#34;&gt;announced&lt;/a&gt; Grafeas, an open source initiative to define a best practice for auditing and governing the modern software supply chain. With Grafeas (“scribe” in Greek), developers can plug in components of the CI/CD pipeline into a central source of truth for tracking and enforcing policies. Google is also working on &lt;a href=&#34;https://github.com/Grafeas/Grafeas/blob/master/case-studies/binary-authorization.md&#34; target=&#34;_blank&#34;&gt;Kritis&lt;/a&gt; (“judge” in Greek), allowing devOps teams to enforce deploy-time image policy using metadata and attestations stored in Grafeas.&lt;/p&gt;

&lt;p&gt;Grafeas allows build, auditing and compliance tools to exchange comprehensive metadata on container images using a central API. This allows enforcing policies that provide central control over the software supply process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-TDD4slMA7gg/WfzDeKVLr2I/AAAAAAAAAGw/dhfWOrCMdmogSNhGr5RrA2ovr02K5nn8ACK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.13%2BPM.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://2.bp.blogspot.com/-TDD4slMA7gg/WfzDeKVLr2I/AAAAAAAAAGw/dhfWOrCMdmogSNhGr5RrA2ovr02K5nn8ACK4BGAYYCw/s400/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.13%2BPM.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;example-application-paymentprocessor&#34;&gt;Example application: PaymentProcessor&lt;/h2&gt;

&lt;p&gt;Let’s consider a simple application, &lt;em&gt;PaymentProcessor&lt;/em&gt;, that retrieves, processes and updates payment info stored in a database. This application is made up of two containers: a standard ruby container and custom logic.&lt;/p&gt;

&lt;p&gt;Due to the sensitive nature of the payment data, the developers and DevOps team really want to make sure that the code meets certain security and compliance requirements, with detailed records on the provenance of this code. There are CI/CD stages that validate the quality of the PaymentProcessor release, but there is no easy way to centrally view/manage this information:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-WeI6zpGd42A/WfzDkkIonFI/AAAAAAAAAG4/wKUaNaXYvaQ-an9p4_9T9J3EQB_zHkRXwCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.23%2BPM.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://1.bp.blogspot.com/-WeI6zpGd42A/WfzDkkIonFI/AAAAAAAAAG4/wKUaNaXYvaQ-an9p4_9T9J3EQB_zHkRXwCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.23%2BPM.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;visibility-and-governance-over-the-paymentprocessor-code&#34;&gt;Visibility and governance over the PaymentProcessor Code&lt;/h2&gt;

&lt;p&gt;Grafeas provides an API for customers to centrally manage metadata created by various CI/CD components and enables deploy time policy enforcement through a Kritis implementation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-SRMfm5z606M/WfzDpHqlz-I/AAAAAAAAAHA/y2suaInhr9E0hU0u78PacBT_kZj2D7DKgCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.34%2BPM.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-SRMfm5z606M/WfzDpHqlz-I/AAAAAAAAAHA/y2suaInhr9E0hU0u78PacBT_kZj2D7DKgCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.34%2BPM.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s consider a basic example of how Grafeas can provide deploy time control for the PaymentProcessor app using a demo verification pipeline.&lt;/p&gt;

&lt;p&gt;Assume that a PaymentProcessor container image has been created and pushed to Google Container Registry. This example uses the gcr.io/exampleApp/PaymentProcessor container for testing. You as the QA engineer want to create an attestation certifying this image for production usage. Instead of trusting an image tag like 0.0.1, which can be reused and point to a different container image later, we can trust the image digest to ensure the attestation links to the full image contents.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Set up the environment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Generate a signing key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gpg --quick-generate-key --yes qa\_bob@example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Export the image signer&amp;rsquo;s public key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gpg --armor --export image.signer@example.com \&amp;gt; ${GPG\_KEY\_ID}.pub
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the ‘qa’ AttestationAuthority note via the Grafeas API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -X POST \  
  &amp;quot;http://127.0.0.1:8080/v1alpha1/projects/image-signing/notes?noteId=qa&amp;quot; \  
  -d @note.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the Kubernetes ConfigMap for admissions control and store the QA signer&amp;rsquo;s public key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create configmap image-signature-webhook \  
  --from-file ${GPG\_KEY\_ID}.pub

kubectl get configmap image-signature-webhook -o yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set up an admissions control webhook to require QA signature during deployment.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f kubernetes/image-signature-webhook.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2. Attempt to deploy an image without QA attestation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Attempt to run the image in paymentProcessor.ymal before it is QA attested:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f pods/nginx.yaml

apiVersion: v1

kind: Pod

metadata:

  name: payment

spec:

  containers:

    - name: payment

      image: &amp;quot;gcr.io/hightowerlabs/payment@sha256:aba48d60ba4410ec921f9d2e8169236c57660d121f9430dc9758d754eec8f887&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the paymentProcessor pod:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f pods/paymentProcessor.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the paymentProcessor pod was not created and the following error was returned:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The  &amp;quot;&amp;quot; is invalid: : No matched signatures for container image: gcr.io/hightowerlabs/payment@sha256:aba48d60ba4410ec921f9d2e8169236c57660d121f9430dc9758d754eec8f887
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3. Create an image signature&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Assume the image digest is stored in Image-digest.txt, sign the image digest:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gpg -u qa\_bob@example.com \  
  --armor \  
  --clearsign \  
  --output=signature.gpg \  
  Image-digest.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;4. Upload the signature to the Grafeas API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Generate a pgpSignedAttestation occurrence from the signature :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat \&amp;gt; occurrence.json \&amp;lt;\&amp;lt;EOF  
{  
  &amp;quot;resourceUrl&amp;quot;: &amp;quot;$(cat image-digest.txt)&amp;quot;,  
  &amp;quot;noteName&amp;quot;: &amp;quot;projects/image-signing/notes/qa&amp;quot;,  
  &amp;quot;attestation&amp;quot;: {  
    &amp;quot;pgpSignedAttestation&amp;quot;: {  
       &amp;quot;signature&amp;quot;: &amp;quot;$(cat signature.gpg)&amp;quot;,  
       &amp;quot;contentType&amp;quot;: &amp;quot;application/vnd.gcr.image.url.v1&amp;quot;,  
       &amp;quot;pgpKeyId&amp;quot;: &amp;quot;${GPG\_KEY\_ID}&amp;quot;  
    }  
  }  
}  
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Upload the attestation through the Grafeas API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -X POST \  
  &#39;http://127.0.0.1:8080/v1alpha1/projects/image-signing/occurrences&#39; \  
  -d @occurrence.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;5. Verify QA attestation during a production deployment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Attempt to run the image in paymentProcessor.ymal now that it has the correct attestation in the Grafeas API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f pods/paymentProcessor.yaml

pod &amp;quot;PaymentProcessor&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the attestation added the pod will be created as the execution criteria are met.&lt;/p&gt;

&lt;p&gt;For more detailed information, see this &lt;a href=&#34;https://github.com/kelseyhightower/grafeas-tutorial&#34; target=&#34;_blank&#34;&gt;Grafeas tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The demo above showed how you can integrate your software supply chain with Grafeas and gain visibility and control over your production deployments. However, the demo verification pipeline by itself is not a full Kritis implementation. In addition to basic admission control, Kritis provides additional support for workflow enforcement, multi-authority signing, breakglass deployment and more. You can read the &lt;a href=&#34;https://github.com/Grafeas/Grafeas/blob/master/case-studies/binary-authorization.md&#34; target=&#34;_blank&#34;&gt;Kritis whitepaper&lt;/a&gt; for more details. The team is actively working on a full open-source implementation. We’d love your feedback!&lt;/p&gt;

&lt;p&gt;In addition, a hosted alpha implementation of Kritis, called Binary Authorization, is available on Google Container Engine and will be available for broader consumption soon.&lt;/p&gt;

&lt;p&gt;Google, JFrog, and other partners joined forces to create Grafeas based on our common experiences building secure, large, and complex microservice deployments for internal and enterprise customers. Grafeas is an industry-wide community effort.&lt;/p&gt;

&lt;p&gt;To learn more about Grafeas and contribute to the project:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Register for the JFrog-Google webinar [&lt;a href=&#34;https://leap.jfrog.com/WN2017-ImplementingaSingleSourceofTruthinaHybridCloudWorld_RegistrationPage.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Try Grafeas now and join the GitHub project: &lt;a href=&#34;https://github.com/grafeas&#34; target=&#34;_blank&#34;&gt;https://github.com/grafeas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Try out the Grafeas demo and tutorial: &lt;a href=&#34;https://github.com/kelseyhightower/grafeas-tutorial&#34; target=&#34;_blank&#34;&gt;https://github.com/kelseyhightower/grafeas-tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Attend Shopify’s talks at &lt;a href=&#34;https://kccncna17.sched.com/event/CU83/securing-shopifys-paas-on-gke-i-jonathan-pulsifer-shopify&#34; target=&#34;_blank&#34;&gt;KubeCon in December&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fill out [&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdr8kDTkAkml5f9TW_kzz06C0s0QuV_sWYzHC7NM90F5CZ2bQ/viewform&#34; target=&#34;_blank&#34;&gt;this form&lt;/a&gt;] if you’re interested in learning more about our upcoming releases or talking to us about integrations&lt;/li&gt;
&lt;li&gt;See &lt;a href=&#34;https://grafeas.io/&#34; target=&#34;_blank&#34;&gt;grafeas.io&lt;/a&gt; for documentation and examples
We hope you join us!&lt;br /&gt;
The Grafeas Team&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:   Containerd Brings More Container Runtime Options for Kubernetes </title>
      <link>https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor&amp;rsquo;s note: Today&amp;rsquo;s post is by Lantao Liu, Software Engineer at Google, and Mike Brown, Open Source Developer Advocate at IBM.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;container runtime&lt;/em&gt; is software that executes containers and manages container images on a node. Today, the most widely known container runtime is &lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34;&gt;Docker&lt;/a&gt;, but there are other container runtimes in the ecosystem, such as &lt;a href=&#34;https://coreos.com/rkt/&#34; target=&#34;_blank&#34;&gt;rkt&lt;/a&gt;, &lt;a href=&#34;https://containerd.io/&#34; target=&#34;_blank&#34;&gt;containerd&lt;/a&gt;, and &lt;a href=&#34;https://linuxcontainers.org/lxd/&#34; target=&#34;_blank&#34;&gt;lxd&lt;/a&gt;. Docker is by far the most common container runtime used in production Kubernetes environments, but Docker’s smaller offspring, containerd, may prove to be a better option. This post describes using containerd with Kubernetes.&lt;/p&gt;

&lt;p&gt;Kubernetes 1.5 introduced an internal plugin API named &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes&#34; target=&#34;_blank&#34;&gt;Container Runtime Interface (CRI)&lt;/a&gt; to provide easy access to different container runtimes. CRI enables Kubernetes to use a variety of container runtimes without the need to recompile. In theory, Kubernetes could use any container runtime that implements CRI to manage pods, containers and container images.&lt;/p&gt;

&lt;p&gt;Over the past 6 months, engineers from Google, Docker, IBM, ZTE, and ZJU have worked to implement CRI for containerd. The project is called &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd&#34; target=&#34;_blank&#34;&gt;cri-containerd&lt;/a&gt;, which had its &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd/releases/tag/v1.0.0-alpha.0&#34; target=&#34;_blank&#34;&gt;feature complete v1.0.0-alpha.0 release&lt;/a&gt; on September 25, 2017. With cri-containerd, users can run Kubernetes clusters using containerd as the underlying runtime without Docker installed.&lt;/p&gt;

&lt;h2 id=&#34;containerd&#34;&gt;containerd&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://containerd.io/&#34; target=&#34;_blank&#34;&gt;Containerd&lt;/a&gt; is an &lt;a href=&#34;https://www.opencontainers.org/&#34; target=&#34;_blank&#34;&gt;OCI&lt;/a&gt; compliant core container runtime designed to be embedded into larger systems. It provides the minimum set of functionality to execute containers and manages images on a node. It was initiated by Docker Inc. and &lt;a href=&#34;https://www.cncf.io/announcement/2017/03/29/containerd-joins-cloud-native-computing-foundation/&#34; target=&#34;_blank&#34;&gt;donated to CNCF&lt;/a&gt; in March of 2017. The Docker engine itself is built on top of earlier versions of containerd, and will soon be updated to the newest version. Containerd is close to a feature complete stable release, with &lt;a href=&#34;https://github.com/containerd/containerd/releases/tag/v1.0.0-beta.1&#34; target=&#34;_blank&#34;&gt;1.0.0-beta.1&lt;/a&gt; available right now.&lt;/p&gt;

&lt;p&gt;Containerd has a much smaller scope than Docker, provides a golang client API, and is more focused on being embeddable.The smaller scope results in a smaller codebase that’s easier to maintain and support over time, matching Kubernetes requirements as shown in the following table:&lt;/p&gt;

&lt;p&gt;| | Containerd Scope (In/Out) | Kubernetes Requirement |
|-|-|-|
| Container Lifecycle Management | In | Container Create/Start/Stop/Delete/List/Inspect (✔️) |
| Image Management | In | Pull/List/Inspect (✔️) |
| Networking | Out  No concrete network solution. User can setup network namespace and put containers into it. | Kubernetes networking deals with pods, rather than containers, so container runtimes should not provide complex networking solutions that  don&amp;rsquo;t satisfy requirements. (✔️) |
| Volumes | Out, No volume management. User can setup host path, and mount it into container. |Kubernetes manages volumes. Container runtimes should not provide internal volume management that may conflict with Kubernetes. (✔️) |
| Persistent Container Logging | Out, No persistent container log. Container STDIO is provided as FIFOs, which can be redirected/decorated as is required. | Kubernetes has specific requirements for persistent container logs, such as format and path etc. Container runtimes should not &amp;nbsp;persist an unmanageable container log. (✔️) |
| Metrics | In  Containerd provides container and snapshot metrics as part of the API. | Kubernetes expects container runtime to provide container metrics (CPU, Memory, writable layer size, etc.) and image filesystem usage (disk, inode usage, etc.). (✔️) |
Overall, from a technical perspective, containerd is a very good alternative container runtime for Kubernetes.|&lt;/p&gt;

&lt;h2 id=&#34;cri-containerd&#34;&gt;cri-containerd&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd&#34; target=&#34;_blank&#34;&gt;Cri-containerd&lt;/a&gt; is exactly that: an implementation of CRI for containerd. It operates on the same node as the Kubelet and containerd. Layered between Kubernetes and containerd, cri-containerd handles all CRI service requests from the Kubelet and uses containerd to manage containers and container images. Cri-containerd manages these service requests in part by forming containerd service requests while adding sufficient additional function to support the CRI requirements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/4NGAPzwhkL0GTNjkAEFN9iWX_Wc0ZE-AZxAxEw4E5aOntuGmv764b3ZYQUyapSnP9BrlUs2rUyo5kiCrj5QuiMHw3-dz2vPUDma029Qt3tej9QABEHFSsOBsq6LjLfFhTBgMhAAc&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Compared with the current Docker CRI implementation (&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim&#34; target=&#34;_blank&#34;&gt;dockershim&lt;/a&gt;), cri-containerd eliminates an extra hop in the stack, making the stack more stable and efficient.&lt;/p&gt;

&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;Cri-containerd uses containerd to manage the full container lifecycle and all container images. As also shown below, cri-containerd manages pod networking via &lt;a href=&#34;https://github.com/containernetworking/cni&#34; target=&#34;_blank&#34;&gt;CNI&lt;/a&gt; (another CNCF project).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/sfkhKO3jiLZ9_TtPpxTsKxkbe1KHg1nrfqkbJYrjN2DbNQE_y31NJVSyDIXe0oQjSwVcQ4gFCyr1MZ9_V4GZuuiHwuU3Pq6ldpRhcRiiuTJaRVuezPK9KFLKovP8mQ6sXTYF_eru&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s use an example to demonstrate how cri-containerd works for the case when Kubelet creates a single-container pod:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;1.Kubelet calls cri-containerd, via the CRI runtime service API, to create a pod;&lt;/li&gt;
&lt;li&gt;2.cri-containerd uses containerd to create and start a special &lt;a href=&#34;https://www.ianlewis.org/en/almighty-pause-container&#34; target=&#34;_blank&#34;&gt;pause container&lt;/a&gt; (the &lt;em&gt;sandbox container&lt;/em&gt;) and put that container inside the pod’s cgroups and namespace (steps omitted for brevity);&lt;/li&gt;
&lt;li&gt;3.cri-containerd configures the pod’s network namespace using CNI;&lt;/li&gt;
&lt;li&gt;4.Kubelet subsequently calls cri-containerd, via the CRI image service API, to pull the application container image;&lt;/li&gt;
&lt;li&gt;5.cri-containerd further uses containerd to pull the image if the image is not present on the node;&lt;/li&gt;
&lt;li&gt;6.Kubelet then calls cri-containerd, via the CRI runtime service API, to create and start the application container inside the pod using the pulled container image;&lt;/li&gt;
&lt;li&gt;7.cri-containerd finally calls containerd to create the application container, put it inside the pod’s cgroups and namespace, then to start the pod’s new application container.
After these steps, a pod and its corresponding application container is created and running.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;status&#34;&gt;Status&lt;/h2&gt;

&lt;p&gt;Cri-containerd v1.0.0-alpha.0 was released on Sep. 25, 2017.&lt;/p&gt;

&lt;p&gt;It is feature complete. All Kubernetes features are supported.&lt;/p&gt;

&lt;p&gt;All &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/cri-validation.md&#34; target=&#34;_blank&#34;&gt;CRI validation test&lt;/a&gt;s have passed. (A CRI validation is a test framework for validating whether a CRI implementation meets all the requirements expected by Kubernetes.)&lt;/p&gt;

&lt;p&gt;All regular &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-node-tests.md&#34; target=&#34;_blank&#34;&gt;node e2e test&lt;/a&gt;s have passed. (The Kubernetes test framework for testing Kubernetes node level functionalities such as managing pods, mounting volumes etc.)&lt;/p&gt;

&lt;p&gt;To learn more about the v1.0.0-alpha.0 release, see the &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd/releases/tag/v1.0.0-alpha.0&#34; target=&#34;_blank&#34;&gt;project repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;try-it-out&#34;&gt;Try it Out&lt;/h2&gt;

&lt;p&gt;For a multi-node cluster installer and bring up steps using ansible and kubeadm, see &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd/blob/master/contrib/ansible/README.md&#34; target=&#34;_blank&#34;&gt;this repo link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For creating a cluster from scratch on Google Cloud, see &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kubernetes the Hard Way&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For a custom installation from release tarball, see &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd/blob/master/docs/installation.md&#34; target=&#34;_blank&#34;&gt;this repo link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For a installation with LinuxKit on a local VM, see &lt;a href=&#34;https://github.com/linuxkit/linuxkit/tree/master/projects/kubernetes&#34; target=&#34;_blank&#34;&gt;this repo link&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We are focused on stability and usability improvements as our next steps.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Stability:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set up a full set of Kubernetes integration test in the Kubernetes test infrastructure on various OS distros such as Ubuntu, COS (&lt;a href=&#34;https://cloud.google.com/container-optimized-os/docs/&#34; target=&#34;_blank&#34;&gt;Container-Optimized OS&lt;/a&gt;) etc.&lt;/li&gt;
&lt;li&gt;Actively fix any test failures and other issues reported by users.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Usability:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Improve the user experience of &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-tools/blob/master/docs/crictl.md&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;crictl&lt;/em&gt;&lt;/a&gt;. Crictl is a portable command line tool for all CRI container runtimes. The goal here is to make it easy to use for debug and development scenarios.&lt;/li&gt;
&lt;li&gt;Integrate cri-containerd with &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/gce/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;kube-up.sh&lt;/em&gt;&lt;/a&gt;, to help users bring up a production quality Kubernetes cluster using cri-containerd and containerd.&lt;/li&gt;
&lt;li&gt;Improve our documentation for users and admins alike.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We plan to release our v1.0.0-beta.0 by the end of 2017.&lt;/p&gt;

&lt;h2 id=&#34;contribute&#34;&gt;Contribute&lt;/h2&gt;

&lt;p&gt;Cri-containerd is a Kubernetes incubator project located at &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-incubator/cri-containerd&lt;/a&gt;. Any contributions in terms of ideas, issues, and/or fixes are welcome. The &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd#getting-started-for-developers&#34; target=&#34;_blank&#34;&gt;getting started guide for developers&lt;/a&gt; is a good place to start for contributors.&lt;/p&gt;

&lt;h2 id=&#34;community&#34;&gt;Community&lt;/h2&gt;

&lt;p&gt;Cri-containerd is developed and maintained by the Kubernetes SIG-Node community. We’d love to hear feedback from you. To join the community:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node&#34; target=&#34;_blank&#34;&gt;sig-node community site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Slack: #sig-node channel in Kubernetes (&lt;a href=&#34;http://kubernetes.slack.com/&#34; target=&#34;_blank&#34;&gt;kubernetes.slack.com&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Mailing List: &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-node&#34; target=&#34;_blank&#34;&gt;https://groups.google.com/forum/#!forum/kubernetes-sig-node&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes the Easy Way </title>
      <link>https://kubernetes.io/blog/2017/11/kubernetes-easy-way/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/11/kubernetes-easy-way/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor&amp;rsquo;s note: Today&amp;rsquo;s post is by Dan Garfield, VP of Marketing at Codefresh, on how to set up and easily deploy a Kubernetes cluster.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kelsey Hightower wrote an invaluable guide for Kubernetes called &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kubernetes the Hard Way&lt;/a&gt;. It’s an awesome resource for those looking to understand the ins and outs of Kubernetes—but what if you want to put Kubernetes on easy mode? That’s something we’ve been working on together with Google Cloud. In this guide, we’ll show you how to get a cluster up and running, as well as how to actually deploy your code to that cluster and run it.&lt;/p&gt;

&lt;p&gt;This is Kubernetes the easy way.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;what-we-ll-accomplish&#34;&gt;What We’ll Accomplish&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;1.Set up a cluster&lt;/li&gt;
&lt;li&gt;2.Deploy an application to the cluster&lt;/li&gt;
&lt;li&gt;3.Automate deployment with rolling updates&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A containerized application&lt;/li&gt;
&lt;li&gt;You can also use &lt;a href=&#34;https://github.com/containers101/demochat&#34; target=&#34;_blank&#34;&gt;a demo app&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;a href=&#34;https://cloud.google.com/?utm_source=kubernetes.io&amp;amp;utm_medium=codefresh-easy-mode&#34; target=&#34;_blank&#34;&gt;Google Cloud Account&lt;/a&gt; or a Kubernetes cluster on another provider&lt;/li&gt;
&lt;li&gt;Everything after Cluster creation is identical with all providers.&lt;/li&gt;
&lt;li&gt;A free account on &lt;a href=&#34;https://codefresh.io/kubernetes-deploy/&#34; target=&#34;_blank&#34;&gt;Codefresh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Codefresh is a service that handles Kubernetes deployment configuration and automation.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We made Codefresh free for open-source projects and offer 200 builds/mo free for private projects, to make adopting Kubernetes as easy as possible. Deploy as much as you like on as many clusters as you like.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;set-up-a-cluster&#34;&gt;Set Up a Cluster&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Create an account at &lt;a href=&#34;https://cloud.google.com/?utm_source=kubernetes.io&amp;amp;utm_medium=codefresh-easy-mode&#34; target=&#34;_blank&#34;&gt;cloud.google.com&lt;/a&gt; and log in.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you’re using a Cluster outside of Google Cloud, you can skip this step.&lt;/p&gt;

&lt;p&gt;Google Container Engine is Google Cloud’s managed Kubernetes service. In our testing, it’s both powerful and easy to use.&lt;/p&gt;

&lt;p&gt;If you’re new to the platform, you can get a $500 credit at the end of this process.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Open the menu and scroll down to &lt;strong&gt;Container Engine&lt;/strong&gt;. Then select &lt;strong&gt;Container Clusters&lt;/strong&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/dqvtK-xyGelr_LW3qlFiamYRrpiq633R68cKitrbCZPtDY_uLBF7R7_PGVNvWja24_mG74vDBzpXddYhbRNeyBGPbQ_yfCq367Zp7eJZoiJEWurFWdmJ0AJlNJJ9TzDivE-8Ak9E&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Click &lt;strong&gt;Create cluster.&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We’re done with step 1. In my experience it usually takes less than 5 minutes for a cluster to be created.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;deploy-an-application-to-kubernetes&#34;&gt;Deploy an Application to Kubernetes&lt;/h2&gt;

&lt;p&gt;First go to &lt;a href=&#34;https://codefresh.io/kubernetes-deploy/&#34; target=&#34;_blank&#34;&gt;Codefresh and create an account using Github, Bitbucket, or Gitlab&lt;/a&gt;. As mentioned previously, Codefresh is free for both open source and smaller private projects. We’ll use it to create the configuration Yaml necessary to deploy our application to Kubernetes. Then we&amp;rsquo;ll deploy our application and automate the process to happen every time we commit code changes. Here are the steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;1.Create a Codefresh account&lt;/li&gt;
&lt;li&gt;2.Connect to Google Cloud (or other cluster)&lt;/li&gt;
&lt;li&gt;3.Add Cluster&lt;/li&gt;
&lt;li&gt;4.Deploy static image&lt;/li&gt;
&lt;li&gt;5.Build and deploy an image&lt;/li&gt;
&lt;li&gt;6.Automate the process&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;connect-to-google-cloud&#34;&gt;Connect to Google Cloud&lt;/h3&gt;

&lt;p&gt;To connect your Clusters in Google Container Engine, go to &lt;em&gt;Account Settings &amp;gt; Integrations &amp;gt; Kubernetes&lt;/em&gt; and click &lt;strong&gt;Authenticate&lt;/strong&gt;. This prompts you to login with your Google credentials.&lt;/p&gt;

&lt;p&gt;Once you log in, all of your clusters are available within Codefresh.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/edYv9DtPymvBBN37KdjUUkhkA9Cy7tZmGMw5V94XEWkesGh9xlOn3O7f6MdsmzKlF75KPM908CXLd9i3bbJCfgZ4BpGy6WvL_l1ADu9tWSIdm9l_uUiB0lPLyvCk1d1FCu2fLc0f&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;add-cluster&#34;&gt;Add Cluster&lt;/h3&gt;

&lt;p&gt;To add your cluster, click the down arrow, and then click &lt;strong&gt;add cluste&lt;/strong&gt; r, select the project and cluster name. You can now deploy images!&lt;/p&gt;

&lt;h3 id=&#34;optional-use-an-alternative-cluster&#34;&gt;Optional: Use an Alternative Cluster&lt;/h3&gt;

&lt;p&gt;To connect a non-GKE cluster we’ll need to add a token and certificate to Codefresh. Go to &lt;em&gt;Account Settings (bottom left) &amp;gt; Integrations &amp;gt; Kubernetes &amp;gt; Configure &amp;gt; Add Provider &amp;gt; Custom Providers&lt;/em&gt;. Expand the dropdown and click &lt;strong&gt;Add Cluster&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/UNXfErkrIV-eoyAi3DS9zkRm8Awk7wMTpIQZssrscKY6hehDo63jzvkBYAdgD3fXJXgcDApi4z5dHI5S99Nk6YbvUVUQU_6hC7qRZ-9Y828k-N86f23OOSG04CXvlTWDE9XDIWhd&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Follow the instructions on how to generate the needed information and click Save. Your cluster now appears under the Kubernetes tab.&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&#34;deploy-static-image-to-kubernetes&#34;&gt;Deploy Static Image to Kubernetes&lt;/h3&gt;

&lt;p&gt;Now for the fun part! Codefresh provides an easily modifiable boilerplate that takes care of the heavy lifting of configuring Kubernetes for your application.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Click on the &lt;strong&gt;Kubernetes&lt;/strong&gt; tab: this shows a list of namespaces.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Think of namespaces as acting a bit like VLANs on a Kubernetes cluster. Each namespace can contain all the services that need to talk to each other on a Kubernetes cluster. For now, we’ll just work off the default namespace (the easy way!).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Click &lt;strong&gt;Add Service&lt;/strong&gt; and fill in the details.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can use the &lt;a href=&#34;https://github.com/containers101/demochat&#34; target=&#34;_blank&#34;&gt;demo application I mentioned earlier&lt;/a&gt; that has a Node.js frontend with a MongoDB.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/YzQzEdIMwWt3lGR9Q4RTELvaB_fYYo2QKqkeXhfTCDnIVX4FBx_quYNgAbo6Wc_wpk0anl7Co3RDwDWnrOyibog9V9DISOZYQqiFE9T4ErlDYuqOGWiRw3-zk4p4WcURaOVg3Dkn&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s the info we need to pass:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cluster&lt;/strong&gt; - This is the cluster we added earlier, our application will be deployed there.&lt;br /&gt;
&lt;strong&gt;Namespace&lt;/strong&gt; - We’ll use default for our namespace but you can create and use a new one if you’d prefer. Namespaces are discrete units for grouping all the services associated with an application.&lt;br /&gt;
&lt;strong&gt;Service name&lt;/strong&gt; - You can name the service whatever you like. Since we’re deploying Mongo, I’ll just name it mongo!&lt;br /&gt;
&lt;strong&gt;Expose port&lt;/strong&gt; - We don’t need to expose the port outside of our cluster so we won’t check the box for now but we will specify a port where other containers can talk to this service. Mongo’s default port is ‘27017’.&lt;br /&gt;
&lt;strong&gt;Image&lt;/strong&gt; - Mongo is a public image on Dockerhub, so I can reference it by name and tag, ‘mongo:latest’.&lt;br /&gt;
&lt;strong&gt;Internal Ports&lt;/strong&gt; - This is the port the mongo application listens on, in this case it’s ‘27017’ again.&lt;/p&gt;

&lt;p&gt;We can ignore the other options for now.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Scroll down and click &lt;strong&gt;Deploy&lt;/strong&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/4dhWXsf0BhyDDyB6XmmuCo2RCNztTPNuy36lYuzAHEYsFmKKkS6ibbKKo3sIqyQIYNTsTE6m5fjtlnEB0gmYoeQ40DZjwuSVyO4-pQKPjZflDT75NZ61aytXnEhFiAUHUDk9l1Wj&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Boom! You’ve just deployed this image to Kubernetes. You can see by clicking on the status that the service, deployment, replicas, and pods are all configured and running. If you click Edit &amp;gt; Advanced, you can see and edit all the raw YAML files associated with this application, or copy them and put them into your repository for use on any cluster.&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&#34;build-and-deploy-an-image&#34;&gt;Build and Deploy an Image&lt;/h3&gt;

&lt;p&gt;To get the rest of our demo application up and running we need to build and deploy the Node.js portion of the application. To do that we’ll need to add our repository to Codefresh.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Click on &lt;em&gt;Repositories &amp;gt; Add Repository&lt;/em&gt;, then copy and paste the &lt;a href=&#34;https://github.com/containers101/demochat&#34; target=&#34;_blank&#34;&gt;demochat repo url&lt;/a&gt; (or use your own repo).&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/Mbs04O7PFJ6yFRRmPo2PDs3MU5IyKq53jrgSB6Xcm1Ki8eStJacoRsPDqv5_m92E0Ki-r-hi_4nbaAqUKRXNE57-TJbmacM3vqrkwM-3ASuBGmmugGc-QkHgfQrRSuAzCP60bSzA&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We have the option to use a dockerfile, or to use a template if we need help creating a dockerfile. In this case, the demochat repo already has a dockerfile so we’ll select that. Click through the next few screens until the image builds.&lt;/p&gt;

&lt;p&gt;Once the build is finished the image is automatically saved inside of the Codefresh docker registry. You can also add any &lt;a href=&#34;https://docs.codefresh.io/v1.0/docs/docker-registry&#34; target=&#34;_blank&#34;&gt;other registry to your account&lt;/a&gt; and use that instead.&lt;/p&gt;

&lt;p&gt;To deploy the image we’ll need&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a pull secret&lt;/li&gt;
&lt;li&gt;the image name and registry&lt;/li&gt;
&lt;li&gt;the ports that will be used&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;creating-the-pull-secret&#34;&gt;Creating the Pull Secret&lt;/h3&gt;

&lt;p&gt;The pull secret is a token that the Kubernetes cluster can use to access a private Docker registry. To create one, we’ll need to generate the token and save it to Codefresh.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;strong&gt;User Settings&lt;/strong&gt; (bottom left) and generate a new token.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the token to your clipboard.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/fJxTvuK0b-ssLls87EgSccmpZoRk_KXTQdxOglvgKlPHlc6pr-yNBht4rKYyLcFF7SERS2czWLSh_YUNGOy7Q9UjQqlGNKJdmG1uyDpVr_IIx3BqsauxfXnIrEtQbdXKAOg-nfr3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Go to &lt;em&gt;Account Settings &amp;gt; Integrations &amp;gt; Docker Registry &amp;gt; Add Registry&lt;/em&gt; and select &lt;strong&gt;Codefresh Registry&lt;/strong&gt;. Paste in your token and enter your username (entry is case sensitive). Your username must match your name displayed at the bottom left of the screen.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Test and save it.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We’ll now be able to create our secret later on when we deploy our image.&lt;/p&gt;

&lt;h3 id=&#34;get-the-image-name&#34;&gt;Get the image name&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Click on &lt;strong&gt;Images&lt;/strong&gt; and open the image you just built. Under &lt;em&gt;Comment&lt;/em&gt; you’ll see the image name starting with r.cfcr.io.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/5XciQfEpUxYZp6Tuic3TWFOkXi5I_x-16i9yXkpAMn4BRC54Rh7Hic4yM5Feo6A65jArBQyXfIgexTZ9rp-mM6l9Rmu4fm3aeE48x98veKN4_39j3hkRVm8goLTaWX0U9KgJuYIi&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Copy the image name; we’ll need to paste it in later.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;deploy-the-private-image-to-kubernetes&#34;&gt;Deploy the private image to Kubernetes&lt;/h3&gt;

&lt;p&gt;We’re now ready to deploy the image we built.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Go to the Kubernetes page and, like we did with mongo, click Add Service and fill out the page. Make sure to select the same namespace you used to deploy mongo earlier.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/reUappaYGmp27xL32HFg6OWHRZfw60o5fUxTII7jrUUmGN4lqNrEaPW8Dl5RHK-N4nOCSOTe-9A6Y0HIiSzPxyCceOzOmrNeTB_QGKRfyI5EnpTM7mT-neGsBwYx-zn4BETgN8Nz&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let’s expose the port so we can access this application. This provisions an IP address and automatically configures ingress.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Click &lt;strong&gt;Deploy&lt;/strong&gt; : your application will be up and running within a few seconds! The IP address may take longer to provision depending on your cluster location.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/nGiPsfscMpcvxfjqseEH5Ft2K3yzvT93ZW3vVJtg_QF3gN_-ndMnZ4Kpcz_WqIr76irCwaBFr7Du6mzVGYYgHxgZFdBNi3hWWW5UWFtnvhyEq2DDM8zCIEXKTo84gjGCOsvenp1r&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From this view you can scale the replicas, see application status, and similar tasks.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Click on the IP address to view the running application.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/GmZYxhd4tgEJONm8MBIY_m1rfOH05_LxCwpnbrFk013pNEIMAcNGsuPqR5DfFevjbTYAKTRqj4aXhwxowXM5D7p5KjBLqZ0YyTP226Awl2BC6MdBXwfb3E-HEAZTI_MlEEkBu5oC&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point you should have your entire application up and running! Not so bad huh? Now to automate deployment!&lt;/p&gt;

&lt;h3 id=&#34;automate-deployment-to-kubernetes&#34;&gt;Automate Deployment to Kubernetes&lt;/h3&gt;

&lt;p&gt;Every time we make a change to our application, we want to build a new image and deploy it to our cluster. We’ve already set up automated builds, but to automate deployment:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;strong&gt;Repositories&lt;/strong&gt; (top left).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click on the pipeline for the demochat repo (the gear icon).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/dD_Dn5SgpSSqTfIeHep4QKhx6rM8zcTWQVR-wHwBWLMzeZ9vsueS320yOeH_nuaKSYlSwrSB3UhML0wcLYZeoCPtga9mvvpyShutYoVKNtZ16e9ZDvglHDiOqugXunkDstUPF_aV&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It’s a good idea to run some tests before deploying. Under &lt;em&gt;Build and Unit Test&lt;/em&gt;, add npm test for the unit test script.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;Deploy Script&lt;/strong&gt; and select &lt;strong&gt;Kubernetes (Beta)&lt;/strong&gt;. Enter the information for the service you’ve already deployed.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/an0aib6sTTqZqhfjMjOGFcWRRcaQSjezjk9XHVxEsLr_6hWi0kslsgQR6D0gP3EiA8D4pON-BhmakaRhpFVCkH16F80jt2-EWAJki4i2u4fYRQSdumiihC5fDjUyOyC9rwm1QilT&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the option to use a deployment file from your repo, or to use the deployment file that you just generated.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You’re done with deployment automation! Now whenever a change is made, the image will build, test, and deploy.&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;We want to make it easy for every team, not just big enterprise teams, to adopt Kubernetes while preserving all of Kubernetes’ power and flexibility. At any point on the Kubernetes service screen you can switch to YAML to view all of the YAMLfiles generated by the configuration you performed in this walkthrough. You can tweak the file content, copy and paste them into local files, etc.&lt;/p&gt;

&lt;p&gt;This walkthrough gives everyone a solid base to start with. When you’re ready, you can tweak the entities directly to specify the exact configuration you’d like.&lt;/p&gt;

&lt;p&gt;We’d love your feedback! Please share with us on &lt;a href=&#34;https://twitter.com/codefresh&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;, or &lt;a href=&#34;https://codefresh.io/contact-us/&#34; target=&#34;_blank&#34;&gt;reach out directly&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;addendums&#34;&gt;Addendums&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Do you have a video to walk me through this?&lt;/strong&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=oFwFuUxxFdI&amp;amp;list=PL8mgsmlx4BWV_j_L5oq-q8JdPnlJc3bUv&#34; target=&#34;_blank&#34;&gt;You bet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Does this work with Helm Charts?&lt;/strong&gt; Yes! We’re currently piloting Helm Charts with a limited set of users. Ping us if you’d like to try it early.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Does this work with any Kubernetes cluster?&lt;/strong&gt; It should work with any Kubernetes cluster and is tested for Kubernetes 1.5 forward.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Can I deploy Codefresh in my own data center?&lt;/strong&gt; Sure, Codefresh is built on top of Kubernetes using Helm Charts. Codefresh cloud is free for open source, and 200 builds/mo. Codefresh on prem is currently for enterprise users only.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Won’t the database be wiped every time we update?&lt;/strong&gt; Yes, in this case we skipped creating a persistent volume. It’s a bit more work to get the persistent volume configured, if you’d like, &lt;a href=&#34;https://codefresh.io/contact-us/&#34; target=&#34;_blank&#34;&gt;feel free to reach out&lt;/a&gt; and we’re happy to help!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Enforcing Network Policies in Kubernetes </title>
      <link>https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Editor&amp;rsquo;s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what&amp;rsquo;s new in Kubernetes 1.8. Today’s post comes from Ahmet Alp Balkan, Software Engineer, Google.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes now offers functionality to enforce rules about which pods can communicate with each other using &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34; target=&#34;_blank&#34;&gt;network policies&lt;/a&gt;. This feature is has become stable Kubernetes 1.7 and is ready to use with supported networking plugins. The Kubernetes 1.8 release has added better capabilities to this feature.&lt;/p&gt;

&lt;h2 id=&#34;network-policy-what-does-it-mean&#34;&gt;Network policy: What does it mean?&lt;/h2&gt;

&lt;p&gt;In a Kubernetes cluster configured with default settings, all pods can discover and communicate with each other without any restrictions. The new Kubernetes object type NetworkPolicy lets you allow and block traffic to pods.&lt;/p&gt;

&lt;p&gt;If you’re running multiple applications in a Kubernetes cluster or sharing a cluster among multiple teams, it’s a security best practice to create firewalls that permit pods to talk to each other while blocking other network traffic. Networking policy corresponds to the Security Groups concepts in the Virtual Machines world.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-add-network-policy-to-my-cluster&#34;&gt;How do I add Network Policy to my cluster?&lt;/h2&gt;

&lt;p&gt;Networking Policies are implemented by networking plugins. These plugins typically install an overlay network in your cluster to enforce the Network Policies configured. A number of networking plugins, including &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/calico-network-policy/&#34; target=&#34;_blank&#34;&gt;Calico&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/romana-network-policy/&#34; target=&#34;_blank&#34;&gt;Romana&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/weave-network-policy/&#34; target=&#34;_blank&#34;&gt;Weave Net&lt;/a&gt;, support using Network Policies.&lt;/p&gt;

&lt;p&gt;Google Container Engine (GKE) also provides beta support for &lt;a href=&#34;https://cloud.google.com/container-engine/docs/network-policy&#34; target=&#34;_blank&#34;&gt;Network Policies&lt;/a&gt; using the Calico networking plugin when you create clusters with the following command:&lt;/p&gt;

&lt;p&gt;gcloud beta container clusters create &amp;ndash;enable-network-policy&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-configure-a-network-policy&#34;&gt;How do I configure a Network Policy?&lt;/h2&gt;

&lt;p&gt;Once you install a networking plugin that implements Network Policies, you need to create a Kubernetes resource of type NetworkPolicy. This object describes two set of label-based pod selector fields, matching:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;a set of pods the network policy applies to (required)&lt;/li&gt;
&lt;li&gt;a set of pods allowed access to each other (optional). If you omit this field, it matches to no pods; therefore, no pods are allowed. If you specify an empty pod selector, it matches to all pods; therefore, all pods are allowed.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;example-restricting-traffic-to-a-pod&#34;&gt;Example: restricting traffic to a pod&lt;/h2&gt;

&lt;p&gt;The following example of a network policy blocks all in-cluster traffic to a set of web server pods, except the pods allowed by the policy configuration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/e8JzhKYICOzh44sHcedjt4IRRpw2zpFNbJ2UY83fBdWYCIvFVSlHJNmIwLzIHVxrScc2eNCyv37mm903TVT9VkMuHPxe_5Hk8CvJTqGsSK7WtEDCbn1Q25S-o_kHcEiKUUl1NV9g&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To achieve this setup, create a NetworkPolicy with the following manifest:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: NetworkPolicy

apiVersion: networking.k8s.io/v1

metadata:

  name: access-nginx

spec:

  podSelector:

    matchLabels:

      app: nginx

  ingress:

  - from:

    - podSelector:

        matchLabels:

          app: foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you apply this configuration, only pods with label &lt;strong&gt;app: foo&lt;/strong&gt; can talk to the pods with the label &lt;strong&gt;app: nginx&lt;/strong&gt;. For a more detailed tutorial, see the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/&#34; target=&#34;_blank&#34;&gt;Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;example-restricting-traffic-between-all-pods-by-default&#34;&gt;Example: restricting traffic between all pods by default&lt;/h2&gt;

&lt;p&gt;If you specify the spec.podSelector field as empty, the set of pods the network policy matches to all pods in the namespace, blocking all traffic between pods by default. In this case, you must explicitly create network policies whitelisting all communication between the pods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/FYmu74F7fW7DabtzBd6PULsgzKz0WmCli2Sw0SW8zVr0U7m-P6eGvov0mZGv9ngxncGXJmPxzapL3yQXXSBKTHsI8zw5kh-2hqzK6fW7YuqU6X5ofb5ilbis2KUJ2HvF3IHXsMcK&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can enable a policy like this by applying the following manifest in your Kubernetes cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.k8s.io/v1

kind: NetworkPolicy

metadata:

  name: default-deny

spec:

  podSelector:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;other-network-policy-features&#34;&gt;Other Network Policy features&lt;/h2&gt;

&lt;p&gt;In addition to the previous examples, you can make the Network Policy API enforce more complicated rules:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Egress network policies: Introduced in Kubernetes 1.8, you can restrict your workloads from establishing connections to resources outside specified IP ranges.&lt;/li&gt;
&lt;li&gt;IP blocks support: In addition to using podSelector/namespaceSelector, you can specify IP ranges with CIDR blocks to allow/deny traffic in ingress or egress rules.&lt;/li&gt;
&lt;li&gt;Cross-namespace policies: Using the ingress.namespaceSelector field, you can enforce Network Policies for particular or for all namespaces in the cluster. For example, you can create privileged/system namespaces that can communicate with pods even though the default policy is to block traffic.&lt;/li&gt;
&lt;li&gt;Restricting traffic to port numbers: Using the ingress.ports field, you can specify port numbers for the policy to enforce. If you omit this field, the policy matches all ports by default. For example, you can use this to allow a monitoring pod to query only the monitoring port number of an application.&lt;/li&gt;
&lt;li&gt;Multiple ingress rules on a single policy: Because spec.ingress field is an array, you can use the same NetworkPolicy object to give access to different ports using different pod selectors. For example, a NetworkPolicy can have one ingress rule giving pods with the kind: monitoring label access to port 9000, and another ingress rule for the label app: foo giving access to port 80, without creating an additional NetworkPolicy resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;learn-more&#34;&gt;Learn more&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Read more: &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34; target=&#34;_blank&#34;&gt;Networking Policy documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read more: &lt;a href=&#34;https://ahmet.im/blog/kubernetes-network-policy/&#34; target=&#34;_blank&#34;&gt;Unofficial Network Policy Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hands-on: &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/&#34; target=&#34;_blank&#34;&gt;Declare a Network Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Try: &lt;a href=&#34;https://github.com/ahmetb/kubernetes-networkpolicy-tutorial&#34; target=&#34;_blank&#34;&gt;Network Policy Recipes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Using RBAC, Generally Available in Kubernetes v1.8 </title>
      <link>https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18/</link>
      <pubDate>Sat, 28 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor&amp;rsquo;s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what&amp;rsquo;s new in Kubernetes 1.8. Today’s post comes from Eric Chiang, software engineer, CoreOS, and SIG-Auth co-lead.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 1.8 represents a significant milestone for the &lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/&#34; target=&#34;_blank&#34;&gt;role-based access control (RBAC) authorizer&lt;/a&gt;, which was promoted to GA in this release. RBAC is a mechanism for controlling access to the Kubernetes API, and since its &lt;a href=&#34;https://kubernetes.io/blog/2017/04/rbac-support-in-kubernetes&#34; target=&#34;_blank&#34;&gt;beta in 1.6&lt;/a&gt;, many Kubernetes clusters and provisioning strategies have enabled it by default.&lt;/p&gt;

&lt;p&gt;Going forward, we expect to see RBAC become a fundamental building block for securing Kubernetes clusters. This post explores using RBAC to manage user and application access to the Kubernetes API.&lt;/p&gt;

&lt;h2 id=&#34;granting-access-to-users&#34;&gt;Granting access to users&lt;/h2&gt;

&lt;p&gt;RBAC is configured using standard Kubernetes resources. Users can be bound to a set of roles (ClusterRoles and Roles) through bindings (ClusterRoleBindings and RoleBindings). Users start with no permissions and must explicitly be granted access by an administrator.&lt;/p&gt;

&lt;p&gt;All Kubernetes clusters install a default set of ClusterRoles, representing common buckets users can be placed in. The “edit” role lets users perform basic actions like deploying pods; “view” lets a user observe non-sensitive resources; “admin” allows a user to administer a namespace; and “cluster-admin” grants access to administer a cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
$ kubectl get clusterroles

NAME            AGE

admin           40m

cluster-admin   40m

edit            40m

# ...


view            40m

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ClusterRoleBindings grant a user, group, or service account a ClusterRole’s power across the entire cluster. Using kubectl, we can let a sample user “jane” perform basic actions in all namespaces by binding her to the “edit” ClusterRole:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
$ kubectl create clusterrolebinding jane --clusterrole=edit --user=jane

$ kubectl get namespaces --as=jane

NAME          STATUS    AGE

default       Active    43m

kube-public   Active    43m

kube-system   Active    43m

$ kubectl auth can-i create deployments --namespace=dev --as=jane

yes

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RoleBindings grant a ClusterRole’s power within a namespace, allowing administrators to manage a central list of ClusterRoles that are reused throughout the cluster. For example, as new resources are added to Kubernetes, the default ClusterRoles are updated to automatically grant the correct permissions to RoleBinding subjects within their namespace.&lt;/p&gt;

&lt;p&gt;Next we’ll let the group “infra” modify resources in the “dev” namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
$ kubectl create rolebinding infra --clusterrole=edit --group=infra --namespace=dev

rolebinding &amp;quot;infra&amp;quot; created

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because we used a RoleBinding, these powers only apply within the RoleBinding’s namespace. In our case, a user in the “infra” group can view resources in the “dev” namespace but not in “prod”:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
$ kubectl get deployments --as=dave --as-group=infra --namespace dev

No resources found.

$ kubectl get deployments --as=dave --as-group=infra --namespace prod

Error from server (Forbidden): deployments.extensions is forbidden: User &amp;quot;dave&amp;quot; cannot list deployments.extensions in the namespace &amp;quot;prod&amp;quot;.

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-custom-roles&#34;&gt;Creating custom roles&lt;/h2&gt;

&lt;p&gt;When the default ClusterRoles aren’t enough, it’s possible to create new roles that define a custom set of permissions. Since ClusterRoles are just regular API resources, they can be expressed as YAML or JSON manifests and applied using kubectl.&lt;/p&gt;

&lt;p&gt;Each ClusterRole holds a list of permissions specifying “rules.” Rules are purely additive and allow specific HTTP verb to be performed on a set of resource. For example, the following ClusterRole holds the permissions to perform any action on &amp;ldquo;deployments”, “configmaps,” or “secrets”, and to view any “pod”:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
kind: ClusterRole

apiVersion: rbac.authorization.k8s.io/v1

metadata:

  name: deployer

rules:

- apiGroups: [&amp;quot;apps&amp;quot;]

  resources: [&amp;quot;deployments&amp;quot;]

  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;delete&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;patch&amp;quot;]



- apiGroups: [&amp;quot;&amp;quot;] # &amp;quot;&amp;quot; indicates the core API group

  resources: [&amp;quot;configmaps&amp;quot;, &amp;quot;secrets&amp;quot;]

  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;delete&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;patch&amp;quot;]



- apiGroups: [&amp;quot;&amp;quot;] # &amp;quot;&amp;quot; indicates the core API group

  resources: [&amp;quot;pods&amp;quot;]

  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verbs correspond to the HTTP verb of the request, while the resource and API groups refer to the resource being referenced. Consider the following Ingress resource:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
apiVersion: extensions/v1beta1

kind: Ingress

metadata:

  name: test-ingress

spec:

  backend:

    serviceName: testsvc

    servicePort: 80

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To POST the resource, the user would need the following permissions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
rules:

- apiGroups: [&amp;quot;extensions&amp;quot;] # &amp;quot;apiVersion&amp;quot; without version

  resources: [&amp;quot;ingresses&amp;quot;]  # Plural of &amp;quot;kind&amp;quot;

  verbs: [&amp;quot;create&amp;quot;]         # &amp;quot;POST&amp;quot; maps to &amp;quot;create&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;roles-for-applications&#34;&gt;Roles for applications&lt;/h2&gt;

&lt;p&gt;When deploying containers that require access to the Kubernetes API, it’s good practice to ship an RBAC Role with your application manifests. Besides ensuring your app works on RBAC enabled clusters, this helps users audit what actions your app will perform on the cluster and consider their security implications.&lt;/p&gt;

&lt;p&gt;A namespaced Role is usually more appropriate for an application, since apps are traditionally run inside a single namespace and the namespace&amp;rsquo;s resources should be tied to the lifecycle of the app. However, Roles cannot grant access to non-namespaced resources (such as nodes) or across namespaces, so some apps may still require ClusterRoles.&lt;/p&gt;

&lt;p&gt;The following Role allows a Prometheus instance to monitor and discover services, endpoints, and pods in the “dev” namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
kind: Role

metadata:

  name: prometheus-role

  namespace: dev

rules:

- apiGroups: [&amp;quot;&amp;quot;] # &amp;quot;&amp;quot; refers to the core API group

  Resources: [&amp;quot;services&amp;quot;, &amp;quot;endpoints&amp;quot;, &amp;quot;pods&amp;quot;]

  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Containers running in a Kubernetes cluster receive service account credentials to talk to the Kubernetes API, and service accounts can be targeted by a RoleBinding. Pods normally run with the “default” service account, but it’s good practice to run each app with a unique service account so RoleBindings don’t unintentionally grant permissions to other apps.&lt;/p&gt;

&lt;p&gt;To run a pod with a custom service account, create a ServiceAccount resource in the same namespace and specify the &lt;code&gt;serviceAccountName&lt;/code&gt; field of the manifest.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
apiVersion: apps/v1beta2 # Abbreviated, not a full manifest

kind: Deployment

metadata:

  name: prometheus-deployment

  namespace: dev

spec:

  replicas: 1

  template:

    spec:

      containers:

      - name: prometheus

        image: prom/prometheus:v1.8.0

        command: [&amp;quot;prometheus&amp;quot;, &amp;quot;-config.file=/etc/prom/config.yml&amp;quot;]

    # Run this pod using the &amp;quot;prometheus-sa&amp;quot; service account.

    serviceAccountName: prometheus-sa

---

apiVersion: v1

kind: ServiceAccount

metadata:

  name: prometheus-sa

  namespace: dev

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get involved&lt;/h2&gt;

&lt;p&gt;Development of RBAC is a community effort organized through the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-auth/README.md&#34; target=&#34;_blank&#34;&gt;Auth Special Interest Group&lt;/a&gt;, one of the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;many SIGs&lt;/a&gt; responsible for maintaining Kubernetes. A great way to get involved in the Kubernetes community is to join a SIG that aligns with your interests, provide feedback, and help with the roadmap.&lt;/p&gt;

&lt;h2 id=&#34;about-the-author&#34;&gt;About the author&lt;/h2&gt;

&lt;p&gt;Eric Chiang is a software engineer and technical lead of Kubernetes development at &lt;a href=&#34;https://coreos.com/?utm_source=k8sblog&amp;amp;utm_medium=social&amp;amp;utm_campaign=organic&#34; target=&#34;_blank&#34;&gt;CoreOS&lt;/a&gt;, the creator of Tectonic, the enterprise-ready Kubernetes platform. Eric co-leads Kubernetes SIG Auth and maintains several open source projects and libraries on behalf of CoreOS.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  It Takes a Village to Raise a Kubernetes </title>
      <link>https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor’s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what&amp;rsquo;s new in Kubernetes 1.8, written by Jaice Singer DuMars from Microsoft.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Each time we release a new version of Kubernetes, it’s enthralling to see how the community responds to all of the hard work that went into it. Blogs on new or enhanced capabilities crop up all over the web like wildflowers in the spring. Talks, videos, webinars, and demos are not far behind. As soon as the community seems to take this all in, we turn around and add more to the mix. It’s a thrilling time to be a part of this project, and even more so, the movement. It’s not just software anymore.&lt;/p&gt;

&lt;p&gt;When circumstances opened the door for me to lead the 1.8 release, I signed on despite a minor case of the butterflies. In a private conversation with another community member, they assured me that “being organized, following up, and knowing when to ask for help” were the keys to being a successful lead. That’s when I knew I could do it — and so I did.&lt;/p&gt;

&lt;p&gt;From that point forward, I was wrapped in a patchwork quilt of community that magically appeared at just the right moments. The community’s commitment and earnest passion for quality, consistency, and accountability formed a bedrock from which the release itself was chiseled.&lt;/p&gt;

&lt;p&gt;The 1.8 release team proved incredibly cohesive despite a late start. We approached even the most difficult situations with humor, diligence, and sincere curiosity. My experience leading large teams served me well, and underscored another difference about this release: it was more valuable for me to focus on leadership than diving into the technical weeds to solve every problem.&lt;/p&gt;

&lt;p&gt;Also, the uplifting power of &lt;a href=&#34;https://kubernetes.slack.com/archives/C2C40FMNF/p1506659664000090&#34; target=&#34;_blank&#34;&gt;emoji in Slack&lt;/a&gt; cannot be overestimated.&lt;/p&gt;

&lt;p&gt;An important inflection point is underway in the Kubernetes project. If you’ve taken a ride on a “startup rollercoaster,” this is a familiar story. You come up with an idea so crazy that it might work. You build it, get traction, and slowly clickity-clack up that first big hill. The view from the top is dizzying, as you’ve poured countless hours of life into something completely unknown. Once you go over the top of that hill, everything changes. Breakneck acceleration defines or destroys what has been built.&lt;/p&gt;

&lt;p&gt;In my experience, that zero gravity point is where everyone in the company (or in this case, project) has to get serious about not only building something, but also maintaining it. Without a commitment to maintenance, things go awry really quickly. From codebases that resemble the Winchester Mystery House to epidemics of crashing production implementations, a fiery descent into chaos can happen quickly despite the outward appearance of success. Thankfully, the Kubernetes community seems to be riding our growth rollercoaster with increasing success at each release.&lt;/p&gt;

&lt;p&gt;As software startups mature, there is a natural evolution reflected in the increasing distribution of labor. Explosive adoption means that full-time security, operations, quality, documentation, and project management staff become necessary to deliver stability, reliability, and extensibility. Also, you know things are getting serious when intentional architecture becomes necessary to ensure consistency over time.&lt;/p&gt;

&lt;p&gt;Kubernetes has followed a similar path. In the absence of company departments or skill-specific teams, Special Interest Groups (SIGs) have organically formed around core project needs like storage, networking, API machinery, applications, and the operational lifecycle. As SIGs have proliferated, the Kubernetes governance model has crystallized around them, providing a framework for code ownership and shared responsibility. SIGs also help ensure the community is sustainable because success is often more about people than code.&lt;/p&gt;

&lt;p&gt;At the Kubernetes &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/community/2017-events/05-leadership-summit&#34; target=&#34;_blank&#34;&gt;leadership summit&lt;/a&gt; in June, a proposed SIG architecture was ratified with a unanimous vote, underscoring a stability theme that seemed to permeate every conversation in one way or another. The days of filling in major functionality gaps appear to be over, and a new era of feature depth has emerged in its place.&lt;/p&gt;

&lt;p&gt;Another change is the move away from project-level release “feature themes” to SIG-level initiatives delivered in increments over the course of several releases. That’s an important shift: SIGs have a mission, and everything they deliver should ultimately serve that. As a community, we need to provide facilitation and support so SIGs are empowered to do their best work with minimal overhead and maximum transparency.&lt;/p&gt;

&lt;p&gt;Wisely, the community also spotted the opportunity to provide safe mechanisms for innovation that are increasingly less dependent on the code in kubernetes/kubernetes. This in turn creates a flourishing habitat for experimentation without hampering overall velocity. The project can also address technical debt created during the initial ride up the rollercoaster. However, new mechanisms for innovation present an architectural challenge in defining what is and is not Kubernetes. SIG Architecture addresses the challenge of defining Kubernetes’ boundaries. It’s a work in progress that trends toward continuous improvement.&lt;/p&gt;

&lt;p&gt;This can be a little overwhelming at the individual level. In reality, it’s not that much different from any other successful startup, save for the fact that authority does not come from a traditional org chart. It comes from SIGs, community technical leaders, the newly-formed steering committee, and ultimately you.&lt;/p&gt;

&lt;p&gt;The Kubernetes release process provides a special opportunity to see everything that makes this project tick. I’ll tell you what I saw: people, working together, to do the best they can, in service to everyone who sets out on the cloud native journey.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:   kubeadm v1.8 Released: Introducing Easy Upgrades for Kubernetes Clusters </title>
      <link>https://kubernetes.io/blog/2017/10/kubeadm-v18-released/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/10/kubeadm-v18-released/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor’s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what&amp;rsquo;s new in Kubernetes 1.8&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since its debut in &lt;a href=&#34;https://kubernetes.io/blog/2016/09/how-we-made-kubernetes-easy-to-install&#34; target=&#34;_blank&#34;&gt;September 2016&lt;/a&gt;, the Cluster Lifecycle Special Interest Group (SIG) has established kubeadm as the easiest Kubernetes bootstrap method. Now, we’re releasing kubeadm v1.8.0 in tandem with the release of &lt;a href=&#34;https://kubernetes.io/blog/2017/09/kubernetes-18-security-workloads-and&#34; target=&#34;_blank&#34;&gt;Kubernetes v1.8.0&lt;/a&gt;. In this blog post, I’ll walk you through the changes we’ve made to kubeadm since the last update, the scope of kubeadm, and how you can contribute to this effort.&lt;/p&gt;

&lt;h2 id=&#34;security-first-kubeadm-v1-6-v1-7&#34;&gt;Security first: kubeadm v1.6 &amp;amp; v1.7&lt;/h2&gt;

&lt;p&gt;Previously, we discussed &lt;a href=&#34;https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters&#34; target=&#34;_blank&#34;&gt;planned updates for kubeadm v1.6&lt;/a&gt;. Our primary focus for v1.6 was security. We started enforcing role based access control (RBAC) as it graduated to beta, gave unique identities and locked-down privileges for different system components in the cluster, disabled the insecure &lt;code&gt;localhost:8080&lt;/code&gt; API server port, started authorizing all API calls to the kubelets, and &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cluster-lifecycle/bootstrap-discovery.md&#34; target=&#34;_blank&#34;&gt;improved the token discovery&lt;/a&gt; method used formerly in v1.5. Token discovery (aka Bootstrap Tokens) graduated to beta in v1.8.&lt;/p&gt;

&lt;p&gt;In number of features, kubeadm v1.7.0 was a much smaller release compared to v1.6.0 and v1.8.0. The main additions were enforcing &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/kubelet-authorizer.md&#34; target=&#34;_blank&#34;&gt;the Node Authorizer&lt;/a&gt;, which significantly reduces the attack surface for a Kubernetes cluster, and initial, limited upgrading support from v1.6 clusters.&lt;/p&gt;

&lt;h2 id=&#34;easier-upgrades-extensibility-and-stabilization-in-v1-8&#34;&gt;Easier upgrades, extensibility, and stabilization in v1.8&lt;/h2&gt;

&lt;p&gt;We had eight weeks between Kubernetes v1.7.0 and our stabilization period (code freeze) to implement new features and to stabilize the upcoming v1.8.0 release. Our goal for kubeadm v1.8.0 was to make it more extensible. We wanted to add a lot of new features and improvements in this cycle, and we succeeded.Upgrades along with better introspectability. The most important update in kubeadm v1.8.0 (and my favorite new feature) is &lt;strong&gt;one-command upgrades&lt;/strong&gt; of the control plane. While v1.7.0 had the ability to upgrade clusters, the user experience was far from optimal, and the process was risky.&lt;/p&gt;

&lt;p&gt;Now, you can easily check to see if your system can handle an upgrade by entering:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm upgrade plan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives you information about which versions you can upgrade to, as well as the health of your cluster.&lt;/p&gt;

&lt;p&gt;You can examine the effects an upgrade will have on your system by specifying the &amp;ndash;dry-run flag. In previous versions of kubeadm, upgrades were essentially blind in that you could only make assumptions about how an upgrade would impact your cluster. With the new dry run feature, there is no more mystery. You can see exactly what applying an upgrade would do before applying it.&lt;/p&gt;

&lt;p&gt;After checking to see how an upgrade will affect your cluster, you can apply the upgrade by typing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm upgrade apply v1.8.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a much cleaner and safer way of performing an upgrade than the previous version. As with any type of upgrade or downgrade, it’s a good idea to backup your cluster first using your preferred solution.&lt;/p&gt;

&lt;h2 id=&#34;self-hosting&#34;&gt;Self-hosting&lt;/h2&gt;

&lt;p&gt;Self-hosting in this context refers to a specific way of setting up the control plane. The self-hosting concept was initially developed by CoreOS in their &lt;a href=&#34;https://github.com/kubernetes-incubator/bootkube&#34; target=&#34;_blank&#34;&gt;bootkube&lt;/a&gt; project. The long-term goal is to move this functionality (currently in an alpha stage) to the generic kubeadm toolbox. Self-hosting means that the control plane components, the API Server, Controller Manager and Scheduler are workloads themselves in the cluster they run. This means the control plane components can be managed using Kubernetes primitives, which has numerous advantages. For instance, leader-elected components like the scheduler and controller-manager will automatically be run on all masters when HA is implemented if they are run in a DaemonSet. Rolling upgrades in Kubernetes can be used for upgrades of the control plane components, and next to no extra code has to be written for that to work; it’s one of Kubernetes’ built-in primitives!&lt;/p&gt;

&lt;p&gt;Self-hosting won’t be the default until v1.9.0, but users can easily test the feature in experimental clusters. If you test this feature, we’d love your feedback!&lt;/p&gt;

&lt;p&gt;You can test out self-hosting by enabling its feature gate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm init --feature-gates=SelfHosting=true
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;extensibility&#34;&gt;Extensibility&lt;/h2&gt;

&lt;p&gt;We’ve added some new extensibility features. You can delegate some tasks, like generating certificates or writing control plane arguments to kubeadm, but still drive the control plane bootstrap process yourself. Basically, you can let kubeadm do some parts and fill in yourself where you need customizations. Previously, you could only use kubeadm init to perform “the full meal deal.” The inclusion of the kubeadm alpha phase command supports our aim to make kubeadm more modular, letting you invoke atomic sub-steps of the bootstrap process.&lt;/p&gt;

&lt;p&gt;In v1.8.0, kubeadm alpha phase is just that: an alpha preview. We hope that we can graduate the command to beta as kubeadm phase in v1.9.0. We can’t wait for feedback from the community on how to better improve this feature!&lt;/p&gt;

&lt;h2 id=&#34;improvements&#34;&gt;Improvements&lt;/h2&gt;

&lt;p&gt;Along with our new kubeadm features, we’ve also made improvements to existing ones. The Bootstrap Token feature that makes &lt;code&gt;kubeadm join&lt;/code&gt; so short and sweet has graduated from alpha to beta and gained even more security features.&lt;/p&gt;

&lt;p&gt;If you made customizations to your system in v1.6 or v1.7, you had to remember what those customizations were when you upgraded your cluster. No longer: beginning with v1.8.0, kubeadm uploads your configuration to a ConfigMap inside of the cluster, and later reads that configuration when upgrading for a seamless user experience.&lt;/p&gt;

&lt;p&gt;The first certificate rotation feature has graduated to beta in v1.8, which is great to see. Thanks to the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-auth&#34; target=&#34;_blank&#34;&gt;Auth Special Interest Group&lt;/a&gt;, the Kubernetes node component kubelet can now &lt;a href=&#34;https://github.com/kubernetes/features/issues/266&#34; target=&#34;_blank&#34;&gt;rotate its client certificate&lt;/a&gt; automatically. We expect this area to improve continuously, and will continue to be a part of this cross-SIG effort to easily rotate all certificates in any cluster.&lt;/p&gt;

&lt;p&gt;Last but not least, kubeadm is more resilient now. kubeadm init will detect even more faulty environments earlier, and time out instead of waiting forever for the expected condition.&lt;/p&gt;

&lt;h2 id=&#34;the-scope-of-kubeadm&#34;&gt;The scope of kubeadm&lt;/h2&gt;

&lt;p&gt;As there are so many different end-to-end installers for Kubernetes, there is some fragmentation in the ecosystem. With each new release of Kubernetes, these installers naturally become more divergent. This can create problems down the line if users rely on installer-specific variations and hooks that aren’t standardized in any way. Our goal from the beginning has been to make kubeadm a building block for deploying Kubernetes clusters and to provide kubeadm init and kubeadm join as best-practice “fast paths” for new Kubernetes users. Ideally, using kubeadm as the basis of all deployments will make it easier to create conformant clusters.&lt;/p&gt;

&lt;p&gt;kubeadm performs the actions necessary to get a minimum viable cluster up and running. It only cares about bootstrapping, not about provisioning machines, by design. Likewise, installing various nice-to-have addons by default like the &lt;a href=&#34;https://github.com/kubernetes/dashboard&#34; target=&#34;_blank&#34;&gt;Kubernetes Dashboard&lt;/a&gt;, some monitoring solution, cloud provider-specific addons, etc. is not in scope. Instead, we expect higher-level and more tailored tooling to be built on top of kubeadm, that installs the software the end user needs.&lt;/p&gt;

&lt;h2 id=&#34;v1-9-0-and-beyond&#34;&gt;v1.9.0 and beyond&lt;/h2&gt;

&lt;p&gt;What’s in store for the future of kubeadm?&lt;/p&gt;

&lt;h4 id=&#34;planned-features&#34;&gt;Planned features&lt;/h4&gt;

&lt;p&gt;We plan to address high availability (replicated etcd and multiple, redundant API servers and other control plane components) as an alpha feature in v1.9.0. This has been a regular request from our user base.&lt;/p&gt;

&lt;p&gt;Also, we want to make self-hosting the default way to deploy your control plane: Kubernetes becomes much easier to manage if we can rely on Kubernetes&amp;rsquo; own tools to manage the cluster components.&lt;/p&gt;

&lt;h4 id=&#34;promoting-kubeadm-adoption-and-getting-involved&#34;&gt;Promoting kubeadm adoption and getting involved&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-kubeadm-adoption&#34; target=&#34;_blank&#34;&gt;kubeadm adoption working group&lt;/a&gt; is an ongoing effort between SIG Cluster Lifecycle and other parties in the Kubernetes ecosystem. This working group focuses on making kubeadm more extensible in order to promote adoption of it for other end-to-end installers in the community. Everyone is welcome to join. So far, we’re glad to announce that &lt;a href=&#34;https://github.com/kubernetes-incubator/kubespray&#34; target=&#34;_blank&#34;&gt;kubespray&lt;/a&gt; started using kubeadm under the hood, and gained new features at the same time! We’re excited to see others follow and make the ecosystem stronger.&lt;/p&gt;

&lt;p&gt;kubeadm is a great way to learn about Kubernetes: it binds all of Kubernetes’ components together in a single package. To learn more about what kubeadm really does under the hood, &lt;a href=&#34;https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.8.md&#34; target=&#34;_blank&#34;&gt;this document&lt;/a&gt; describes kubeadm functions in v1.8.0.&lt;/p&gt;

&lt;p&gt;If you want to get involved in these efforts, join SIG Cluster Lifecycle. We &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;meet on Zoom&lt;/a&gt; once a week on Tuesdays at 16:00 UTC. For more information about what we talk about in our weekly meetings, &lt;a href=&#34;https://docs.google.com/document/d/1deJYPIF4LmhGjDVaqrswErIrV7mtwJgovtLnPCDxP7U/edit#&#34; target=&#34;_blank&#34;&gt;check out our meeting notes&lt;/a&gt;. Meetings are a great educational opportunity, even if you don’t want to jump in and present your own ideas right away. You can also sign up for our &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;mailing list&lt;/a&gt;, join our &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;Slack channel,&lt;/a&gt; &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;o&lt;/a&gt;r check out the &lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP29D0nYgAGWt1ZFqS9Z7lw4&amp;amp;disable_polymer=true&#34; target=&#34;_blank&#34;&gt;video archive&lt;/a&gt; of our past mee&lt;a href=&#34;https://kubernetes.slack.com/messages/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;t&lt;/a&gt;i&lt;a href=&#34;https://kubernetes.slack.com/messages/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;n&lt;/a&gt;g&lt;a href=&#34;https://kubernetes.slack.com/messages/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;s&lt;/a&gt;. Even if you’re only interested in watching the video calls initially, we’re excited to welcome you as a new member to SIG Cluster Lifecycle!&lt;/p&gt;

&lt;p&gt;If you want to know what a kubeadm developer does at a given time in the Kubernetes release cycle, check out &lt;a href=&#34;https://github.com/kubernetes/kubeadm/blob/master/docs/release-cycle.md&#34; target=&#34;_blank&#34;&gt;this doc&lt;/a&gt;. Finally, don’t hesitate to join if any of our upcoming projects are of interest to you!&lt;/p&gt;

&lt;p&gt;Thank you,&lt;br /&gt;
Lucas Käldström&lt;br /&gt;
Kubernetes maintainer &amp;amp; SIG Cluster Lifecycle co-lead&lt;br /&gt;
&lt;a href=&#34;https://www.weave.works/?utm_source=k8&amp;amp;utm_medium=ww&amp;amp;utm_campaign=blog&#34; target=&#34;_blank&#34;&gt;Weaveworks&lt;/a&gt; contractor&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Five Days of Kubernetes 1.8 </title>
      <link>https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 1.8 is live, made possible by hundreds of contributors pushing thousands of commits in this latest releases.&lt;/p&gt;

&lt;p&gt;The community has tallied more than 66,000 commits in the main repo and continues rapid growth outside of the main repo, which signals growing maturity and stability for the project. The community has logged more than 120,000 commits across all repos and 17,839 commits across all repos for v1.7.0 to v1.8.0 alone.&lt;/p&gt;

&lt;p&gt;With the help of our growing community of 1,400 plus contributors, we issued more than 3,000 PRs and pushed more than 5,000 commits to deliver Kubernetes 1.8 with significant security and workload support updates. This all points to increased stability, a result of our project-wide focus on maturing &lt;a href=&#34;https://github.com/kubernetes/sig-release&#34; target=&#34;_blank&#34;&gt;process&lt;/a&gt;, formalizing &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-architecture&#34; target=&#34;_blank&#34;&gt;architecture&lt;/a&gt;, and strengthening Kubernetes’ &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/community/elections/2017&#34; target=&#34;_blank&#34;&gt;governance model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While many improvements have been contributed, we highlight key features in this series of in-depth&amp;nbsp;posts listed below. &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;Follow along&lt;/a&gt; and see what’s new and improved with storage, security and more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Day 1:&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18&#34; target=&#34;_blank&#34;&gt;5 Days of Kubernetes 1.8&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Day 2:&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/kubeadm-v18-released&#34; target=&#34;_blank&#34;&gt;kubeadm v1.8 Introduces Easy Upgrades for Kubernetes Clusters&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Day 3:&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes&#34; target=&#34;_blank&#34;&gt;Kuberentes v.1.8 Retrospective: It Takes a Village to Raise a Kubernetes&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Day 4:&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18&#34; target=&#34;_blank&#34;&gt;Using RBAC, Generally Available in Kubernetes v1.8&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Day 5:&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes&#34; target=&#34;_blank&#34;&gt;Enforcing Network Policies in Kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Connect&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Connect with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Get involved with the Kubernetes project on &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Introducing Software Certification for Kubernetes </title>
      <link>https://kubernetes.io/blog/2017/10/software-conformance-certification/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/10/software-conformance-certification/</guid>
      <description>
        
        
        &lt;p&gt;&lt;em&gt;&lt;strong&gt;Editor&amp;rsquo;s Note: Today&amp;rsquo;s post is by William Denniss, Product Manager, Google Cloud on the new Certified Kubernetes Conformance Program.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Over the last three years, Kubernetes® has seen wide-scale adoption by a vibrant and diverse community of providers. In fact, there are now more than &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1LxSqBzjOxfGx3cmtZ4EbB_BGCxT_wlxW_xgHVVa23es/edit#gid=0&#34; target=&#34;_blank&#34;&gt;60&lt;/a&gt; known Kubernetes platforms and distributions. From the start, one goal of Kubernetes has been consistency and portability.&lt;/p&gt;

&lt;p&gt;In order to better serve this goal, today the Kubernetes community and the Cloud Native Computing Foundation® (CNCF®) announce the availability of the beta Certified Kubernetes Conformance Program. The Kubernetes conformance certification program gives users the confidence that when they use a Certified Kubernetes™ product, they can rely on a high level of common functionality. Certification provides Independent Software Vendors (ISVs) confidence that if their customer is using a Certified Kubernetes product, their software will behave as expected.&lt;/p&gt;

&lt;p&gt;CNCF and the Kubernetes Community invites all vendors to &lt;a href=&#34;https://github.com/cncf/k8s-conformance/blob/master/instructions.md&#34; target=&#34;_blank&#34;&gt;run the conformance test suite&lt;/a&gt;, and submit conformance testing results for review and certification by the CNCF. When the program graduates to GA (generally available) later this year, all vendors receiving certification during the beta period will be listed in the launch announcement.&lt;/p&gt;

&lt;p&gt;Just like Kubernetes itself, conformance certification is an evolving program managed by contributors in our community. Certification is versioned alongside Kubernetes, and certification requirements receive updates with each version of Kubernetes as features are added and the architecture changes. The Kubernetes community, through &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-architecture&#34; target=&#34;_blank&#34;&gt;SIG Architecture&lt;/a&gt;, controls changes and overseers what it means to be Certified Kubernetes. The &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-testing&#34; target=&#34;_blank&#34;&gt;Testing SIG&lt;/a&gt; works on the mechanics of conformance tests, while the &lt;a href=&#34;https://github.com/cncf/k8s-conformance&#34; target=&#34;_blank&#34;&gt;Conformance Working Group&lt;/a&gt; develops process and policy for the certification program.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/-seEomiDY4syaWVbl0KT7k9fcJmylYK1n9_VANKyo5oIP5gH9MuIq_dcB_q3qvjE5YzOdM2HthMyc_wduC4xLmPStsb6Q6ASPBfOWi7ssGylfy1I7Pbd64THobytWa_1JX-pscH4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Once the program moves to GA, certified products can proudly display the new Certified Kubernetes logo mark with stylized version information on their marketing materials. Certified products can also take advantage of a new combination trademark rule the CNCF adopted for Certified Kubernetes providers that keep their certification up to date.&lt;/p&gt;

&lt;p&gt;Products must complete a recertification each year for the current or previous version of Kubernetes to remain certified. This ensures that when you see the Certified Kubernetes™ mark on a product, you’re not only getting something that’s proven conformant, but also contains the latest features and improvements from the community.&lt;/p&gt;

&lt;p&gt;Visit &lt;a href=&#34;https://github.com/cncf/k8s-conformance&#34; target=&#34;_blank&#34;&gt;https://github.com/cncf/k8s-conformance&lt;/a&gt; for more information about the Certified Kubernetes Conformance Program, and learn how you can include your product in a growing list of Certified Kubernetes providers.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Cloud Native Computing Foundation”, “CNCF” and “Kubernetes” are registered trademarks of The Linux Foundation in the United States and other countries. “Certified Kubernetes” and the Certified Kubernetes design are trademarks of The Linux Foundation in the United States and other countries.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Request Routing and Policy Management with the Istio Service Mesh </title>
      <link>https://kubernetes.io/blog/2017/10/request-routing-and-policy-management/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2017/10/request-routing-and-policy-management/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Editor&amp;rsquo;s note: Today’s post by Frank Budinsky, Software Engineer, IBM, Andra Cismaru, Software Engineer, Google, and Israel Shalom, Product Manager, Google, is the second post in a three-part series on Istio. It offers a closer look at request routing and policy management.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&#34;https://kubernetes.io/blog/2017/05/managing-microservices-with-istio-service-mesh&#34; target=&#34;_blank&#34;&gt;previous article&lt;/a&gt;, we looked at a &lt;a href=&#34;https://istio.io/docs/guides/bookinfo.html&#34; target=&#34;_blank&#34;&gt;simple application (Bookinfo)&lt;/a&gt; that is composed of four separate microservices. The article showed how to deploy an application with Kubernetes and an Istio-enabled cluster without changing any application code. The article also outlined how to view Istio provided L7 metrics on the running services.&lt;/p&gt;

&lt;p&gt;This article follows up by taking a deeper look at Istio using Bookinfo. Specifically, we’ll look at two more features of Istio: request routing and policy management.&lt;/p&gt;

&lt;h2 id=&#34;running-the-bookinfo-application&#34;&gt;Running the Bookinfo Application&lt;/h2&gt;

&lt;p&gt;As before, we run the v1 version of the Bookinfo application. After &lt;a href=&#34;https://istio.io/docs/setup/kubernetes/quick-start.html&#34; target=&#34;_blank&#34;&gt;installing Istio&lt;/a&gt; in our cluster, we start the app defined in &lt;a href=&#34;https://raw.githubusercontent.com/istio/istio/master/samples/kubernetes-blog/bookinfo-v1.yaml&#34; target=&#34;_blank&#34;&gt;bookinfo-v1.yaml&lt;/a&gt; using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f \&amp;lt;(istioctl kube-inject -f bookinfo-v1.yaml)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We created an Ingress resource for the app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat \&amp;lt;\&amp;lt;EOF | kubectl create -f -

apiVersion: extensions/v1beta1

kind: Ingress

metadata:

 name: bookinfo

 annotations:

     kubernetes.io/ingress.class: &amp;quot;istio&amp;quot;

spec:

 rules:

 - http:

         paths:

         - path: /productpage

             backend:

                 serviceName: productpage

                 servicePort: 9080

         - path: /login

             backend:

                 serviceName: productpage

                 servicePort: 9080

         - path: /logout

             backend:

                 serviceName: productpage

                 servicePort: 9080

EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we retrieved the NodePort address of the Istio Ingress controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export BOOKINFO\_URL=$(kubectl get po -n istio-system -l istio=ingress -o jsonpath={.items[0].status.hostIP}):$(kubectl get svc -n istio-system istio-ingress -o jsonpath={.spec.ports[0].nodePort})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we pointed our browser to &lt;a href=&#34;about:blank&#34; target=&#34;_blank&#34;&gt;http://$BOOKINFO_URL/productpage&lt;/a&gt;, to see the running v1 application:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/kGRJnhkf30FBOY2pyZzID90f_zxlyMUv43hEvfq70bcmYhKrGv2em2qph21k-ahlwfBthV3XQSf6CuUQXMlvgSlOUJr4W1ksDVXIvChEd6a5Y51lwepHmyQx2ksJgUpyTiEbZN11&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;http-request-routing&#34;&gt;HTTP request routing&lt;/h2&gt;

&lt;p&gt;Existing container orchestration platforms like Kubernetes, Mesos, and other microservice frameworks allow operators to control when a particular set of pods/VMs should receive traffic (e.g., by adding/removing specific labels). Unlike existing techniques, Istio decouples traffic flow and infrastructure scaling. This allows Istio to provide a variety of traffic management features that reside outside the application code, including dynamic HTTP &lt;a href=&#34;https://istio.io/docs/concepts/traffic-management/request-routing.html&#34; target=&#34;_blank&#34;&gt;request routing&lt;/a&gt; for A/B testing, canary releases, gradual rollouts, &lt;a href=&#34;https://istio.io/docs/concepts/traffic-management/handling-failures.html&#34; target=&#34;_blank&#34;&gt;failure recovery&lt;/a&gt; using timeouts, retries, circuit breakers, and &lt;a href=&#34;https://istio.io/docs/concepts/traffic-management/fault-injection.html&#34; target=&#34;_blank&#34;&gt;fault injection&lt;/a&gt; to test compatibility of failure recovery policies across services.&lt;/p&gt;

&lt;p&gt;To demonstrate, we’ll deploy v2 of the &lt;strong&gt;reviews&lt;/strong&gt; service and use Istio to make it visible only for a specific test user. We can create a Kubernetes deployment, reviews-v2, with &lt;a href=&#34;https://raw.githubusercontent.com/istio/istio/master/samples/kubernetes-blog/bookinfo-reviews-v2.yaml&#34; target=&#34;_blank&#34;&gt;this YAML file&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1

kind: Deployment

metadata:

 name: reviews-v2

spec:

 replicas: 1

 template:

     metadata:

         labels:

             app: reviews

             version: v2

     spec:

         containers:

         - name: reviews

             image: istio/examples-bookinfo-reviews-v2:0.2.3

             imagePullPolicy: IfNotPresent

             ports:

             - containerPort: 9080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From a Kubernetes perspective, the v2 deployment adds additional pods that the reviews service selector includes in the round-robin load balancing algorithm. This is also the default behavior for Istio.&lt;/p&gt;

&lt;p&gt;Before we start reviews:v2, we’ll start the last of the four Bookinfo services, ratings, which is used by the v2 version to provide ratings stars corresponding to each review:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f \&amp;lt;(istioctl kube-inject -f bookinfo-ratings.yaml)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we were to start &lt;strong&gt;reviews:v2&lt;/strong&gt; now, we would see browser responses alternating between v1 (reviews with no corresponding ratings) and v2 (review with black rating stars). This will not happen, however, because we’ll use Istio’s traffic management feature to control traffic.&lt;/p&gt;

&lt;p&gt;With Istio, new versions don’t need to become visible based on the number of running pods. Version visibility is controlled instead by rules that specify the exact criteria. To demonstrate, we start by using Istio to specify that we want to send 100% of reviews traffic to v1 pods only.&lt;/p&gt;

&lt;p&gt;Immediately setting a default rule &lt;a href=&#34;https://github.com/istio/istio/blob/master/samples/bookinfo/kube/route-rule-all-v1.yaml&#34; target=&#34;_blank&#34;&gt;for every service&lt;/a&gt; in the mesh is an Istio best practice. Doing so avoids accidental visibility of newer, potentially unstable versions. For the purpose of this demonstration, however, we’ll only do it for the reviews service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat \&amp;lt;\&amp;lt;EOF | istioctl create -f -

apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

   name: reviews-default

spec:

   destination:

       name: reviews

   route:

   - labels:

           version: v1

       weight: 100

EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command directs the service mesh to send 100% of traffic for the reviews service to pods with the label “version: v1”. With this rule in place, we can safely deploy the v2 version without exposing it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f \&amp;lt;(istioctl kube-inject -f bookinfo-reviews-v2.yaml)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Refreshing the Bookinfo web page confirms that nothing has changed.&lt;/p&gt;

&lt;p&gt;At this point we have all kinds of options for how we might want to expose &lt;strong&gt;reviews:v2&lt;/strong&gt;. If for example we wanted to do a simple canary test, we could send 10% of the traffic to v2 using a rule like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

   name: reviews-default

spec:

   destination:

       name: reviews

   route:

   - labels:

           version: v2

       weight: 10

   - labels:

           version: v1

       weight: 90
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A better approach for early testing of a service version is to instead restrict access to it much more specifically. To demonstrate, we’ll set a rule to only make reviews:v2 visible to a specific test user. We do this by setting a second, higher priority rule that will only be applied if the request matches a specific condition:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat \&amp;lt;\&amp;lt;EOF | istioctl create -f -

apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

 name: reviews-test-v2

spec:

 destination:

     name: reviews

 precedence: 2

 match:

     request:

         headers:

             cookie:

                 regex: &amp;quot;^(.\*?;)?(user=jason)(;.\*)?$&amp;quot;

 route:

 - labels:

         version: v2

     weight: 100

EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we’re specifying that the request headers need to include a user cookie with value “tester” as the condition. If this rule is not matched, we fall back to the default routing rule for v1.&lt;/p&gt;

&lt;p&gt;If we login to the Bookinfo UI with the user name “tester” (no password needed), we will now see version v2 of the application (each review includes 1-5 black rating stars). Every other user is unaffected by this change.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/WLvX01Oja8R_cMb_AD91jIiF0bHW0nSJTRJ6Vt3Xz75MLzivZ5-ghHEZkdTJryhNXyTCUemF4OwxYn_96ntimOwyjABuZjaH3O2RsJyYQbqWoQgvSQktQd98t3T3Qe3KZSd20Cam&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Once the v2 version has been thoroughly tested, we can use Istio to proceed with a canary test using the rule shown previously, or we can simply migrate all of the traffic from v1 to v2, optionally in a gradual fashion by using a sequence of rules with weights less than 100 (for example: 10, 20, 30, &amp;hellip; 100). This traffic control is independent of the number of pods implementing each version. If, for example, we had auto scaling in place, and high traffic volumes, we would likely see a corresponding scale up of v2 and scale down of v1 pods happening independently at the same time. For more about version routing with autoscaling, check out &lt;a href=&#34;https://istio.io/blog/canary-deployments-using-istio.html&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Canary Deployments using Istio&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In our case, we’ll send all of the traffic to v2 with one command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat \&amp;lt;\&amp;lt;EOF | istioctl replace -f -

apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

   name: reviews-default

spec:

   destination:

       name: reviews

   route:

   - labels:

           version: v2

       weight: 100

EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We should also remove the special rule we created for the tester so that it doesn’t override any future rollouts we decide to do:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;istioctl delete routerule reviews-test-v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the Bookinfo UI, we’ll see that we are now exposing the v2 version of reviews to all users.&lt;/p&gt;

&lt;h2 id=&#34;policy-enforcement&#34;&gt;Policy enforcement&lt;/h2&gt;

&lt;p&gt;Istio provides policy enforcement functions, such as quotas, precondition checking, and access control. We can demonstrate Istio’s open and extensible framework for policies with an example: rate limiting.&lt;/p&gt;

&lt;p&gt;Let’s pretend that the Bookinfo ratings service is an external paid service&amp;ndash;for example, &lt;a href=&#34;https://www.rottentomatoes.com/&#34; target=&#34;_blank&#34;&gt;Rotten Tomatoes®&lt;/a&gt;&amp;ndash;with a free quota of 1 request per second (req/sec). To make sure the application doesn’t exceed this limit, we’ll specify an Istio policy to cut off requests once the limit is reached. We’ll use one of Istio’s built-in policies for this purpose.&lt;/p&gt;

&lt;p&gt;To set a 1 req/sec quota, we first configure a &lt;strong&gt;memquota&lt;/strong&gt; handler with rate limits:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat \&amp;lt;\&amp;lt;EOF | istioctl create -f -

apiVersion: &amp;quot;config.istio.io/v1alpha2&amp;quot;

kind: memquota

metadata:

 name: handler

 namespace: default

spec:

 quotas:

 - name: requestcount.quota.default

     maxAmount: 5000

     validDuration: 1s

     overrides:

     - dimensions:

             destination: ratings

         maxAmount: 1

         validDuration: 1s

EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we create a &lt;strong&gt;quota&lt;/strong&gt; instance that maps incoming attributes to quota dimensions, and create a &lt;strong&gt;rule&lt;/strong&gt; that uses it with the &lt;strong&gt;memquota&lt;/strong&gt; handler:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat \&amp;lt;\&amp;lt;EOF | istioctl create -f -

apiVersion: &amp;quot;config.istio.io/v1alpha2&amp;quot;

kind: quota

metadata:

 name: requestcount

 namespace: default

spec:

 dimensions:

     source: source.labels[&amp;quot;app&amp;quot;] | source.service | &amp;quot;unknown&amp;quot;

     sourceVersion: source.labels[&amp;quot;version&amp;quot;] | &amp;quot;unknown&amp;quot;

     destination: destination.labels[&amp;quot;app&amp;quot;] | destination.service | &amp;quot;unknown&amp;quot;

     destinationVersion: destination.labels[&amp;quot;version&amp;quot;] | &amp;quot;unknown&amp;quot;

---

apiVersion: &amp;quot;config.istio.io/v1alpha2&amp;quot;

kind: rule

metadata:

 name: quota

 namespace: default

spec:

 actions:

 - handler: handler.memquota

     instances:

     - requestcount.quota

EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To see the rate limiting in action, we’ll generate some load on the application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wrk -t1 -c1 -d20s http://$BOOKINFO\_URL/productpage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the web browser, we’ll notice that while the load generator is running (i.e., generating more than 1 req/sec), browser traffic is cut off. Instead of the black stars next to each review, the page now displays a message indicating that ratings are not currently available.&lt;/p&gt;

&lt;p&gt;Stopping the load generator means the limit will no longer be exceeded: the black stars return when we refresh the page.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We’ve shown you how to introduce advanced features like HTTP request routing and policy injection into a service mesh configured with Istio without restarting any of the services. This lets you develop and deploy without worrying about the ongoing management of the service mesh; service-wide policies can always be added later.&lt;/p&gt;

&lt;p&gt;In the next and last installment of this series, we’ll focus on Istio’s security and authentication capabilities. We’ll discuss how to secure all interservice communications in a mesh, even against insiders with access to the network, without any changes to the application code or the deployment.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>